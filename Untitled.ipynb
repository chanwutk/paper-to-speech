{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215cfe6e-0cb7-46b1-babb-13d4cf377cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e6d72f-61af-492c-a32a-ad689261d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04476fcc-4d4f-429b-b462-2df320b89121",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_from_pdf('vldb_viva.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea844fff-b305-461d-a2ef-64a2a5afedc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Optimizing Video Analytics with Declarative Model Relationships\\n\\nFrancisco Romero∗\\nStanford University\\nfaromero@stanford.edu\\n\\nDaniel Kang\\nStanford University\\nddkang@cs.stanford.edu\\n\\nJohann Hauswald∗\\nStanford University &\\nSutter Hill Ventures\\njohannh@stanford.edu\\n\\nMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\n\\nAditi Partap\\nStanford University\\naditi712@stanford.edu\\n\\nChristos Kozyrakis\\nStanford University\\nchristos@cs.stanford.edu\\n\\nABSTRACT\\n\\n1 INTRODUCTION\\n\\nThe availability of vast video collections and the accuracy of ML\\nmodels has generated significant interest in video analytics sys-\\ntems. Since naively processing all frames using expensive models\\nis impractical, researchers have proposed optimizations such as\\nselectively using faster but less accurate models to replace or filter\\nframes for expensive models. However, these optimizations are\\ndifficult to apply on queries with multiple predicates and models, as\\nusers must manually explore a large optimization space. Without\\nsignificant systems expertise or time investment, an analyst may\\nmanually create an execution plan that is unnecessarily expensive\\nand/or terribly inaccurate.\\n\\nWe propose Relational Hints, a declarative interface that allows\\nusers to suggest ML model relationships based on domain knowl-\\nedge. Users can express two key relationships: when a model can\\nreplace another (CAN REPLACE) and when a model can be used\\nto filter frames for another (CAN FILTER). We aim to design an\\ninterface to express model relationships informed by domain specific\\nknowledge and define the constraints by which these relationships\\nhold. We then present the VIVA video analytics system that uses\\nrelational hints to optimize SQL queries on video datasets. VIVA\\nautomatically selects and validates the hints applicable to the query,\\ngenerates possible query plans using a formal set of transformations,\\nand finds the best performance plan that meets a user’s accuracy\\nrequirements. VIVA relieves users from rewriting and manually\\noptimizing video queries as new models become available and exe-\\ncution environments evolve. We evaluate VIVA implemented on\\ntop of Spark and show that hints improve performance up to 16.6×\\nwithout sacrificing accuracy.\\n\\nPVLDB Reference Format:\\nFrancisco Romero, Johann Hauswald, Aditi Partap, Daniel Kang, Matei\\nZaharia, and Christos Kozyrakis. Optimizing Video Analytics with\\nDeclarative Model Relationships. PVLDB, 16(3): 447 - 460, 2022.\\ndoi:10.14778/3570690.3570695\\n\\nPVLDB Artifact Availability:\\n\\nThe source code, data, and/or other artifacts have been made available at\\nhttps://github.com/stanford-mast/viva-vldb23-artifact.\\n\\n∗Denotes equal contribution.\\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\\nthis license. For any use beyond those covered by this license, obtain permission by\\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\nlicensed to the VLDB Endowment.\\nProceedings of the VLDB Endowment, Vol. 16, No. 3 ISSN 2150-8097.\\ndoi:10.14778/3570690.3570695\\n\\nVideo analytics, the ability to extract insights from video, is enabled\\nby increasingly accurate machine learning (ML) models and ac-\\ncess to large archives of professionally produced content or videos\\ncaptured by devices like cellphones, security cameras, and video-\\nconference systems. While we can already answer queries over\\nvideos like “have any cars passed this intersection that match an\\nAMBER alert?”, several challenges remain before video analytics\\nare as practical and as performant over analytics on structured data.\\nFor complex video analytics queries with multiple predicates or ML\\nmodels, users must manually optimize their queries to avoid the\\nhigh cost of naively executing large models on every frame using\\nexpensive hardware. For example, it takes over 14 GPU-months to\\nprocess 100 camera-months of video using a very accurate YOLOv5\\nmodel for object detection [55].\\n\\nConsider an analyst studying political coverage of major ca-\\nble news channels that writes a query to find instances of Bernie\\nSanders, a politician, reacting angrily to Jake Tapper, a TV news\\nhost [20]. Their query may use object detection to find scenes with\\ntwo people, face recognition to find instances of Jake Tapper and\\nBernie Sanders, and emotion detection to detect angry reactions.\\nThis query can take minutes to execute using unnecessarily ac-\\ncurate models, even on small video inputs, making it challenging\\nfor the analyst to interactively explore their dataset. To improve\\nperformance, the analyst may use domain knowledge to explore\\nthe following model optimizations:\\n• Replacement: use a different model for a task, such as a cheaper\\n\\nbut less accurate object detector [25, 27, 28, 47].\\n\\n• Input Filtering: use a fast model to filter inputs to an expensive\\nmodel [26, 36, 63]. For example, insert a binary classifier to detect\\nfaces before recognizing Tapper or Sanders in frames.\\n\\n• Predicate Reordering: run emotion detection before face detection\\n\\nbecause it is more selective.\\n\\nThe domain knowledge needed to consider such optimizations may\\ncome from (1) historical or similar queries using alternate models,\\n(2) insights about the training data or query dataset like knowing\\nangry emotions are less prevalent than neutral or happy ones, or\\n(3) knowledge about a general area of expertise (e.g., news, traffic,\\nor sports analysis) suggesting a particular fine-tuned model would\\nbe better suited for the domain.\\n\\nUnfortunately, systems today do not provide an interface for\\nusers to specify optimizations based on domain knowledge. Users\\nmust manually explore the performance-accuracy tradeoff across\\n\\n\\x0cnumerous combinations of optimizations in queries with multiple\\npredicates. We found that, for the news analysis query in which\\nthere are nearly 100 plan options, performance can vary by up to\\n11.7× across different query plans with an accuracy requirement\\nof 80%. Systems today provide no easy way to validate potential\\noptimizations either. Hence, in order to optimize video analysis\\nqueries, users must build significant expertise in ML models and\\nsystems, taking away valuable time and money from their primary\\ntask of gleaning insights from video data.\\n\\nThe first goal of this work is to design a user interface to express\\nmodel relationships informed by domain specific knowledge and de-\\nfine the constraints by which these relationships hold. Our second\\ngoal is to develop a video query engine that automatically validates\\nrelationships and optimizes complex queries. The engine explores\\nalternate query plans and handles performance-accuracy tradeoffs,\\nrelieving users from manual exploration and optimization.\\n\\nWe propose a declarative SQL interface for model relationships\\ncalled relational hints. We capture the semantics of two relationships\\nbetween a model M and model H considered for optimization:\\n• H CAN REPLACE M denotes H and M are interchangeable in\\nthe query plan. For example, H may be a faster object detection\\nmodel the user wants to consider instead of the current one.\\n• H CAN FILTER M denotes H can be used to filter inputs to M.\\nFor example, H could be a fast binary classifier for detecting the\\npresence of a face prior to recognizing a specific person.\\n\\nHints capture high-level relationships based on the models’ output\\nsignatures and their class labels. Hence, they can often be reused\\nacross queries similar to an index. They remove the need for users\\nto manually rewrite queries when a new model becomes available\\nand reason about how the probabilistic nature of models impact\\ntheir query’s end-to-end accuracy goal.\\n\\nWe develop VIVA a video analytics system that optimizes com-\\nplex SQL queries using relational hints. VIVA’s hint validator first\\ndetermines what hints are applicable for the query. VIVA’s planner\\nuses the validated hints to generate alternate plans using model\\nreplacements, data filtering, and predicate reordering while pruning\\nand limiting the search space for fast query optimization. VIVA’s\\noptimizer enumerates plans enabled by hints and automatically\\nnavigates the performance-accuracy tradeoff to select the best per-\\nformance plan meeting the user’s accuracy requirements.\\nIn summary, we make the following contributions:\\n\\n• We highlight the difficulty of manually optimizing complex video\\nqueries, showing that performance on the same query applying\\ndifferent optimizations can vary by up to 11.7× (Section 2).\\n• We formalize a declarative SQL interface for users to specify in-\\ntuitive relationships between ML models used in video analytics\\nbased on domain knowledge (Sections 3 and 4).\\n\\n• We detail the design of VIVA that incorporates relational hints\\nin query planning and optimization given the user’s accuracy\\nrequirements (Section 5).\\n\\n• We implement VIVA of Spark [64] (Section 6) and show that\\nacross four real-world queries with different video inputs, hints\\nimprove performance up to 16.6× while meeting user accuracy\\nrequirements (Section 7).\\n\\n2 THE COMPLEXITY OF VIDEO QUERIES\\n\\nRecent work on video analytics optimization focuses primarily on\\noptimizing a single predicate that uses an ML model, implemented\\nas a user-defined function (UDF) in a query execution engine. Cur-\\nrent proposed techniques explore the performance-accuracy trade-\\noff using fast proxy models to replace more expensive ones [25, 63],\\ncheap filters to reduce the amount of processing needed [26, 36]\\nand indexing for video data [28]. In contrast, we focus on complex\\nqueries composed of multiple compute-intensive ML models and\\npredicates. These queries are typically applied on large datasets\\nusing scale-out execution engines [42, 48]. Unfortunately, executing\\nthese complex queries with multiple ML models is prohibitively\\nexpensive and slow. Prior work estimates that a similar query to\\nthe TV News analysis query previously mentioned over one year\\nof CNN videos would, at time of writing, take over 4 hours and cost\\nmore than $300 using cloud GPUs [31].\\n\\nWhile it is possible to optimize in isolation each model and\\npredicate in a complex query, this approach is unlikely to lead to\\nbest overall performance (lowest latency) and makes it difficult\\nto achieve an overall accuracy goal. We use the TV News analy-\\nsis query to illustrate the difficulty of manual optimization. We\\nconsider the impact of predicate reordering and two optimizations:\\n• Model Replacement – replace the original model with one that\\nhas the same input/output specification but a different perfor-\\nmance accuracy profile. BlazeIt [25] and TASTI [28] are tech-\\nniques that can generate very fast, purpose-built models but still\\nrequire a user to manually specify the use of these fast models.\\n• Data Filter Model – run a cheap classifier ahead of a more\\nexpensive model to filter frames that are unlikely to satisfy the\\npredicate. Systems like PP [36] and CORE [63] automate the\\ninsertion of filter models but do not take into account the effects\\nof other optimizations from an end-to-end query perspective.\\nIn the TV News analysis query, the analyst is looking for instances\\nof the politician Bernie Sanders reacting angrily to a news anchor\\nJake Tapper. The analyst uses object detection to find frames with\\ntwo people, face recognition to find frames with both Sanders and\\nTapper, and emotion detection to find the angry emotion. Figure 1\\nshows the accuracies and latencies of different plans for this query\\nafter applying the optimizations discussed above. Accuracy is cal-\\nculated using F1 score [3, 5, 63] with respect to the original plan.\\nThe analyst sets an accuracy requirement of 80%.\\n\\nFigure 1 shows that selecting the best of the 6 possible reorder-\\nings of the 3 predicates in the query leads to 5× latency improve-\\nment. Execution engines today treat UDFs as black-boxes that are\\nnot optimized by the execution engine [64]; the analyst must ex-\\nplore the impact of all possible orders. Next, we show the impact\\nof manually considering model replacements. If the analyst uses\\na faster emotion detection (ED) model, latency improves by 1.01×\\nbut accuracy drops to 79%. In contrast, replacing object detection\\n(OD) with a faster person detection model to label people in frames\\nreduces latency by 1.2× over the best reordering without affecting\\naccuracy (Best). Finally, we investigate the impact of using fast\\nfilter models, such as a cheap face detection (Haar) model to filter\\nframes without faces [56] and a similarity detector (Sim) to remove\\nframes that are not similar to a reference frame [4]. The Haar filter\\ndoes not degrade accuracy but increases the latency by 1.5× over\\nOD as its selectivity is low and acts as a poor filter. The Sim filter\\n\\n\\x0cTable 1: Model Relationship Matrix: dimensions to evaluate how to\\nrelate ML models. The result is the relationship in a query plan.\\n\\nSignature\\n\\nEqual\\nNot Equal\\n\\nClasses\\n\\nEqual or Overlap\\nCAN REPLACE\\nCAN FILTER\\n\\nDisjoint\\nCAN FILTER\\nCAN FILTER\\n\\nFigure 1: Manually Optimizing a TV News Analysis Query. PR: Pred-\\nicate Reordering, RP: Replacing models, FT: Filters, dashed line\\ndenotes the user’s accuracy requirement of 80%.\\n\\nreduces latency by 2× over OD: it skips the expensive face recog-\\nnition model for 94% of the frames without impacting accuracy.\\nThe key difference between the Haar and Sim filters is the former\\nsupports general face detection while the latter finds similarities to\\na reference frame.\\n\\nThe process of exploring the performance-accuracy tradeoffs\\nand the interactions between these optimizations is long and cum-\\nbersome. Users must exhaustively study all options or use some\\nad-hoc trial and error process to find a good plan. For this query\\nwhere the are 6 permutations, 2 replacements, and 2 filters, there\\nare almost 100 plans to consider. The details to derive the number\\nof plans using these optimizations are described Section 5.2. As\\nnew models and optimization opportunities arise, the number of\\nplans for complex queries will only grow. Even for expert users,\\nit is challenging to manually reason about this large number of\\nquery plans. It requires users to not only have an intuition about the\\npotential of an optimization but also have deep knowledge of the\\nperformance-accuracy tradeoffs and selectivity of models within a\\nlarge space of query plans.\\n\\n3 DOMAIN SPECIFIC MODEL RELATIONSHIPS\\n\\nThere is potential for multiple models and predicates to improve\\nthe execution of a query. However, there is no framework to reason\\nabout the relationship between models. In this section, we propose\\na framework to define model relationships.\\n\\nSuppose we have two models, M and H, that we use to process a\\nset of frames F. The models emit a labeled frame with high confi-\\ndence that satisfies a predicate or produce no output and the frame\\nis dropped. The label is the output of the model assigned to the\\nframe given its trained classes while the predicate is part of a user’s\\nquery that filters by a specific label(s). For example, for an object\\ndetection model, the trained classes can be bus, car, person, etc.\\nwhile the predicate can be to only return frames where the label\\nis a person. This is a common scenario where a user runs a model\\nthat generates class labels and only wants to keep frames from a\\n\\ncertain class. There are multiple execution plans that, with a given\\nprobability, can produce the same set of frames and labels on F:\\n• Plan A: Run M on F\\n• Plan B: Run H on F\\n• Plan C: Run H on F, then M on H’s output\\n• Plan D: Run M on F, then H on M’s output\\nWe then ask the question: under what conditions do the above\\nplans produce approximately the same results? For plans A and B\\nto produce the same results, M and H must be interchangeable: H\\ncan replace M in the execution plan (or vice-versa). For Plans C and\\nD to produce the same results as A and B, H can only drop frames\\nM would also have dropped; H is a filter for M. We can characterize\\na model by its signature and output classes. The signature is the\\nmodel’s input and output specification. This is similar to terminol-\\nogy used by TensorFlow [54]. To compare two models, we ask the\\nfollowing questions:\\n• Are the model signatures equal or not?\\n• Do the models have equal, overlapping, or disjoint output classes?\\nTable 1 captures the different options along these dimensions. If\\nH and M have equal signatures and equal or overlapping classes,\\nthen H can replace M. While two models can produce equivalent\\noutputs, they may still differ in performance (execution latency)\\nand/or accuracy. This type of model is referred to as a variant [46–\\n48] or a proxy model [25]. The model architecture, dataset, and\\ntraining parameters affect a model’s performance and accuracy.\\n\\nIf H and M have equal signatures and disjoint classes, or their\\nsignatures are different, H can potentially filter frames for M. For\\nexample, consider an image classifier that outputs animal labels per\\nimage. Now consider an object detector that can produce the same\\nclass labels but the class is attributed to a bounding box. These two\\nmodel signatures are not equal but there is overlap in the classes.\\nThe image classifier can be predicated on whether an animal was\\nfound. This only passes frames to the object detector that are highly\\nlikely to have an animal. The image classifier acts as a filter. The\\nsetup is similar for disjoint labels except a user specifies under what\\nconditions the predicate for the image classifier is true. This can be\\nspecified based on a user’s domain knowledge.\\n\\n4 RELATIONAL HINTS\\nWe next propose a declarative interface called Relational Hints\\n(hints for short) that formalizes the model relationships defined in\\nTable 1. Hints allow users to declaratively express domain specific\\nknowledge about model relationships. The goal is to provide a query\\nplanner with information that enables alternate query plans. A\\nplanner can select among these plans to improve query performance\\nor reduce the price while meeting a user’s accuracy requirements.\\nWe first describe the different types of hints and their syntax. Next,\\nwe walk through a workflow of how hints are used in SQL queries.\\nFinally, we describe sources informing the design of relational hints\\nand relate them to well-known optimizations and domain intuition.\\n\\n4.1 Relational Hint Types\\n\\nA hint takes as input two models and a type, CAN REPLACE or\\nCAN FILTER, that establishes a relationship between the models.\\nWe map Table 1 to our declarative hint interface.\\n\\nOrigBestEDODHaarSim050100150200250300Latency (sec)0255075100F1 scorePRRPFTLatencyF1 score\\x0cDefinition 1 A relational hint is a user defined model relation-\\nship informed by domain knowledge for the purpose of suggesting\\nalternate query plans to an optimizer.\\n\\nSimilar to MicrosoftSQL hints [37] or MySQL hints [39], hints are\\noptions specified to the query optimizer to consider alternate query\\nplans. Unlike the aforementioned hints, relational hints are not en-\\nforced. Users set a minimum query accuracy requirement and the\\noptimizer chooses which hint(s) (if any) meet that requirement. The\\naccuracy is in reference to the unmodified query plan where the\\nlabels produced by the original models represent the ground-truth.\\nThe accuracy is calculated on a video supplied by the user called a\\ncanary input. A canary input is a shorter clip that represents the\\ntype of events the user is looking for. A hint associates a hint model\\nH to an original model M using model specific domain knowledge.\\n\\nDefinition 2 Domain knowledge is external information about\\na model’s signature and class labels in relation to another model.\\n\\nCAN REPLACE Hint. If a model H’s signature and classes are\\n\\nequal or overlap with model M’s signature and classes, a user can\\ndefine a CAN REPLACE hint to suggest H can replace M in a plan:\\n\\nCREATE HINT H CAN REPLACE M\\n\\n[ FALLBACK DISABLED | ENABLED ]\\n\\nA CAN REPLACE hint is optionally parameterized by a FALLBACK\\nargument. When disabled (the default), this expresses to the system\\nthat model H should completely replace M when processing frames.\\nIf enabled, the processing will fallback to the original model M if\\nH does not produce a label because its confidence is too low. We\\nassume confidence thresholds are pre-tuned and set for each model\\nas is commonly done with existing optimizations [25]. In effect,\\nthis threshold arbitrates whether the model will generate a label\\nor not. This can also be exposed to the user as a parameter to tune.\\nSetting FALLBACK ENABLED may result in M having to process the\\nsame inputs that did not satisfy H’s confidence threshold. This may\\nnegate some of the performance benefits of using H. However, it\\ngives finer control to the user of the relationship and how much\\nthey want to trade-off performance and accuracy.\\nCAN FILTER Hint. If model H’s signature is equal and its classes\\nare disjoint from model M, or if model H’s signature is not equal\\nto model M’s signature, a user can define a CAN FILTER hint to\\nsuggest that model H can filter frames for model M. Specifically,\\nframes are only processed by M if they satisfy H’s predicate with\\nhigh confidence using the model’s pre-set threshold:\\n\\nCREATE HINT H CAN FILTER M\\n\\n[ CONDITIONED ON ANY | <list -of - classes > ]\\n\\nA CAN FILTER hint is optionally parameterized by a CONDITIONED\\nON parameter which specifies the relationship between the model\\nclasses. By default, this parameter is set to ANY: any class in H can\\nsatisfy the condition. A user can optionally specify a list of classes.\\nThe list of classes means a user can condition M’s input on the\\nresults of H’s predicate as defined by the condition.\\n\\nPrior work investigates automatically inferring relationships\\nusing historical data [36, 63]. Our interface could be extended\\n\\nto support automatic inference of these relationships by setting\\nCONDITIONED ON to AUTO. In this work, we focus on the overall\\ninterface of expressing model relationships and leave it to future\\nwork to investigate inferring these relationships.\\n\\n4.2 Example Workflow with Relational Hints\\n\\nWe now walk through a workflow using three relational hints for\\nthe TV News analysis query searching for Bernie Sanders reacting\\nangrily to Jake Tapper Hints are registered once and automatically\\nused on future queries when applicable. The first hint expresses\\nknowledge that two object detection models have the same signa-\\nture (labeled bounding boxes of objects) and generate the same\\nnumber of classes but vary in performance and accuracy. These\\nmodels can be related using a CAN REPLACE hint:\\n\\nCREATE HINT ObjectDetectFast CAN REPLACE ObjectDetect\\n\\nThe second hint uses a tuned face recognition model trained on\\njournalists and personalities. This is similar to a BlazeIt trained\\nmodel to represent a more expensive model [18]. This model has\\nthe same signature (labeled bounding boxes of faces) as a general\\nface recognition model, with some overlap in classes, including\\nlabels for Bernie Sanders and Jake Tapper. These models are again\\nrelated using a CAN REPLACE:\\n\\nCREATE HINT FaceRecogNews CAN REPLACE FaceRecognition\\n\\nFALLBACK ENABLED\\n\\nThe CAN REPLACE hint is parameterized with FALLBACK ENABLED\\nto indicate the original FaceRecognition model should be used if\\nFaceRecogNews does not emit a label due to low confidence.\\n\\nThe third hint considers a binary detector with labels face/no\\nface. This binary detector can be trained using optimizations like\\nProbabilistic Predicates [36]. Since frames with a face detected\\ntypically imply that a face is recognized, a user can express the\\nfollowing hint:\\n\\nCREATE HINT FaceDetect CAN FILTER FaceRecognition\\n\\nCONDITIONED ON [ ' face ']\\n\\nConsider an analyst exploring a VIDEO table that contains frames\\nfrom a TV dataset. They submit a query searching for instances of\\nBernie Sanders reacting angrily to Jake Tapper. The analyst sets an\\naccuracy requirement of 90% and provides a short clip (the canary\\ninput) of Bernie Sanders being interviewed and reacting angrily to\\nJake Tapper. The system will use this video to estimate accuracy of\\nnew plans. In the following query, we bold models for which there\\nare valid hints available:\\n\\nSELECT frameID , EmotionDetect ( frame ) AS e ,\\nFaceRecognition ( frame ) AS f ,\\nCOUNT ( SELECT ObjectDetect ( frame ) AS o\\n\\nFROM VIDEO\\nGROUP BY frameID\\nWHERE o. label = ' person ' ) AS pcount ,\\n\\nFROM VIDEO\\nWHERE e. label = ' angry ' AND pcount = 2 \\\\\\n\\nAND f. label LIKE '% Sanders % ' AND f. label LIKE '% Tapper % '\\n\\nACCURACY 90%\\n\\nThe cost-based optimizer will generate additional plans using the\\nregistered hints and selects the fastest plan that meets the user\\naccuracy requirement. The lowest cost plan that meets the user’s\\n\\n\\x0cfrom the model. At query time, the input frame embeddings are\\ncompared to the index and if the frames are similar enough, the\\nstored results are used. This obviates the need to run an expensive\\nmodel on the dataset. This techniques requires training an index\\nfor each target model.\\nArea Expertise. A practitioner could use their knowledge of the\\ntraining data or the dataset to define even richer relationships. For\\nexample, a biologist may wish to detect bears or deer to study their\\nforaging habits [9]. As an alternative way of detecting animals, the\\nbiologist knows camera trap feeds are mostly static and detecting\\nmotion is usually a good indication of an animal being present. A\\nmotion detection model has disjoint labels from an animal detection\\nmodel. This can be expressed using a CAN FILTER hint:\\n\\nCREATE HINT MotionDetect CAN FILTER AnimalDetect\\n\\nCONDITIONED ON [ ' motion ']\\n\\nAnother example can be a sports analyst creating a highlight video\\nof a basketball game. They can use an expensive action recognition\\nmodel that does pose estimation to analyze if there was a scoring\\nmotion and if the ball went through the hoop. Alternatively, they\\ncould replace the action recognition model with an optical character\\nrecognition (OCR) model to detect score changes using a bounding\\nbox on the broadcast score [15, 40]. This would be cheaper because\\nonly a small section of the frame is analyzed. This can be expressed\\nusing a CAN REPLACE hint:\\n\\nCREATE HINT ScoreChangeOCR CAN REPLACE ScoreActionRecog\\n\\nUsers can also express relationships where one of the models do\\nnot process frames. For example, consider the TV News analysis\\nexample from Section 1. An alternative way of detecting Bernie\\nSanders can be to search for him in video transcripts. This can be\\nexpressed as a CAN FILTER hint since it has disjoint labels from\\nthe face recognition model:\\n\\nCREATE HINT TranscriptSearch CAN FILTER FaceRecognition\\n\\nCONDITIONED ON [ ' Sanders ']\\n\\n5 APPLYING RELATIONAL HINTS\\n\\nWe now describe the design of VIVA a video analytics system that\\ninterprets hints to optimize queries. VIVA enables users to query\\nvideos using SQL, provides an interface to specify hints and an\\naccuracy threshold, and automatically applies hints to a query.\\n\\nFigure 2 shows VIVA’s system architecture. Blue components are\\nspecially designed for translating hints to executable plans. Users\\nregister hints with the registrar which are stored in the hints table.\\nThe parser processes the query and creates a query model tree. This\\ntree is passed to the hint validator that determines which hints are\\nrelevant to the existing query. The planner and optimizer apply\\nhint transformation rules to produce additional query plans, esti-\\nmates accuracy, selectivity, and finally the overall cost of each plan.\\nDepending on a user-defined optimization target (performance,\\ncheapest price or best performance per dollar), VIVA selects the\\nplan that meets the user’s accuracy requirements. This plan is sent\\nto the execution engine to process the user’s input.\\n\\nWe walk through each blue component of Figure 2 by breaking\\ndown the query optimization steps shown in Figure 3. We design\\nour system with the following goals in mind:\\n\\nFigure 2: VIVA Architecture Diagram.\\n\\naccuracy is the following, where the changes made are bolded:\\nResult\\n\\n+ EmotionDetect . label isin [ ' angry ']\\n\\n+ ObjectDetectFast . pcount = 2 AND \\\\\\n\\nObjectDetectFast . label isin [ ' person ']\\n+ FaceRecognition . label isin [ ' Tapper ' , ' Sanders ']\\n\\n+ FaceRecogNews . label isin [ ' Sanders ' ] \\\\\\n\\nAND FaceRecogNews . conf > 0.8\\n\\n+ FaceDetect . label isin [ ' face ' ] AND \\\\\\n\\nFaceDetect . conf > 0.7\\n\\nNote the optimizer in applying the hints also finds the optimial\\nexecution order to execute the most selective models first.\\n\\n4.3 Sources of Relational Hints\\n\\nWe now give examples of where model relationships can originate\\nfrom and how users can capture these relationships using hints.\\nModel Variants. Models having the same signatures and either\\nequal or overlapping classes are ideal candidates for CAN REPLACE\\nrelationships. For example, when analyzing 5 popular open-source\\nrepositories for object detection models, we find there are at least 24\\nunique models with varying accuracy and performance characteris-\\ntics [13, 45, 55, 57, 60]. Also, there are 16 pre-trained image classifi-\\ncation models in PyTorch with varying performance and accuracy\\nprofiles [44]. These different profiles can come from the model archi-\\ntecture or the use of techniques like quantization that post-process\\nthe model to further trade accuracy for performance [23].\\n\\nAdditionally, an emerging trend is building models using a com-\\nmon set of layers and fine-tuning the rest for a specific application.\\nThe common layers are known as the “prefix” and the fine-tuned\\nlayers as the “suffix”. These are also considered variants of the orig-\\ninal model. A system can take advantage of saving the results of\\nthe prefix after the first call and reuse the results multiple times\\nonly needing to run the suffix model [24]. Model variants are best\\nexpressed as CAN REPLACE relationships.\\nProxy Models. Proxy models can be trained to be smaller (in total\\nGFLOPs) approximate versions of a larger, more accurate ML model.\\nThey can also be used to limit the number of invocations to the\\nlarger model. For example, a model may be trained on a subset of\\ndata because a fixed-view camera only needs to identify the original\\nmodel’s objects from a single viewpoint [25]. Other techniques like\\nTASTI [28] train embedding indices at query optimization time.\\nThese indices run a model on a small fraction of the input dataset\\nand store a representation (embeddings) of the frames and results\\n\\nVIVAQuery PlannerQuery OptimizerRelational hintregistrarQuery ParserRelational hints tableH1...H2...Execution engineModel libraryHint Validator\\x0cFigure 3: VIVA Query Optimization Steps using Relational Hints.\\n\\nfor node in DepthFirstSearch(QueryTree) do\\n\\n𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠.𝑎𝑝𝑝𝑒𝑛𝑑 (Permutations(𝑛𝑜𝑑𝑒 ) )\\nwhile 𝑁𝑢𝑚𝑃𝑙𝑎𝑛𝑠! = 𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠 do\\n\\nAlgorithm 1 Plan Generation with Hints\\n1: 𝑉 𝑎𝑙𝑖𝑑𝐻𝑖𝑛𝑡𝑠 ← 𝑉 𝑎𝑙𝑖𝑑𝑎𝑡𝑒𝐻𝑖𝑛𝑡𝑠 (𝐴𝑙𝑙𝐻𝑖𝑛𝑡𝑠, 𝑄𝑢𝑒𝑟 𝑦)\\n2: procedure Planner(𝑄𝑢𝑒𝑟 𝑦𝑇 𝑟𝑒𝑒)\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12: end procedure\\n\\n𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠.𝑎𝑝𝑝𝑒𝑛𝑑 (ApplyHints(𝑛𝑜𝑑𝑒 ) )\\n𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠 ← 𝐿𝑒𝑛 (𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠 )\\n𝑁𝑢𝑚𝑃𝑙𝑎𝑛𝑠 ← 𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠\\n\\nend for\\nreturn 𝐴𝑙𝑙𝑃𝑙𝑎𝑛𝑠\\n\\nend while\\n\\n• Validating hints (Section 5.1) – The validator requires rules to\\narbitrate which hints can be used to generate additional plans.\\n• Applying hints to generate plans (Section 5.2) – The planner\\nneeds to translate validated hints to query plans using a formal\\nset of transformation rules while limiting the number of total\\nplans considered.\\n\\n• Providing accuracy guarantees (Section 5.4) – Given a user’s\\naccuracy requirement, the optimizer must compare plans, and\\nonly select those that meet the accuracy requirement.\\n\\n5.1 Validating Hints\\n\\nAfter VIVA’s parser produces a tree with the models to run, the\\nhint validator determines which registered hints are valid for the\\nquery. This is Step 1 in Figure 3. Throughout this section, we refer\\nto the original model as M and the hint model as H. For both hint\\ntypes, this is a fast static analysis where VIVA is comparing hint\\narguments to the model classes.\\nCAN REPLACE hints. CAN REPLACE is only applied if the\\noverlap in classes of H and M are equal or larger than the classes\\npredicated in the user’s query. The requirement is the model H must\\nproduce the same predicated results as M. The hint is discarded if\\nthe classes are not equal or there is no overlap. To limit the search\\nspace, hint validation only retains hints that are likely to yield\\na plan with a lower cost than the original query ccomparing the\\nprofiled latency of the hint model to the original model. The latency\\nis profiled offline as a one-time step.\\nCAN FILTER hints. VIVA validates that the user-specified classes\\nin CONDITIONED ON are in model H. Because the classes can either\\noverlap, be equal, or disjoint, there is no validation for M.\\n\\n5.2 Generating Plans using Hints\\n\\nWe now describe the steps the planner takes to recursively apply\\nhints to a query (Step 2 in Figure 3).\\nTransforming a Query Plan. VIVA’s parser transforms a user’s\\nquery into an intermediate representation called a model tree. Each\\ntree node represents a model and its predicate. If a parent node\\nhas a child, the output of the parent is dependent on its child (or\\nchildren’s) predicate(s) which preserves dependencies.\\n\\nAlgorithm 1 shows the planner’s steps. The planner first reorders\\nthe predicates of that node’s children (line 4) then applies the hints\\nexhaustively (lines 5 ∼ 8) until no new candidate trees can be gen-\\nerated at that node. The order the hints are applied does not matter.\\nRemaining order-agnostic is necessary: a hint could generate a tree\\nthat could be further modified. This is Step 2 of Figure 3, where we\\nshow a subset of the plans generated for the query after applying\\nthe hints from the table. A CAN REPLACE hint can modify a tree\\nin two ways: (1) H replaces M in the new tree, (2) the planner will\\ninsert H before M if the user has set FALLBACK ENABLED. M will\\nonly run on frames where H did not produce a label because the\\nconfidence was too low. The predicate is applied to the union of H\\nand M’s results. For CAN FILTER hints, the planner will generate a\\ntree where a model H predicated on CONDITIONED ON is inserted\\nbefore model M. All inputs to M are predicated on H.\\nEnumerating Plans. To ensure all plans have been generated\\nusing hints, VIVA’s planner analytically computes the number of\\nexpected plans based on the number of models and hints. The stan-\\ndard way of generating alternate query plans is to permute the\\npredicates of independent models. Let 𝑁 be the number of indepen-\\ndent models at depth 𝑖. The number of valid plans is the product\\nof the permutations of those models: \\uf0ce𝑖 −1\\n𝑗=1 N𝑗 ! For example, if a\\nplan has three independent models A, B and C, there are six plans\\nrepresenting the permutations of these models (ABC, BAC, etc.).\\nWe also to consider cases where hints generate additional plans. A\\nCAN REPLACE hint will only generate an alternate plan if either\\none or both models of the hint appear in the plan. A CAN FILTER\\nhint will only generate an alternate plan if M appears in the plan\\nwhere H will be inserted into the plan.\\n\\n5.3 Canary Inputs\\n\\nVIVA takes as input the query plan, an accuracy requirement and\\na canary input video. A canary input is a short video the user\\nprovides that represents events they are querying in the dataset.\\n\\nHintML UDFExecutionProfilesPlanOptimizationAccuracyEstimation100%90%85%40%> 80%500400Cost Estimation300MINBest PlanHintValidation CREATE HINT AS ObjDetV1 CAN REPLACEObjDetV2 FALLBACK 0 Relational HintsidHINTPARAMETERSH1ObjDetV1 CAN REPLACE ObjDetV2FALLBACK 0H2FaceDet CAN FILTER FaceRecogCONDITIONED ON ['FACE'] .........PlanGenerationH1FROFRFDOH2FRFDOH1 + H2QueryPlanFRO...CandidatePlans2Valid133a3bOffline hint generationSELECT FaceRecog(frame) AS f, ObjDetV2(frame) AS o FROM video WHERE ...ACCURACY 0.90 QueryCanary Input Video\\x0cUsing small, representative canaries is common when tuning or\\ncalibrating computer vision, machine learning, and data mining\\nsystems before execution [35]. They have also been used by recent\\nvideo analytics systems [59]. Canary data is similar to sampling\\nduring pre-processing to optimize the query [7] except in our case\\nthe user explicitly defines the data used. Empirically, we found it\\nsuffices to use a canary with at least one occurence of the event\\nqueried and some amount of noise to generate true positives and\\ntrue negatives for calculating F1 score.\\n\\nVIVA runs the original query plan on this input to generate\\nground-truth labels. Alternate query plans are also run on the ca-\\nnary and their accuracies are computed with respect to the ground-\\ntruth labels. The canary needs to closely resemble but not necessar-\\nily match the type of events a user is looking for. VIVA uses this to\\nfind which alternate plans will closely resemble the original plan\\ni.e., have high accuracy. The canary is specifically provided by the\\nuser and not sampled from the input dataset because it serves as a\\nlabelled set of frames, similar to a validation set.\\n\\nIn the TV News analysis example from Section 4.2, a canary input\\ncould contain scenes of Bernie Sanders upset in an interview with\\nJake Tapper or, more generally, scenes of two people being inter-\\nviewed with one person reacting angrily. A poor canary has events\\nfor which the models in the original query plan would have low\\naccuracy (e.g., a basketball player dunking). In this work, canaries\\nare manually selected to be representative of the query. In future\\nwork, we plan to explore techniques to automate the selection of\\ncanaries or notify users when their canary is not representative.\\n\\n5.4 Selecting Plans to Meet User Requirements\\nThe planner produces 𝑋 plans P = {𝑃1, 𝑃2, · · · 𝑃𝑋 }. A plan 𝑃𝑥\\nconsists of 𝑁 ordered models M = {𝑀1, 𝑀2, · · · 𝑀𝑁 }. The goal of\\nVIVA’s optimizer is to select the fastest or most price-efficient plan\\nP∗ that satisfies a user’s accuracy requirement A. We now discuss\\nhow VIVA’s optimizer selects this plan.\\nEstimating Plan Accuracy. Relational hints enable users to ex-\\npress knowledge to the query optimizer to evaluate alternate query\\nplans. Using hints, the planner may generate query plans that trade-\\noff accuracy for performance. VIVA provides users the ability to\\nexpress an accuracy target the optimizer must still meet in selecting\\nthe plan with the lowest cost. Step 3a of Figure 3 shows this step\\nof the optimizer estimating accuracy for each plan.\\n\\nWe next detail our approach to accuracy estimation where we\\nuse common techniques from recent work for using the original\\nmodels to generate ground-truth labels [25]. We use F1 score to\\nestimate accuracy [3, 5, 63], and compute an F1 score per plan\\n(not per model). To estimate each plan’s accuracy, VIVA first runs\\nthe original models and candidate models over the canary input’s\\nframes and stores these results in a table. During query optimization,\\nVIVA queries the table only with each plan’s predicates to produce\\na final set of labels, 𝑅. This eliminates the need to run the model\\nagain for each plan. The results from the user’s initial query plan\\nare used as the ground-truth labels 𝑅𝑡𝑟𝑢𝑡ℎ. Finally, the candidate\\nplan’s F1 score is computed using 𝑅𝑡𝑟𝑢𝑡ℎ and 𝑅.\\nSelectivity and Cost Estimation. The last step in plan selec-\\ntion, Step 3b of Figure 3, is to estimate cost. VIVA determines\\nhow many frames 𝑓𝑖 a model 𝑀𝑖 needs to process. This is based on\\n\\nthe selectivity 𝑠𝑖 −1 of the upstream model 𝑀𝑖 −1 and is given by:\\n𝑓𝑖 = 𝑀𝑖 −1 × 𝑠𝑖 −1. We use a standard approach of estimating selec-\\ntivity: VIVA samples a number of frames from the input dataset.\\nWe sample frames at a fixed rate from the input dataset similar to\\nprior work; other techniques for sampling can also be used [3, 38].\\nVIVA estimates selectivity independently for each model.\\n\\n(𝐵). For each new model, VIVA profiles 𝐿𝑀𝑖\\n𝐻 𝑗\\n\\nVIVA’s cost model is designed to support arbitrary backend\\nhardware platforms and models with arbitrary batch sizes. The\\nlatency of executing 𝑀𝑖 on hardware platform 𝐻 𝑗 for a batch of 𝐵\\ninputs is 𝐿𝑀𝑖\\n(𝐵) once\\n𝐻 𝑗\\nand stores its value for future cost estimations. If there is a data\\ntransfer time associated with a particular platform like the GPU,\\nVIVA will profile transferring different batches of frames and builds\\na model to estimate transferring any number of frames. Profiling\\nand building the model is a one-time, offline step. When estimating\\nthe cost, VIVA includes the data transfer time estimated by the\\nmodel as part of the overall plan cost. For 𝑉 different hardware\\nplatforms, there are up to 𝑁 𝑉 different hardware configurations\\nthat models in 𝑃𝑥 can run on K𝑃𝑥 = {H 1\\n, H 2\\n}. A\\n𝑃𝑥\\n𝑃𝑥\\nhardware configuration H𝑐\\n= {𝐻1, 𝐻2, . . . , 𝐻𝑁 } corresponds to a\\n𝑃𝑥\\nmodel 𝑀𝑖 in plan 𝑃𝑥 running on hardware platform 𝐻𝑖 . Then, the\\nestimated cost of running 𝑃𝑥 with H𝑐\\n𝑃𝑥 is:\\n𝑁\\n∑︁\\n\\n, . . . , H 𝑁 𝑉\\n𝑃𝑥\\n\\n𝐶 (𝑃𝑥 , H𝑐\\n𝑃𝑥\\n\\n) = 𝐿𝑇 𝑟𝑎𝑖𝑛 +\\n\\n𝐿𝑀𝑖\\n𝐻𝑖\\n\\n(𝐵) × (𝑓𝑖 /𝐵)\\n\\n𝑖=1\\n\\n𝐿𝑇 𝑟𝑎𝑖𝑛 is the time to train models specific to the query. If models are\\ntrained in parallel, 𝐿𝑇 𝑟𝑎𝑖𝑛 is the maximum time to train all models.\\nIf models are trained sequentially, 𝐿𝑇 𝑟𝑎𝑖𝑛 is the sum of times to train\\nall query-specific proxy models [25, 28]. If all models are available,\\n𝐿𝑇 𝑟𝑎𝑖𝑛 = 0. The cost-optimal hardware configuration for models in\\n𝑃𝑥 is:\\n\\nH ∗\\n\\n𝑃𝑥 = arg min\\n\\nH𝑐\\n\\n𝐶 (𝑃𝑥 , H𝑐\\n𝑃𝑥\\n\\n)\\n\\n𝑃𝑥 ∈ K𝑃𝑥\\nFinally, let 𝐴𝑥 be the estimated accuracy for 𝑃𝑥 . The cost-optimal\\nplan, P∗, among plans satisfying the user accuracy requirement is:\\n𝐶 (𝑃𝑥 , H ∗\\n𝑃𝑥\\n\\n), 𝑠.𝑡 . 𝐴𝑥 ≥ A\\n\\nP∗ = arg min\\n𝑃𝑥 ∈ P\\n\\nA user can parameterize the query with 3 targets: performance,\\ncheapest price, or best performance per dollar. For performance,\\nVIVA will return the fastest plan given the hardware available.\\nFor cheapest price, VIVA will return the cheapest plan based on\\nestimated latency multiplied by the cost per hour. For best perfor-\\nmance per dollar, VIVA will return the plan and target platform that\\ndelivers the highest end-to-end performance at the lowest price.\\n\\n5.5 Plan Pruning\\n\\nAs the complexity of queries increases with more models, pred-\\nicates, and hints, the number of possible plans generated grows\\nexponentially. This increases VIVA’s query optimization latency\\nbecause of the larger search space to consider given plans generated\\nusing hints. We apply several heuristics and pruning techniques to\\nlimit the search space and provide fast query optimization.\\n\\nAt hint validation, VIVA finds any model that is more expensive\\nthan the original model in the user’s query and removes it before\\nthe plan generation step. Using these models would generate plans\\nthat are strictly more expensive than the original query. During plan\\n\\n\\x0cTable 2: Queries, Datasets, Predicates, and Validated Hints Per Query.\\n\\nApplication\\n\\nDataset\\n\\nQuery Description\\n\\nPredicates\\n\\n# Hints\\n\\nTraffic\\nNews\\nSports\\nBias\\n\\nJackson square traffic camera [25, 28]\\n“Big three news” broadcasts [2, 20]\\nNBA games [21, 52]\\nCasual conversations dataset [17]\\n\\nCars turning left with people in intersection at night\\nJake Tapper interviewing angry Bernie Sanders\\nLeBron James dunks\\nNon-white females over the age of 19\\n\\ntime of day = night ∧ object = (people & car) ∧ object track\\nemotion = angry ∧ count(object = people) = 2 ∧ face = (Sanders & Tapper)\\naction = dunking basketball ∧ face = James\\nage > 19 ∧ race != non-white ∧ gender = female\\n\\n7\\n7\\n2\\n3\\n\\ngeneration, VIVA eliminates redundant calls to models that could\\nbe a result of CAN REPLACE hints and push predicates down to the\\nfirst call of the model. Thus, duplicate plans are eliminated which\\ncan occur as a result of hints replacing interchangeable models.\\n\\nOur pruning approach is akin to recent work in video analytics\\noptimization, CORE [63], which uses branch-and-bound to evaluate\\nthe plan cost after each model. CORE only retains plans that are\\nlikely to have a lower cost than the best plan found thus far. VIVA\\nprunes a plan if 1) the accuracy requirement was already met with\\na lower accuracy model and the current plan uses a higher accuracy\\nmodel, 2) the accuracy requirement was not met with a higher\\naccuracy model and the current plan uses a lower accuracy model,\\nor 3) if a plan’s estimated cost exceeds the best complete plan’s cost\\nafter a given model, it is pruned.\\n\\nVIVA transparently applies these heuristics and pruning tech-\\nniques which significantly limits the search space making query\\noptimization fast. In future work, we plan to investigate other prun-\\ning techniques. For example, we can use statistics about CAN FIL-\\nTER hints and their historical accuracy when generating plans.\\nHistorically low accuracy CAN FILTER hints can be pruned.\\n\\n6 IMPLEMENTATION\\n\\nVIVA is built on top of Spark [64] where users express queries using\\nUDFs and predicates in SQL or Spark’s dataframe API in Python. We\\nuse Spark’s execution engine for UDF execution and take advantage\\nof its optimizer for structured query optimizations. Video ingestion\\nand indexing uses FFmpeg [11]. Frames are stored as raw byte\\narrays in a PySpark DataFrame to enable data pipelining.\\n\\nWe use pretrained PyTorch models for applications such as action\\nrecognition, object detection, and facial recognition [10, 41, 43, 45],\\nand TensorFlow for bias analysis and emotion recognition [1, 53].\\nComputer vision models to detect day/night scenes, motion detec-\\ntion, and similarity detection are implemented using OpenCV [4].\\nFor the day/night scene detector, we use Scikit-learn’s Support Vec-\\ntor Machine (SVM) [50] implementation which we trained on 240\\nimages of day/night frames from traffic camera feeds [25].\\n\\nBy giving users the ability to express relationships across a wide\\nrange of models, hints naturally capture several existing video ana-\\nlytics optimizations. We use cheaper, less accurate object detection\\nmodels to represent techniques like BlazeIt [25] that propose train-\\ning low accuracy purpose-built models. We implement three layer\\nsharing models for race, gender, and age detection, using on Deep-\\nFace [51], that share a common set of layers like those produced by\\nMainstream [24]. CAN FILTER hints can be used to relate cheap\\nbinary classifiers such as those produced using techniques like\\nProbabilistic Predicates [36] or CORE [63]. We use models of simi-\\nlar computational footprints to represent these optimizations. We\\nuse TASTI [28] to generate candidate models for CAN REPLACE\\nor CAN FILTER hints. We use a pre-trained ResNet18 embedding\\nmodel. Unless otherwise noted, these indexes are trained and avail-\\nable at query time.\\n\\n7 EVALUATION\\n\\nWe now evaluate VIVA using the queries from Table 2. In all cases,\\nthe query plans benefit from the execution engine’s query optimizer\\nwhich applies standard structured query optimizations where pos-\\nsible. We deployed VIVA on Google Cloud Platform (GCP). We\\nuse a n1-highmem-16 instance (16 vCPUs, 104 GB of DRAM). This\\ninstance features Intel Xeon E5-2699 v4 CPUs operating at 2.20GHz,\\nUbuntu 20.04 with 5.15.0 kernel. For GPU experiments, we consider\\nn1-highmem-16 instances with an NVIDIA T4 and a V100.\\nQueries and Datasets. Table 2 shows the queries, datasets, predi-\\ncates, and total number of validated hints used to evaluated VIVA.\\nAll queries are complex: each with multiple models and predicates\\nand represent a range of different applications. TV News analysis is\\nbased on queries used to explore a decade of US cable news [12, 20].\\nSports analysis is commonly used for game planning, as well as for\\ncreating highlight reels [21]. Traffic analysis is used for landscaping\\nand autonomous vehicle training [25, 26, 32]. Bias analysis is used\\nto assess and detect model bias from training data [17].\\n\\nWe consider two inputs: the event searched is present in the\\nvideo — Event Present, and no instances of the event are in the\\nvideo — Event not Present. As shown in Table 2, these inputs\\ncome from the same dataset. All videos are one hour long, 360p,\\nand are processed at 1 FPS. The framerate we use is chosen to be\\nconsistent with prior work [25]. The canary input is 15 seconds.\\nWe use F1 score for accuracy. Also consistent with prior work,\\nselectivity estimation is performed over 3% of the input frames [6].\\nRelational Hints. Table 3 shows example tasks, models, and hints\\nwe use in our evaluation. In total, we use 19 different hints — 11\\nCAN REPLACE, 4 CAN REPLACE with FALLBACK ENABLED, and\\n4 CAN FILTER— across 30 different models. We capture several\\nsources of domain knowledge in our choice of hints.\\nModel Variants. Using a smaller object detection as a replacement\\nfor a larger model is a classic example of a model variant where the\\nmodels have the same output and classes but have different model\\narchitectures e.g., SmallObjDet CAN REPLACE LargeObjDet. For\\nlayer sharing models, a user specifies the suffix layers to run in the\\nmodel relationship e.g., RaceID CAN REPLACE SuffixRaceID. VIVA\\nautomatically determines whether it is worthwhile to execute the\\ncombination of prefix and suffix layers.\\nProxy Models. Cheaper, less accurate models generated using TASTI\\ncan be used as a replacement for more expensive ones like face\\ndetection and action detection. To indicate that these models should\\nuse the larger models when a label cannot be produced, we set\\nFALLBACK ENABLED for CAN REPLACE hints.\\nArea Expertise. Classical computer vision techniques can be used\\nby practitioners and expressed as model relationships. For example,\\n\\n\\x0cTask\\n\\nModels\\n\\nRelational Hints\\n\\nTable 3: Tasks, Models, and Sample Relational Hints.\\n\\nEmotion Detection\\n\\nMTCNN Emotion Detection, HAAR Emotion Detection, TASTI Emotion Detection\\n\\nObject Detection\\n\\nImage Classification\\nFacial Recognition\\nRace Identification\\nAction Recognition\\nDay/Night Detection\\n\\nSmall Object Detection, Large Object Detection,\\nObject Similarity Detection, Motion Detection\\nResNet50 Image Classification, ResNet18 Quantized Image Classification\\nFace Recognition, TASTI Face Recognition\\nRaceID, Suffix RaceID\\nAction Recog, Action Similarity Recog\\nPixel Brightness Detect, SVM for Day/Night Detection\\n\\nMTCNNEmoDet CAN REPLACE HAAREmoDet,\\nTASTIEmoDet CAN REPLACE MTCNNEmoDet FALLBACK ENABLED\\nSmallObjDet CAN REPLACE LargeObjDet, ObjSimDet CAN FILTER LargeObjDet,\\nMotDet CAN FILTER LargeObjDet CONDITIONED ON [‘motion’]\\nQImgCls CAN FILTER LargeObjDet\\nTASTIFaceRecog CAN REPLACE FaceRecog FALLBACK ENABLED\\nRaceID CAN REPLACE SuffixRaceID\\nActionSimDet CAN FILTER ActionRecog\\nPixelBriDet CAN REPLACE SVM\\n\\n7.1 Improving Query Performance\\n\\nWe first explore how VIVA uses hints to improve performance\\nover predicate reordering with an accuracy requirement of 90%.\\nFigure 4 shows the performance for each query for Event Present\\n(Figure 4a) and Event not Present (Figure 4b). The latencies\\npresented are inclusive of query optimization time. Table 4 shows\\nthe plan UpperPR uses and the best plan VIVA uses and its accuracy.\\nTraffic Analysis. For Event Present, UpperPR filters by time of\\nday, objects, and object tracking. EVA and BestPR filter by time of\\nday last. Since Event Present is all night scenes, no filtering occurs\\nwith the SVM day/night detection. VIVA uses a pixel brightness\\ndetection and a faster object detection model than EVA since its\\naccuracy estimator determines it can use what EVA considers a\\n“low” accuracy model. This enables VIVA to improve performance\\nby 4.8× over the baselines. For Event not Present, VIVA and EVA\\nfirst filter by time of day since this input is all day scenes. VIVA is\\nslightly faster (1.2×) because it uses the pixel brightness detection.\\nUpperPR runs the time of day detection last, which leads to a 16.6×\\ndrop in performance compared to VIVA.\\nNews Analysis. For Event Present, UpperPR first filters by emo-\\ntion, which is the least efficient since this expensive model must\\nprocess all frames. EVA and BestPR first filter by faces — a faster\\nmodel — before doing object and emotion detection, respectively.\\nVIVA uses a faster object detection (which EVA would classify as\\nlow accuracy), along with a TASTI-trained model for emotion detec-\\ntion. The TASTI-trained emotion detection is backed by the HAAR\\nemotion detection. This improves performance 4.8× over UpperPR,\\nand 1.3× over EVA and BestPR. For Event not Present, VIVA uses\\nobject similarity detection as a result of a CAN FILTER hint.\\nSports Analysis. For Event Present, all baselines use the same two\\nmodels, with EVA and BestPR benefiting from predicate reordering.\\nVIVA uses a TASTI-trained action detection backed by the original\\naction detection model. This enables VIVA to improve performance\\nby 1.5× over UpperPR, and 1.2× over EVA and BestPR. For Event\\nnot Present, VIVA uses a similarity detection for detecting dunks\\nfrom a reference image. This improves performance up to 2.5×.\\nBias Analysis. For Event Present, VIVA uses a plan with common\\nprefix layer models for race and age detection, specified using a\\nCAN REPLACE hint. The common layers are run once and reused\\nfor the two suffix models. This improves performance by 1.5× over\\nUpperPR, and matches the performance of EVA and BestPR. For\\nEvent not Present, VIVA does not use the common prefix layer\\nmodels, since the gender detection model can filter the majority of\\n\\n(a) Event Present in frames.\\n\\n(b) Event not Present in frames.\\nFigure 4: Query Speedup Relative to UpperPR.\\n\\na pixel brightness detector can be used as a replacement for an\\nSVM model trained to detect whether a frame is from day or night\\n(PixelBriDet CAN REPLACE SVM). Performing similarity detection\\nto find frames similar to a reference image can be used as a replace-\\nment for detecting actions. In the case of the Traffic analysis query,\\nsince the analyzer is querying static camera feeds, they can infer\\nthat detecting cars and people can be cheaper using motion detect.\\nThey can relate motion detection to object detection as: MotDet\\nCAN FILTER LargeObjDet CONDITIONED ON [‘motion’].\\nWe compare VIVA to the following baselines:\\n• Upper Bound Predicate Reorder (UpperPR): this is the worst-\\ncase latency of predicate reordering for a given accuracy require-\\nment if a system does not support selectivity and cost estimation\\nfor ML UDFs which is common in today’s execution engines.\\n• Best Predicate Reorder (BestPR): this represents what a\\nuser can expect if a video analytics system is able to do selectivity\\nand cost estimation for ML UDFs to find the lowest latency\\nordering given an accuracy requirement.\\n\\n• EVA: a recently-proposed, state-of-the-art video analytics sys-\\ntem whose optimizer makes model and predicate reordering\\nselections given a fixed accuracy [62]. Users specify a model’s\\naccuracy using coarse-grained indicators: low for accuracies 80%\\nand below, medium for accuracies [80%, 90%), and high for accura-\\ncies 90% and above. During query optimization, EVA selects each\\nmodel to use separately based on the plan accuracy requirement.\\n\\nTrafficNewsSportsBias0246Query Speedup1.01.01.01.01.03.81.21.51.03.81.21.54.84.81.51.5UpperPRBestPREVAVIVATrafficNewsSportsBias01020Query Speedup1.01.01.01.014.24.61.81.614.24.61.81.616.68.22.51.5UpperPRBestPREVAVIVA\\x0cTable 4: Best Plan Identified by VIVA. PR: Predicate Reorder, RP: CAN REPLACE, RPF: CAN REPLACE with FALLBACK ENABLED, FT: CAN FILTER.\\n\\nApplication\\n\\nOriginal Plan\\n\\nBest Hint Plan: ∃: Event Present, \\uf09a: Event not Present\\n\\nAccuracy\\n\\nTraffic\\n\\nNews\\n\\nSports\\n\\nBias\\n\\nTimeOfDay ∧ Object ∧ ObjectTrack\\n\\nEmotion ∧ Object ∧ Face\\n\\nAction ∧ Face\\n\\nAge ∧ Gender ∧ Race\\n\\n∃: RP(Object) ∧ ObjectTrack ∧ RP(TimeOfDay)\\n\\uf09a: RP(TimeOfDay) ∧ RP(Object) ∧ ObjectTrack\\n∃: RP(Object) ∧ Face ∧ RPF(Emotion) ∧ RP(Emotion)\\n\\uf09a: FT(Object) ∧ Object ∧ Face ∧ Emotion\\n∃: Face ∧ RPF(Action) ∧ Action\\n\\uf09a: FT(Action) ∧ Action ∧ Face\\n∃: Gender ∧ RP(Race) ∧ RP(Age)\\n\\uf09a: Gender ∧ Age ∧ Race\\n\\n100%\\n100%\\n91%\\n91%\\n100%\\n90%\\n100%\\n100%\\n\\nVIVA spends on average 20% of execution time on query opti-\\nmization. This is in line with recently released systems like FiGO [6],\\nMIRIS [3], and Jellybean [58] where 20%-25% of query execution is\\nspent on optimization. Pruning eliminates on average 70% of plans\\nfor 3 out of 4 queries. The Sports application does not benefit from\\npruning because of the small number of hints used and relatively\\nsmall number of additional plans generated. Without pruning, the\\nNews analysis query’s optimization time is 2.1× higher. Query exe-\\ncution time represents the majority of the time for all queries: 80%\\non average, up to 84%. For larger inputs assuming the same query,\\nthe time spent on query execution will grow while query optimiza-\\ntion stays constant. Lastly, query optimization time varies only by\\nup to 50% across all queries despite the number of plans differing\\nby up to 72×. This shows VIVA can scale as queries become more\\ncomplex and the number of hints grows.\\n\\n7.3 Performance Impact of Hint Types\\n\\nWe next analyze what hints improved performance by ablating the\\nregistered hints VIVA applies. We use the News and Traffic queries.\\nFor each query, we consider each available hint type separately (RP:\\nCAN REPLACE, RPF: CAN REPLACE with FALLBACK ENABLED, FT:\\nCAN FILTER) and VIVA using all available hints). Traffic analysis\\nhas no CAN REPLACE with FALLBACK ENABLED.\\nNews Analysis. Figure 5a shows the ablated performance for Event\\nPresent and Event not Present. In both cases, the best perfor-\\nmance comes from using a mix of hints. For Event Present, RP\\nuses a faster object detect. RPF uses TASTI-trained models for emo-\\ntion and object detect but uses the more expensive object detection\\nas a fallback model. Interestingly, VIVA selects a different predi-\\ncate ordering in each case: object detection runs first for RP while\\nface detection runs first with RPF. RP is faster than EVA and BestPR\\nsince RP identifies a faster but lower accuracy object detection that\\ncan meet the accuracy requirement compared to the high accuracy\\none used by EVA. FT uses the same plan as EVA and BestPR since\\nusing CAN FILTER hints do not meet the accuracy requirement.\\nFor Event not Present, VIVA uses an object similarity detection\\nmodel to improve performance.\\nTraffic Analysis.\\nFigure 5b shows the ablated performance for\\nEvent Present and Event not Present. For both inputs, the\\nbest plan that meets the accuracy requirement is to use only CAN\\nREPLACE hints. For Event Present, FT again picks the same plan\\nas EVA since the motion detect model specified using the CAN\\nFILTER hint does not meet the accuracy requirement. Plans are\\ncloser in performance for Event not Present since filtering by\\n\\n(a) News analysis.\\n\\n(b) Traffic analysis.\\nFigure 5: Query Latency when Ablating Hints. RP: CAN REPLACE,\\nRPF: CAN REPLACE with FALLBACK ENABLED, FT: CAN FILTER.\\n\\nTable 5: Query Optimization Latencies for Figure 4a Queries.\\n\\nApplication\\n\\n# Plans w/o\\nPruning\\n\\n# Pruned\\nPlans\\n\\nQuery Opt.\\n(% Total)\\n\\nQuery Exec.\\n(% Total)\\n\\nTotal\\n\\nTraffic\\nNews\\nSports\\nBias\\n\\n60\\n432\\n6\\n42\\n\\n17\\n25\\n6\\n24\\nAverage\\n\\n92s (17%)\\n116s (28%)\\n130s (18%)\\n88s (16%)\\n107s (20%)\\n\\n453s (83%)\\n302s (72%)\\n592s (82%)\\n473s (84%)\\n455s (80%)\\n\\n545s\\n418s\\n722s\\n561s\\n562s\\n\\nframes. VIVA is slightly slower in this case (1.1×) compared to EVA\\nand BestPR since both run the same plan but VIVA additionally\\nperforms accuracy estimation. Performance is similar across the\\nboard for this query because the original models and the CAN\\nREPLACE suffix models have similar performance.\\n\\n7.2 Query Optimization Latency\\n\\nWe next evaluate VIVA’s query optimization latency. During query\\noptimization, VIVA estimates hint-generated plan accuracies and\\nselectivities. This time increases with the number of independent\\nquery predicates and the number of applicable hints per query.\\nTable 5 shows the absolute and relative time breakdowns for query\\noptimization and execution using Event Present (a one hour input)\\nwith the 15 second canary input.\\n\\nBestPREVAFTRPRPFVIVA0200400600800Latency (sec)Event PresentEvent not PresentBestPREVAFTRPVIVA050010001500Latency (sec)268226822729Event PresentEvent not Present\\x0c(a) News analysis.\\n\\n(b) Traffic analysis.\\nFigure 6: Query Latency as Accuracy Requirements Vary.\\n\\nFigure 7: Query Latency of News Analysis with Training Latency.\\nArrow: TASTI index creation.\\n\\ntime of day first is best in all cases. RP and VIVA use pixel brightness\\ndetection to improve performance over EVA and FT.\\n\\n7.4 Trading Off Latency and Accuracy\\n\\nWe now evaluate how VIVA automatically chooses the highest\\nperformance plan as the accuracy requirement varies. Using the\\nNews and Traffic analysis queries, we sweep accuracy requirements\\nfrom 60% to 95% and use the Event Present input. EVA uses low\\naccuracy models for requirements 80% and below, medium accuracy\\nmodels for [80%, 90%) requirements, and high accuracy models for\\nrequirements 90% and above.\\nNews Analysis. Figure 6a shows the results of varying accuracy\\nwhere, aexpected, more stringent accuracy requirements also result\\nin a decrease in performance. For accuracy requirements of 80%\\nand 90%, VIVA selects the plan shown in Table 4. For accuracy\\nrequirement of 95%, VIVA uses the faster object detection model,\\nbut no longer uses TASTI-trained models or the faster emotion\\ndetect. However, the performance is similar to the plan shown in\\nTable 4. The performance difference between these plans is 1.8×.\\nVIVA outperforms EVA for all accuracy requirements (up to 1.5×)\\nsince it identifies the best performing combination of hints that\\nmeet the accuracy requirements. Indeed, for accuracy requirements\\n\\nof 90% and 95%, VIVA uses faster models that meet the accuracy\\nrequirements, while EVA uses the slower, high accuracy models.\\nTraffic Analysis. Figure 6b shows the results where VIVA identifies\\nthe plan shown in Table 4 meets all accuracy requirements, and\\nhence uses the same plan in all cases. EVA has similar performance\\nfor low accuracies, but uses increasingly larger object detection\\nmodels as the accuracy requirement becomes more stringent. This\\nenables VIVA to improve performance over EVA by up to 4.8×.\\n\\nIt can be difficult to know when models should be updated or\\nre-trained using optimizations like TASTI and BlazeIt. By selecting\\nto use these models for lower accuracy requirements, but not for\\nmore stringent ones, VIVA can guide training decisions.\\n\\n7.5 Impact of Training and Indexing\\n\\nWe next investigate how plan selection can be impacted by the need\\nto construct an index or train a model for replacement or filtering at\\nquery time. This results in additional cost from training or indexing\\nto plans that include hints with CAN FILTER or CAN REPLACE\\nwith FALLBACK ENABLED. We use the setup from Section 7.1 and\\nfocus on the News analysis query. We vary the training latency\\nfrom 0sec (already exists) to 100sec in increments of 10sec.\\n\\nFigure 7 shows the end-to-end latency (y-axis) for the two base-\\nlines and VIVA as the training latency varies (x-axis). We note VIVA\\nmatches or outperforms EVA even when spending up to a minute\\nfor training, since it spends less time on query execution. The arrow\\nshows the case for creating a TASTI [28] index, which is on the\\norder of seconds if frame embeddings are available. Proxy models\\ncan be trained in tens to hundreds of seconds [25], which can still be\\nworth this additional upfront cost. Furthermore, caching this model\\nmeans VIVA only incurs a one-time training cost that can benefit\\nfuture queries as well. As noted in Section 5.4, VIVA considers this\\ntraining time when selecting the best plan to execute.\\n\\n7.6 Optimizing Across Hardware Platforms\\n\\nWe now evaluate VIVA’s ability to generate and compare plans\\nfor different hardware platforms. We consider three instances: a\\nstandalone n1-highmem-16 (CPU), a n1-highmem-16 with a T4\\nGPU, and a n1-highmem-16 with a V100 GPU. We use the GCP\\npricing for each: 0.66 $/hr for CPU, 0.91 $/hr for T4, and 2.40 $/hr\\nfor V100 [14]. We use out-of-the-box GPU implementation and\\nfallback to CPU implementations if not available on the GPU. We\\nstudy three optimization goals: performance (fastest plan), cheapest\\nprice, or best end-to-end performance per dollar. We use the Traffic\\nand TV News queries on the Event Present input.\\n\\nFigure 8 shows the results where VIVA optimizes for perfor-\\nmance with the final dollar cost of the plan annotated. For Traffic\\nanalysis, the T4 GPU is 1.8× faster than the CPU while being 30%\\ncheaper. While the T4 GPU instance is more expensive, the faster\\nexecution means the instance can be provisioned for less time. Sim-\\nilarly for News analysis, execution with the T4 is ∼2× faster and\\n42% cheaper. In both cases, the V100’s performance improvement\\nof ∼2× does not outweigh its high cost, 1.8× more expensive com-\\npared to a CPU. As shown in Table 6, the optimizer chooses the\\nsame plan in all cases since object detection can be significantly\\naccelerated on GPU compared to running on CPU and the latency\\nis the only variable when estimating cost. In this study, we do not\\n\\n60%70%80%90%95%0200400600800Latency (sec)EVAVIVA60%70%80%90%95%0100020003000Latency (sec)EVAVIVA020406080100Model Training Latency (sec)02004006008001000End-to-End Latency (sec)EVAVIVA\\x0cor used instead of VIVA’s execution engine to further accelerate\\nqueries. Several recent projects have also focused on optimizing as-\\npects of video retrieval from storage and how video data are stored\\nand decoded [8, 16, 30, 61]. These techniques are also important for\\nend-to-end efficiency but are complimentary to this paper’s focus.\\nFunctional Dependencies. Functional dependencies [22, 34] help\\ndatabase designers automatically determine the relation of one\\nattribute to another. However, existing work is limited to struc-\\ntured data that can be easily analyzed to determine relationships.\\nVideo analytics queries execute expensive DNNs over unstructured\\nrecords, which makes it infeasible to infer the relationships without\\nfirst materializing the results. Hints enable VIVA to consider addi-\\ntional query plans that can improve performance and cost without\\nhaving to first materialize the results.\\nSpecifying Domain Knowledge. Providing extra knowledge to\\na system to improve query execution is an idea with roots in the\\nearly days of query processing. Hints most closely resemble early\\nwork in semantic integrity constraints [33], and more generally\\nhints in existing database systems, such as MicrosoftSQL hints [37]\\nand MySQL hints [39]. A key difference from domain knowledge\\nfor structured data is that ML models are probabilistic and require a\\nsystem to consider and provide accuracy guarantees. VIVA reasons\\nabout the accuracy impact on plans using hints.\\n\\n9 CONCLUSION\\n\\nIn this paper, we addressed the challenge of users having to manu-\\nally explore performance-accuracy tradeoffs across combinations\\nof optimizations in video analytics queries with multiple predicates.\\nWe proposed relational hints, a declarative interface to express ML\\nmodel relationships, informed by domain specific knowledge. Rela-\\ntional hints eliminate the need for users to manually rewrite their\\nqueries when a new model becomes available and manually reason\\nabout how the use and order of the various models available impact\\ntheir query’s performance and accuracy. To determine how and\\nwhen relational hints can be used to optimize queries, we designed\\nthe VIVA video analytics system. VIVA uses hints that are validated\\nfor each query to generate additional query plans using a formal\\nset of transformations, and selects the best performance plan that\\nmeets user accuracy requirements. Using relational hints, we show\\nthat VIVA over Spark improves performance up to 16.6× without\\nsacrificing accuracy for a range of complex queries.\\n\\nACKNOWLEDGEMENTS\\n\\nWe thank the Stanford Platform Lab and its affiliates (Cisco, Face-\\nbook, Google, Nasdaq, NEC, VMware, and Wells Fargo), the Open\\nPhilanthropy project, and Sutter Hill Ventures. We also thank af-\\nfiliates of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, and VMware—as well as Toyota Research Institute (“TRI”),\\nCisco, SAP, and the NSF under CAREER grant CNS-1651570. Any\\nopinions, findings, and conclusions or recommendations expressed\\nin this material are those of the authors and do not necessarily re-\\nflect the views of the NSF. TRI provided funds to assist the authors\\nwith their research but this article solely reflects the opinions and\\nconclusions of its authors and not TRI or any other Toyota entity.\\nFrancisco Romero was supported by a Stanford DARE Fellowship.\\n\\nFigure 8: VIVA Execution using the CPU, T4 GPU, and V100 GPU. La-\\ntencies presented are the fastest plans given the available hardware.\\n\\nTable 6: Hardware Platform Selection. Perf./$ normalized to CPU.\\nChosen HW is bolded. ∗Model executes on CPU if GPU selected.\\n\\nApp.\\n\\nTraffic\\n\\nNews\\n\\nOpt. Target (HW avail.)\\nPerf. (CPU, T4, V100)\\nCost (CPU, T4)\\nCost (CPU, V100)\\nPerf. (CPU, T4, V100)\\nCost (CPU, T4)\\nCost (CPU, V100)\\n\\nSelected Plan\\n\\nPerf./$\\n\\nRP(Obj.) ∧ ObjTrack ∧ RP(TimeOfDay)∗\\n\\nRP(Obj.) ∧ Face ∧ RPF(Emo.)∗ ∧ RP(Emo.)\\n\\n1.04\\n2.33\\n1.00\\n1.31\\n2.23\\n1.00\\n\\ninclude mixed precision models that could take advantage of half-\\nprecision units on the GPU. Such models can be defined as CAN\\nREPLACE hints and be considered during query optimization.\\n\\nTable 6 shows the results when VIVA optimizes for cost. VIVA\\nselects a different hardware platform depending on hardware avail-\\nability. Consistent with our previous results, when optimizing for\\ndollar cost, VIVA will favor a CPU plan if the accelerator available\\nis the V100 and favor the T4 over the CPU. When optimizing for\\nperformance per dollar, VIVA will choose the T4 plan since it is up\\nto 2.3× better than the plans for CPU and V100.\\n\\n8 RELATED WORK\\n\\nAccelerating Queries via Specialization. A large body of work\\nuses cheap approximations to accelerate specific classes of queries,\\nranging from selection [26, 27, 36, 63], aggregation [25], and aggre-\\ngation with predicates [29]. There is also work on using embedding\\nindexes as cheap approximations [19, 28]. VIVA is the first system to\\nprovide a general interface, hints, that captures these optimizations\\nand their impact on complex query performance and accuracy.\\nVideo Frame Sampling. Several projects have focused on decreas-\\ning the amount of data models need to process via dynamic sam-\\npling rates. MIRIS [3] executes object detection and object tracking\\nat reduced framerates and increases the framerate for low confi-\\ndence detections. ExSample [38] splits a video dataset into temporal\\nchunks and prioritizes processing chunks with higher probabilities\\nof finding a new object. It iteratively updates its estimates as more\\nframes are processed by leveraging an adaptive sampling algorithm\\nbased on Thompson sampling [49]. Depending on the query type,\\nvarying the sampling rate can affect the accuracy since lower sam-\\npling rates may lead to missed objects. The optimizations enabled\\nby hints are orthogonal to existing sampling techniques.\\nOptimizing ML Execution and Storage. Systems such as Scan-\\nner [42], VideoStorm [65], and Llama [48] have focused on optimiz-\\ning DNN execution by efficiently utilizing hardware resources for\\nexecution plans for video analytics. The scale-out and serverless\\ntechniques underpinning these systems are complementary to opti-\\nmization with hints. Hence these systems can be integrated into,\\n\\nTrafficNews0200400600Latency (sec)$0.10$0.07$0.07$0.05$0.18$0.12VIVA (CPU)VIVA (T4)VIVA (V100)\\x0cREFERENCES\\n[1] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-\\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,\\nBenoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan\\nYu, and Xiaoqiang Zheng. 2016. TensorFlow: A system for large-scale machine\\nlearning. In OSDI.\\n[2]\\nInternet Archive. 2022. TV News Archive. https://archive.org/details/tv.\\n[3] Favyen Bastani, Songtao He, Arjun Balasingam, Karthik Gopalakrishnan, Mo-\\nhammad Alizadeh, Hari Balakrishnan, Michael Cafarella, Tim Kraska, and Sam\\nMadden. 2020. MIRIS: Fast Object Track Queries in Video. In SIGMOD.\\n\\n[4] G. Bradski. 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools\\n\\n[5]\\n\\n(2000).\\nJiashen Cao, Ramyad Hadidi, Joy Arulraj, and Hyesoon Kim. 2021. THIA: Accel-\\nerating Video Analytics using Early Inference and Fine-Grained Query Planning.\\narXiv:2102.08481\\nJiashen Cao, Karan Sarkar, Ramyad Hadidi, Joy Arulraj, and Hyesoon Kim. 2022.\\nFiGO: Fine-Grained Query Optimization in Video Analytics. In SIGMOD.\\n[7] Surajit Chaudhuri, Bolin Ding, and Srikanth Kandula. 2017. Approximate query\\n\\n[6]\\n\\nprocessing: No silver bullet. In SIGMOD.\\n\\n[8] Maureen Daum, Haynes Brandon, Dong He, Amrita Mazumdar, and Magdalena\\nBalazinska. 2021. TASM: A Tile-Based Storage Manager for Video Analytics. In\\nICDE.\\n\\n[9] Maureen Daum, Enhao Zhang, Dong He, Magdalena Balazinska, Brandon Haynes,\\nRanjay Krishna, Apryle Craig, and Aaron Wirsing. 2022. VOCAL: Video Organi-\\nzation and Interactive Compositional AnaLytics. In CIDR.\\n\\n[10] Tim Esler. 2022. InceptionResNet Face Recognition in PyTorch. https://github.\\n\\ncom/timesler/facenet-pytorch\\n\\n[11] FFmpeg. 2022. FFmpeg. https://ffmpeg.org/.\\n[12] Daniel Y Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang, Anh Truong,\\nAvanika Narayan, Maneesh Agrawala, Christopher Ré, and Kayvon Fatahalian.\\n2019. Rekall: Specifying video events using compositions of spatiotemporal\\nlabels. arXiv:1910.02993\\n\\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX:\\n\\nExceeding YOLO Series in 2021. arXiv:2107.08430\\n\\n[14] Google. 2022. Google Compute Engine: GPUs Pricing. https://cloud.google.com/\\n\\ncompute/gpus-pricing.\\n\\n[15] Google. 2022. Tesseract OCR. https://github.com/tesseract-ocr/tesseract.\\n[16] Brandon Haynes, Maureen Daum, Dong He, Amrita Mazumdar, Magdalena\\nBalazinska, Alvin Cheung, and Luis Ceze. 2021. VSS: A Storage System for Video\\nAnalytics. In SIGMOD.\\n\\n[17] Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo,\\nand Cristian Canton Ferrer. 2021. Towards Measuring Fairness in AI: the Casual\\nConversations Dataset. arXiv:2104.02821\\n\\n[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn.\\n\\nIn ICCV. IEEE.\\n\\n[19] Wenjia He, Michael R. Anderson, Maxwell Strome, and Michael Cafarella. 2020.\\n\\n[20]\\n\\n[21]\\n\\n[22]\\n\\nA Method for Optimizing Opaque Filter Queries. In SIGMOD.\\nJames Hong, Will Crichton, Haotian Zhang, Daniel Y. Fu, Jacob Ritchie, Jeremy\\nBarenholtz, Ben Hannel, Xinwei Yao, Michaela Murray, Geraldine Moriba, Ma-\\nneesh Agrawala, and Kayvon Fatahalian. 2021. Analysis of Faces in a Decade of\\nUS Cable TV News. In SIGKDD.\\nJames Hong, Matthew Fisher, Michaël Gharbi, and Kayvon Fatahalian. 2021.\\nVideo Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition.\\nIn ICCV.\\nIhab F. Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboulnaga. 2004.\\nCORDS: Automatic Discovery of Correlations and Soft Functional Dependencies.\\nIn SIGMOD.\\n\\n[23] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An-\\ndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization\\nand training of neural networks for efficient integer-arithmetic-only inference.\\nIn CVPR.\\n\\n[24] Angela H Jiang, Daniel L-K Wong, Christopher Canel, Lilia Tang, Ishan Misra,\\nMichael Kaminsky, Michael A Kozuch, Padmanabhan Pillai, David G Andersen,\\nand Gregory R Ganger. 2018. Mainstream: Dynamic {Stem-Sharing} for {Multi-\\nTenant} Video Processing. In ATC.\\n\\n[25] Daniel Kang, Peter Bailis, and Matei Zaharia. 2019. BlazeIt: Optimizing Declara-\\ntive Aggregation and Limit Queries for Neural Network-Based Video Analytics.\\nIn PVLDB.\\n\\n[26] Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, and Matei Zaharia. 2017.\\nNoScope: optimizing neural network queries over video at scale. In PVLDB.\\n[27] Daniel Kang, Edward Gan, Peter Bailis, Tatsunori Hashimoto, and Matei Zaharia.\\n2020. Approximate Selection with Guarantees using Proxies. In PVLDB.\\n[28] Daniel Kang, John Guibas, Peter Bailis, Tatsunori Hashimoto, and Matei Zaharia.\\n2022. Semantic Indexes for Machine Learning-based Queries over Unstructured\\nData. In SIGMOD.\\n\\n[29] Daniel Kang, John Guibas, Peter Bailis, Yi Sun, Tatsunori Hashimoto, and Matei\\nZaharia. 2021. Accelerating Approximate Aggregation Queries with Expensive\\nPredicates. In PVLDB.\\n\\n[30] Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, and Matei Zaharia.\\n2021. Jointly optimizing preprocessing and inference for DNN-based visual\\nanalytics. In PVLDB.\\n\\n[31] Daniel Kang, Francisco Romero, Peter Bailis, Christos Kozyrakis, and Matei\\nZaharia. 2022. VIVA: An End-to-End System for Interactive Video Analytics. In\\nCIDR.\\n\\n[32] Fiodar Kazhamiaka, Matei Zaharia, and Peter Bailis. 2021. Challenges and Op-\\n\\nportunities for Autonomous Vehicle Query Systems. In CIDR.\\nJonathan J King. 1979. Exploring the Use of Domain Knowledge for Query Processing\\nEfficiency. Technical Report. Stanford University, Dept of Computer Science.\\n\\n[33]\\n\\n[34] Sebastian Kruse and Felix Naumann. 2018. Efficient Discovery of Approximate\\n\\nDependencies. In PVLDB.\\n\\n[35] Michael A Laurenzano, Parker Hill, Mehrzad Samadi, Scott Mahlke, Jason Mars,\\nand Lingjia Tang. 2016. Input responsiveness: using canary inputs to dynamically\\nsteer approximation. In PLDI.\\n\\n[36] Yao Lu, Aakanksha Chowdhery, Srikanth Kandula, and Surajit Chaudhuri. 2018.\\nAccelerating Machine Learning Inference with Probabilistic Predicates. In SIG-\\nMOD.\\n\\n[37] Microsoft. 2022. MicrosoftSQL Hints (Transact-SQL). https://docs.microsoft.\\n\\ncom/en-us/sql/t-sql/queries/hints-transact-sql?view=sql-server-ver15.\\n[38] Oscar Moll, Favyen Bastani, Sam Madden, Mike Stonebraker, Vijay Gadepally,\\nand Tim Kraska. 2022. ExSample: Efficient Searches on Video Repositories\\nthrough Adaptive Sampling. In ICDE.\\n\\n[39] MySQL. 2022. MySQL Optimizer Hints. https://dev.mysql.com/doc/refman/8.0/\\n\\nen/optimizer-hints.html.\\n\\n[40] Scoreboard OCR. 2022. Scoreboard OCR. https://scoreboard-ocr.com/start.\\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS.\\n\\n[42] Alex Poms, William Crichton, Pat Hanrahan, and Kayvon Fatahalian. 2018. Scan-\\n\\nner: Efficient Video Analysis at Scale. In SIGGRAPH.\\n\\n[43] PyTorch. 2022. 3D ResNet Video Classification in PyTorch. https://pytorch.org/\\n\\nhub/facebookresearch_pytorchvideo_resnet/\\n\\n[44] PyTorch. 2022. Image Classification: Models and Pre-Trained Weights. https:\\n\\n//pytorch.org/vision/stable/models.html#classification\\n\\n[45] PyTorch. 2022.\\n\\nObject Detection: Models and Pre-Trained Weights.\\n\\nhttps://pytorch.org/vision/stable/models.html#object-detection-instance-\\nsegmentation-and-person-keypoint-detection\\n\\n[46] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther\\nSchmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark\\nCharlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan\\nDeng, Greg Diamos, Jared Duke, Dave Fick, J. Scott Gardner, Itay Hubara, Sachin\\nIdgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jef-\\nfery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius,\\nColin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip\\nSequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei,\\nEphrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong,\\nPeizhao Zhang, and Yuchen Zhou. 2020. MLPerf Inference Benchmark. In ISCA.\\n[47] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. 2021.\\n\\nINFaaS: Automated Model-less Inference Serving. In ATC.\\n\\n[48] Francisco Romero, Mark Zhao, Neeraja J. Yadwadkar, and Christos Kozyrakis.\\n2021. Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video\\nAnalytics Pipelines. In SoCC.\\n\\n[49] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen.\\n\\n2017. A Tutorial on Thompson Sampling. arXiv:1707.02038\\n\\n[50] Scikit. 2022. Support Vector Classifier in Scikit. https://scikit-learn.org/stable/\\n\\nmodules/generated/sklearn.svm.SVC.html\\n\\n[51] Sefik Ilkin Serengil and Alper Ozpinar. 2021. HyperExtended LightFace: A Facial\\n\\nAttribute Analysis Framework. In ICEET.\\n\\n[52] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai\\nPhilipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: A GPU\\nCluster Engine for Accelerating DNN-Based Video Analysis. In SOSP.\\nJustin Shenk. 2022. Facial Expression Recognition.\\njustinshenk/fer\\n\\nhttps://github.com/\\n\\n[53]\\n\\n[54] TensorFlow. 2022. Signatures in TensorFlow Lite. https://www.tensorflow.org/\\n\\nlite/guide/signatures.\\n\\n[55] Ultralytics. 2022. Yolov5 Object Detection in PyTorch. https://github.com/\\n\\nultralytics/yolov5.\\n\\n[56] Paul Viola and Michael Jones. 2001. Rapid object detection using a boosted\\n\\ncascade of simple features. In CVPR.\\n\\n[57] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. 2021. You Only Learn\\nOne Representation: Unified Network for Multiple Tasks. arXiv:2105.04206\\n\\n\\x0c[58] Yongji Wu, Matthew Lentz, Danyang Zhuo, and Yao Lu. 2023. Serving and\\nOptimizing Machine Learning Workflows on Heterogeneous Infrastructures. In\\nPVLDB.\\n\\n[59] Ran Xu, Jinkyu Koo, Rakesh Kumar, Peter Bai, Subrata Mitra, Sasa Misailovic,\\nand Saurabh Bagchi. 2018. VideoChef: Efficient Approximation for Streaming\\nVideo Processing Pipelines. In ATC.\\n\\n[60] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng\\nDeng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua\\nLai. 2022. PP-YOLOE: An evolved version of YOLO. arXiv:2203.16250\\n\\n[61] Tiantu Xu, Luis Materon Botelho, and Felix Xiaozhu Lin. 2019. VStore: A Data\\n\\nStore for Analytics on Large Videos. In EuroSys.\\n\\n[62] Zhuangdi Xu, Gaurav Kakker, Joy Arulraj, and Umakishore Ramachandran. 2022.\\nEVA: A Symbolic Approach to Accelerating Exploratory Video Analytics with\\nMaterialized Views. In SIGMOD.\\n\\n[63] Zhihui Yang, Zuozhi Wang, Yicong Huang, Yao Lu, Chen Li, and X. Sean Wang.\\n2022. Optimizing Machine Learning Inference Queries with Correlative Proxy\\nModels. In PVLDB.\\n\\n[64] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion\\nStoica. 2010. Spark: Cluster computing with working sets.. In HotCloud.\\n[65] Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose,\\nParamvir Bahl, and Michael J Freedman. 2017. Live Video Analytics at Scale\\nwith Approximation and Delay-Tolerance. In NSDI.\\n\\n\\x0c\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d005f70f-aff7-4b3d-876e-15ff0dcea835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Optim', 'oss\\n\\n']\n",
      "['numer', 'ter\\n\\n']\n",
      "['Table', 'score']\n",
      "['Defin', 'r’s\\n\\n']\n",
      "['from ', 'dator']\n",
      "['Figur', 'Video']\n",
      "['Using', 'lan\\n\\n']\n",
      "['Table', 'le,\\n\\n']\n",
      "['Task\\n', 'AVIVA']\n",
      "['Table', 'esent']\n",
      "['(a) N', 'AVIVA']\n",
      "['or us', 'V100)']\n",
      "['REFER', '206\\n\\n']\n",
      "['[58] ', 'DI.\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "for line in text.split('\\x0c'):\n",
    "    if len(line) != 0:\n",
    "        print([line[:5], line[-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec5e25f-fc64-498e-a26d-66db6dee5501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Optim', 's sys']\n",
      "['tems.', 'knowl']\n",
      "['edge.', 'd exe']\n",
      "['cutio', 'nd ac']\n",
      "['cess ', 'video']\n",
      "['confe', 'or ca']\n",
      "['ble n', 'ly ac']\n",
      "['curat', 'nd de']\n",
      "['fine ', 's com']\n",
      "['plex ', 't per']\n",
      "['forma', 'fy in']\n",
      "['tuiti', '. Cur']\n",
      "['rent ', 'trade']\n",
      "['off u', 'analy']\n",
      "['sis q', 'erfor']\n",
      "['mance', ' tech']\n",
      "['nique', 's cal']\n",
      "['culat', 'order']\n",
      "['ings ', 'prove']\n",
      "['ment.', 'st ex']\n",
      "['plore', ' Pred']\n",
      "['icate', 'recog']\n",
      "['nitio', 'd cum']\n",
      "['berso', 'confi']\n",
      "['dence', 'minol']\n",
      "['ogy u', 'ation']\n",
      "['ship ', 'ot en']\n",
      "['force', 'signa']\n",
      "['ture ', ' esti']\n",
      "['mates', 'teris']\n",
      "['tics ', 'ssifi']\n",
      "['catio', 'archi']\n",
      "['tectu', 'a com']\n",
      "['mon s', ' orig']\n",
      "['inal ', 'e gen']\n",
      "['erate', ' stan']\n",
      "['dard ', 'depen']\n",
      "['dent ', 'he ca']\n",
      "['nary ', 'round']\n",
      "['truth', 'essar']\n",
      "['ily m', 'inter']\n",
      "['viewe', 'to ex']\n",
      "['press', 'trade']\n",
      "['off a', 'selec']\n",
      "['tion,', 'selec']\n",
      "['tivit', 'erfor']\n",
      "['mance', ' pred']\n",
      "['icate', ' tech']\n",
      "['nique', ' prun']\n",
      "['ing t', 'N FIL']\n",
      "['TER h', 'detec']\n",
      "['tion,', 't Vec']\n",
      "['tor M', 'o ana']\n",
      "['lytic', 'train']\n",
      "['ing l', ' Deep']\n",
      "['Face ', ' simi']\n",
      "['lar c', 'avail']\n",
      "['able ', 'e pos']\n",
      "['sible', 'predi']\n",
      "['cates', 'y emo']\n",
      "['tion,', 'detec']\n",
      "['tion.', 'place']\n",
      "['ment ', 'worst']\n",
      "['case ', 'quire']\n",
      "['ment ', 's sys']\n",
      "['tem w', 'ccura']\n",
      "['cies ', ' opti']\n",
      "['mizat', 'y exe']\n",
      "['cutio', 'imiza']\n",
      "['tion ', 'erfor']\n",
      "['mance', 'r emo']\n",
      "['tion ', 'predi']\n",
      "['cate ', ' base']\n",
      "['lines', 'erfor']\n",
      "['mance', '. Sim']\n",
      "['ilarl', 'e com']\n",
      "['pared', 'ng as']\n",
      "['pects', 'struc']\n",
      "['tured', ' addi']\n",
      "['tiona', ' manu']\n",
      "['ally ', ' Rela']\n",
      "['tiona', ' Face']\n",
      "['book,', 'nk af']\n",
      "['filia', 'ly re']\n",
      "['flect', 'U. La']\n",
      "['tenci', ' half']\n",
      "['preci', 'avail']\n",
      "['abili', 'aggre']\n",
      "['gatio', 'creas']\n",
      "['ing t', 'c sam']\n",
      "['pling', 'confi']\n",
      "['dence', 'r sam']\n",
      "['pling', ' Scan']\n",
      "['ner [', 'timiz']\n",
      "['ing D', ' opti']\n",
      "['mizat', ', Man']\n",
      "['junat', 'n, Mo']\n",
      "['hamma', 'Accel']\n",
      "['erati', 'rgani']\n",
      "['zatio', 'a, Ma']\n",
      "['neesh', 'g, An']\n",
      "['drew ', 'Multi']\n",
      "['Tenan', 'clara']\n",
      "['tive ', 'nd Op']\n",
      "['\\nport', 'n SIG']\n",
      "['MOD.\\n', ' Scan']\n",
      "['\\nner:', 'tance']\n",
      "['segme', ', Jef']\n",
      "['fery ', 'I.\\n\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "for line in text.split('-\\n'):\n",
    "    if len(line) != 0:\n",
    "        print([line[:5], line[-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc77bb09-a50f-4555-be1a-d7557e56694f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Video Analytics with Declarative Model Relationships\n",
      "\n",
      "Francisco Romero∗ Stanford University faromero@stanford.edu\n",
      "\n",
      "Daniel Kang Stanford University ddkang@cs.stanford.edu\n",
      "\n",
      "Johann Hauswald∗ Stanford University & Sutter Hill Ventures johannh@stanford.edu\n",
      "\n",
      "Matei Zaharia Stanford University matei@cs.stanford.edu\n",
      "\n",
      "Aditi Partap Stanford University aditi712@stanford.edu\n",
      "\n",
      "Christos Kozyrakis Stanford University christos@cs.stanford.edu\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "The availability of vast video collections and the accuracy of ML models has generated significant interest in video analytics systems. Since naively processing all frames using expensive models is impractical, researchers have proposed optimizations such as selectively using faster but less accurate models to replace or filter frames for expensive models. However, these optimizations are difficult to apply on queries with multiple predicates and models, as users must manually explore a large optimization space. Without significant systems expertise or time investment, an analyst may manually create an execution plan that is unnecessarily expensive and/or terribly inaccurate.\n",
      "\n",
      "We propose Relational Hints, a declarative interface that allows users to suggest ML model relationships based on domain knowledge. Users can express two key relationships: when a model can replace another (CAN REPLACE) and when a model can be used to filter frames for another (CAN FILTER). We aim to design an interface to express model relationships informed by domain specific knowledge and define the constraints by which these relationships hold. We then present the VIVA video analytics system that uses relational hints to optimize SQL queries on video datasets. VIVA automatically selects and validates the hints applicable to the query, generates possible query plans using a formal set of transformations, and finds the best performance plan that meets a user’s accuracy requirements. VIVA relieves users from rewriting and manually optimizing video queries as new models become available and execution environments evolve. We evaluate VIVA implemented on top of Spark and show that hints improve performance up to 16.6 (times) without sacrificing accuracy.\n",
      "\n",
      "PVLDB Reference Format: Francisco Romero, Johann Hauswald, Aditi Partap, Daniel Kang, Matei Zaharia, and Christos Kozyrakis. Optimizing Video Analytics with Declarative Model Relationships. PVLDB, 16(3): 447 - 460, 2022. doi:10.14778/3570690.3570695\n",
      "\n",
      "PVLDB Artifact Availability:\n",
      "\n",
      "The source code, data, and/or other artifacts have been made available at https://github.com/stanford-mast/viva-vldb23-artifact.\n",
      "\n",
      "∗Denotes equal contribution. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 16, No. 3 ISSN 2150-8097. doi:10.14778/3570690.3570695\n",
      "\n",
      "Video analytics, the ability to extract insights from video, is enabled by increasingly accurate machine learning (ML) models and access to large archives of professionally produced content or videos captured by devices like cellphones, security cameras, and videoconference systems. While we can already answer queries over videos like “have any cars passed this intersection that match an AMBER alert?”, several challenges remain before video analytics are as practical and as performant over analytics on structured data. For complex video analytics queries with multiple predicates or ML models, users must manually optimize their queries to avoid the high cost of naively executing large models on every frame using expensive hardware. For example, it takes over 14 GPU-months to process 100 camera-months of video using a very accurate YOLOv5 model for object detection [55].\n",
      "\n",
      "Consider an analyst studying political coverage of major cable news channels that writes a query to find instances of Bernie Sanders, a politician, reacting angrily to Jake Tapper, a TV news host [20]. Their query may use object detection to find scenes with two people, face recognition to find instances of Jake Tapper and Bernie Sanders, and emotion detection to detect angry reactions. This query can take minutes to execute using unnecessarily accurate models, even on small video inputs, making it challenging for the analyst to interactively explore their dataset. To improve performance, the analyst may use domain knowledge to explore the following model optimizations: (bullet point) Replacement: use a different model for a task, such as a cheaper\n",
      "\n",
      "but less accurate object detector [25, 27, 28, 47].\n",
      "\n",
      "(bullet point) Input Filtering: use a fast model to filter inputs to an expensive model [26, 36, 63]. For example, insert a binary classifier to detect faces before recognizing Tapper or Sanders in frames.\n",
      "\n",
      "(bullet point) Predicate Reordering: run emotion detection before face detection\n",
      "\n",
      "because it is more selective.\n",
      "\n",
      "The domain knowledge needed to consider such optimizations may come from (1) historical or similar queries using alternate models, (2) insights about the training data or query dataset like knowing angry emotions are less prevalent than neutral or happy ones, or (3) knowledge about a general area of expertise (e.g., news, traffic, or sports analysis) suggesting a particular fine-tuned model would be better suited for the domain.\n",
      "\n",
      "Unfortunately, systems today do not provide an interface for users to specify optimizations based on domain knowledge. Users must manually explore the performance-accuracy tradeoff across numerous combinations of optimizations in queries with multiple predicates. We found that, for the news analysis query in which there are nearly 100 plan options, performance can vary by up to 11.7 (times) across different query plans with an accuracy requirement of 80%. Systems today provide no easy way to validate potential optimizations either. Hence, in order to optimize video analysis queries, users must build significant expertise in ML models and systems, taking away valuable time and money from their primary task of gleaning insights from video data.\n",
      "\n",
      "The first goal of this work is to design a user interface to express model relationships informed by domain specific knowledge and define the constraints by which these relationships hold. Our second goal is to develop a video query engine that automatically validates relationships and optimizes complex queries. The engine explores alternate query plans and handles performance-accuracy tradeoffs, relieving users from manual exploration and optimization.\n",
      "\n",
      "We propose a declarative SQL interface for model relationships called relational hints. We capture the semantics of two relationships between a model M and model H considered for optimization: (bullet point) H CAN REPLACE M denotes H and M are interchangeable in the query plan. For example, H may be a faster object detection model the user wants to consider instead of the current one. (bullet point) H CAN FILTER M denotes H can be used to filter inputs to M. For example, H could be a fast binary classifier for detecting the presence of a face prior to recognizing a specific person.\n",
      "\n",
      "Hints capture high-level relationships based on the models’ output signatures and their class labels. Hence, they can often be reused across queries similar to an index. They remove the need for users to manually rewrite queries when a new model becomes available and reason about how the probabilistic nature of models impact their query’s end-to-end accuracy goal.\n",
      "\n",
      "We develop VIVA a video analytics system that optimizes complex SQL queries using relational hints. VIVA’s hint validator first determines what hints are applicable for the query. VIVA’s planner uses the validated hints to generate alternate plans using model replacements, data filtering, and predicate reordering while pruning and limiting the search space for fast query optimization. VIVA’s optimizer enumerates plans enabled by hints and automatically navigates the performance-accuracy tradeoff to select the best performance plan meeting the user’s accuracy requirements. In summary, we make the following contributions:\n",
      "\n",
      "(bullet point) We highlight the difficulty of manually optimizing complex video queries, showing that performance on the same query applying different optimizations can vary by up to 11.7 (times) (Section 2). (bullet point) We formalize a declarative SQL interface for users to specify intuitive relationships between ML models used in video analytics based on domain knowledge (Sections 3 and 4).\n",
      "\n",
      "(bullet point) We detail the design of VIVA that incorporates relational hints in query planning and optimization given the user’s accuracy requirements (Section 5).\n",
      "\n",
      "(bullet point) We implement VIVA of Spark [64] (Section 6) and show that across four real-world queries with different video inputs, hints improve performance up to 16.6 (times) while meeting user accuracy requirements (Section 7).\n",
      "\n",
      "2 THE COMPLEXITY OF VIDEO QUERIES\n",
      "\n",
      "Recent work on video analytics optimization focuses primarily on optimizing a single predicate that uses an ML model, implemented as a user-defined function (UDF) in a query execution engine. Current proposed techniques explore the performance-accuracy tradeoff using fast proxy models to replace more expensive ones [25, 63], cheap filters to reduce the amount of processing needed [26, 36] and indexing for video data [28]. In contrast, we focus on complex queries composed of multiple compute-intensive ML models and predicates. These queries are typically applied on large datasets using scale-out execution engines [42, 48]. Unfortunately, executing these complex queries with multiple ML models is prohibitively expensive and slow. Prior work estimates that a similar query to the TV News analysis query previously mentioned over one year of CNN videos would, at time of writing, take over 4 hours and cost more than $300 using cloud GPUs [31].\n",
      "\n",
      "While it is possible to optimize in isolation each model and predicate in a complex query, this approach is unlikely to lead to best overall performance (lowest latency) and makes it difficult to achieve an overall accuracy goal. We use the TV News analysis query to illustrate the difficulty of manual optimization. We consider the impact of predicate reordering and two optimizations: (bullet point) Model Replacement – replace the original model with one that has the same input/output specification but a different performance accuracy profile. BlazeIt [25] and TASTI [28] are techniques that can generate very fast, purpose-built models but still require a user to manually specify the use of these fast models. (bullet point) Data Filter Model – run a cheap classifier ahead of a more expensive model to filter frames that are unlikely to satisfy the predicate. Systems like PP [36] and CORE [63] automate the insertion of filter models but do not take into account the effects of other optimizations from an end-to-end query perspective. In the TV News analysis query, the analyst is looking for instances of the politician Bernie Sanders reacting angrily to a news anchor Jake Tapper. The analyst uses object detection to find frames with two people, face recognition to find frames with both Sanders and Tapper, and emotion detection to find the angry emotion. Figure 1 shows the accuracies and latencies of different plans for this query after applying the optimizations discussed above. Accuracy is calculated using F1 score [3, 5, 63] with respect to the original plan. The analyst sets an accuracy requirement of 80%.\n",
      "\n",
      "Figure 1 shows that selecting the best of the 6 possible reorderings of the 3 predicates in the query leads to 5 (times) latency improvement. Execution engines today treat UDFs as black-boxes that are not optimized by the execution engine [64]; the analyst must explore the impact of all possible orders. Next, we show the impact of manually considering model replacements. If the analyst uses a faster emotion detection (ED) model, latency improves by 1.01 (times) but accuracy drops to 79%. In contrast, replacing object detection (OD) with a faster person detection model to label people in frames reduces latency by 1.2 (times) over the best reordering without affecting accuracy (Best). Finally, we investigate the impact of using fast filter models, such as a cheap face detection (Haar) model to filter frames without faces [56] and a similarity detector (Sim) to remove frames that are not similar to a reference frame [4]. The Haar filter does not degrade accuracy but increases the latency by 1.5 (times) over OD as its selectivity is low and acts as a poor filter. The Sim filter Table 1: Model Relationship Matrix: dimensions to evaluate how to relate ML models. The result is the relationship in a query plan.\n",
      "\n",
      "Signature\n",
      "\n",
      "Equal Not Equal\n",
      "\n",
      "Classes\n",
      "\n",
      "Equal or Overlap CAN REPLACE CAN FILTER\n",
      "\n",
      "Disjoint CAN FILTER CAN FILTER\n",
      "\n",
      "Figure 1: Manually Optimizing a TV News Analysis Query. PR: Predicate Reordering, RP: Replacing models, FT: Filters, dashed line denotes the user’s accuracy requirement of 80%.\n",
      "\n",
      "reduces latency by 2 (times) over OD: it skips the expensive face recognition model for 94% of the frames without impacting accuracy. The key difference between the Haar and Sim filters is the former supports general face detection while the latter finds similarities to a reference frame.\n",
      "\n",
      "The process of exploring the performance-accuracy tradeoffs and the interactions between these optimizations is long and cumbersome. Users must exhaustively study all options or use some ad-hoc trial and error process to find a good plan. For this query where the are 6 permutations, 2 replacements, and 2 filters, there are almost 100 plans to consider. The details to derive the number of plans using these optimizations are described Section 5.2. As new models and optimization opportunities arise, the number of plans for complex queries will only grow. Even for expert users, it is challenging to manually reason about this large number of query plans. It requires users to not only have an intuition about the potential of an optimization but also have deep knowledge of the performance-accuracy tradeoffs and selectivity of models within a large space of query plans.\n",
      "\n",
      "3 DOMAIN SPECIFIC MODEL RELATIONSHIPS\n",
      "\n",
      "There is potential for multiple models and predicates to improve the execution of a query. However, there is no framework to reason about the relationship between models. In this section, we propose a framework to define model relationships.\n",
      "\n",
      "Suppose we have two models, M and H, that we use to process a set of frames F. The models emit a labeled frame with high confidence that satisfies a predicate or produce no output and the frame is dropped. The label is the output of the model assigned to the frame given its trained classes while the predicate is part of a user’s query that filters by a specific label(s). For example, for an object detection model, the trained classes can be bus, car, person, etc. while the predicate can be to only return frames where the label is a person. This is a common scenario where a user runs a model that generates class labels and only wants to keep frames from a\n",
      "\n",
      "certain class. There are multiple execution plans that, with a given probability, can produce the same set of frames and labels on F: (bullet point) Plan A: Run M on F (bullet point) Plan B: Run H on F (bullet point) Plan C: Run H on F, then M on H’s output (bullet point) Plan D: Run M on F, then H on M’s output We then ask the question: under what conditions do the above plans produce approximately the same results? For plans A and B to produce the same results, M and H must be interchangeable: H can replace M in the execution plan (or vice-versa). For Plans C and D to produce the same results as A and B, H can only drop frames M would also have dropped; H is a filter for M. We can characterize a model by its signature and output classes. The signature is the model’s input and output specification. This is similar to terminology used by TensorFlow [54]. To compare two models, we ask the following questions: (bullet point) Are the model signatures equal or not? (bullet point) Do the models have equal, overlapping, or disjoint output classes? Table 1 captures the different options along these dimensions. If H and M have equal signatures and equal or overlapping classes, then H can replace M. While two models can produce equivalent outputs, they may still differ in performance (execution latency) and/or accuracy. This type of model is referred to as a variant [46– 48] or a proxy model [25]. The model architecture, dataset, and training parameters affect a model’s performance and accuracy.\n",
      "\n",
      "If H and M have equal signatures and disjoint classes, or their signatures are different, H can potentially filter frames for M. For example, consider an image classifier that outputs animal labels per image. Now consider an object detector that can produce the same class labels but the class is attributed to a bounding box. These two model signatures are not equal but there is overlap in the classes. The image classifier can be predicated on whether an animal was found. This only passes frames to the object detector that are highly likely to have an animal. The image classifier acts as a filter. The setup is similar for disjoint labels except a user specifies under what conditions the predicate for the image classifier is true. This can be specified based on a user’s domain knowledge.\n",
      "\n",
      "4 RELATIONAL HINTS We next propose a declarative interface called Relational Hints (hints for short) that formalizes the model relationships defined in Table 1. Hints allow users to declaratively express domain specific knowledge about model relationships. The goal is to provide a query planner with information that enables alternate query plans. A planner can select among these plans to improve query performance or reduce the price while meeting a user’s accuracy requirements. We first describe the different types of hints and their syntax. Next, we walk through a workflow of how hints are used in SQL queries. Finally, we describe sources informing the design of relational hints and relate them to well-known optimizations and domain intuition.\n",
      "\n",
      "4.1 Relational Hint Types\n",
      "\n",
      "A hint takes as input two models and a type, CAN REPLACE or CAN FILTER, that establishes a relationship between the models. We map Table 1 to our declarative hint interface.\n",
      "\n",
      "OrigBestEDODHaarSim050100150200250300Latency (sec)0255075100F1 scorePRRPFTLatencyF1 score\f",
      "Definition 1 A relational hint is a user defined model relationship informed by domain knowledge for the purpose of suggesting alternate query plans to an optimizer.\n",
      "\n",
      "Similar to MicrosoftSQL hints [37] or MySQL hints [39], hints are options specified to the query optimizer to consider alternate query plans. Unlike the aforementioned hints, relational hints are not enforced. Users set a minimum query accuracy requirement and the optimizer chooses which hint(s) (if any) meet that requirement. The accuracy is in reference to the unmodified query plan where the labels produced by the original models represent the ground-truth. The accuracy is calculated on a video supplied by the user called a canary input. A canary input is a shorter clip that represents the type of events the user is looking for. A hint associates a hint model H to an original model M using model specific domain knowledge.\n",
      "\n",
      "Definition 2 Domain knowledge is external information about a model’s signature and class labels in relation to another model.\n",
      "\n",
      "CAN REPLACE Hint. If a model H’s signature and classes are\n",
      "\n",
      "equal or overlap with model M’s signature and classes, a user can define a CAN REPLACE hint to suggest H can replace M in a plan:\n",
      "\n",
      "CREATE HINT H CAN REPLACE M\n",
      "\n",
      "[ FALLBACK DISABLED | ENABLED ]\n",
      "\n",
      "A CAN REPLACE hint is optionally parameterized by a FALLBACK argument. When disabled (the default), this expresses to the system that model H should completely replace M when processing frames. If enabled, the processing will fallback to the original model M if H does not produce a label because its confidence is too low. We assume confidence thresholds are pre-tuned and set for each model as is commonly done with existing optimizations [25]. In effect, this threshold arbitrates whether the model will generate a label or not. This can also be exposed to the user as a parameter to tune. Setting FALLBACK ENABLED may result in M having to process the same inputs that did not satisfy H’s confidence threshold. This may negate some of the performance benefits of using H. However, it gives finer control to the user of the relationship and how much they want to trade-off performance and accuracy. CAN FILTER Hint. If model H’s signature is equal and its classes are disjoint from model M, or if model H’s signature is not equal to model M’s signature, a user can define a CAN FILTER hint to suggest that model H can filter frames for model M. Specifically, frames are only processed by M if they satisfy H’s predicate with high confidence using the model’s pre-set threshold:\n",
      "\n",
      "CREATE HINT H CAN FILTER M\n",
      "\n",
      "[ CONDITIONED ON ANY | <list -of - classes > ]\n",
      "\n",
      "A CAN FILTER hint is optionally parameterized by a CONDITIONED ON parameter which specifies the relationship between the model classes. By default, this parameter is set to ANY: any class in H can satisfy the condition. A user can optionally specify a list of classes. The list of classes means a user can condition M’s input on the results of H’s predicate as defined by the condition.\n",
      "\n",
      "Prior work investigates automatically inferring relationships using historical data [36, 63]. Our interface could be extended\n",
      "\n",
      "to support automatic inference of these relationships by setting CONDITIONED ON to AUTO. In this work, we focus on the overall interface of expressing model relationships and leave it to future work to investigate inferring these relationships.\n",
      "\n",
      "4.2 Example Workflow with Relational Hints\n",
      "\n",
      "We now walk through a workflow using three relational hints for the TV News analysis query searching for Bernie Sanders reacting angrily to Jake Tapper Hints are registered once and automatically used on future queries when applicable. The first hint expresses knowledge that two object detection models have the same signature (labeled bounding boxes of objects) and generate the same number of classes but vary in performance and accuracy. These models can be related using a CAN REPLACE hint:\n",
      "\n",
      "CREATE HINT ObjectDetectFast CAN REPLACE ObjectDetect\n",
      "\n",
      "The second hint uses a tuned face recognition model trained on journalists and personalities. This is similar to a BlazeIt trained model to represent a more expensive model [18]. This model has the same signature (labeled bounding boxes of faces) as a general face recognition model, with some overlap in classes, including labels for Bernie Sanders and Jake Tapper. These models are again related using a CAN REPLACE:\n",
      "\n",
      "CREATE HINT FaceRecogNews CAN REPLACE FaceRecognition\n",
      "\n",
      "FALLBACK ENABLED\n",
      "\n",
      "The CAN REPLACE hint is parameterized with FALLBACK ENABLED to indicate the original FaceRecognition model should be used if FaceRecogNews does not emit a label due to low confidence.\n",
      "\n",
      "The third hint considers a binary detector with labels face/no face. This binary detector can be trained using optimizations like Probabilistic Predicates [36]. Since frames with a face detected typically imply that a face is recognized, a user can express the following hint:\n",
      "\n",
      "CREATE HINT FaceDetect CAN FILTER FaceRecognition\n",
      "\n",
      "CONDITIONED ON [ ' face ']\n",
      "\n",
      "Consider an analyst exploring a VIDEO table that contains frames from a TV dataset. They submit a query searching for instances of Bernie Sanders reacting angrily to Jake Tapper. The analyst sets an accuracy requirement of 90% and provides a short clip (the canary input) of Bernie Sanders being interviewed and reacting angrily to Jake Tapper. The system will use this video to estimate accuracy of new plans. In the following query, we bold models for which there are valid hints available:\n",
      "\n",
      "SELECT frameID , EmotionDetect ( frame ) AS e , FaceRecognition ( frame ) AS f , COUNT ( SELECT ObjectDetect ( frame ) AS o\n",
      "\n",
      "FROM VIDEO GROUP BY frameID WHERE o. label = ' person ' ) AS pcount ,\n",
      "\n",
      "FROM VIDEO WHERE e. label = ' angry ' AND pcount = 2 \\\n",
      "\n",
      "AND f. label LIKE '% Sanders % ' AND f. label LIKE '% Tapper % '\n",
      "\n",
      "ACCURACY 90%\n",
      "\n",
      "The cost-based optimizer will generate additional plans using the registered hints and selects the fastest plan that meets the user accuracy requirement. The lowest cost plan that meets the user’s from the model. At query time, the input frame embeddings are compared to the index and if the frames are similar enough, the stored results are used. This obviates the need to run an expensive model on the dataset. This techniques requires training an index for each target model. Area Expertise. A practitioner could use their knowledge of the training data or the dataset to define even richer relationships. For example, a biologist may wish to detect bears or deer to study their foraging habits [9]. As an alternative way of detecting animals, the biologist knows camera trap feeds are mostly static and detecting motion is usually a good indication of an animal being present. A motion detection model has disjoint labels from an animal detection model. This can be expressed using a CAN FILTER hint:\n",
      "\n",
      "CREATE HINT MotionDetect CAN FILTER AnimalDetect\n",
      "\n",
      "CONDITIONED ON [ ' motion ']\n",
      "\n",
      "Another example can be a sports analyst creating a highlight video of a basketball game. They can use an expensive action recognition model that does pose estimation to analyze if there was a scoring motion and if the ball went through the hoop. Alternatively, they could replace the action recognition model with an optical character recognition (OCR) model to detect score changes using a bounding box on the broadcast score [15, 40]. This would be cheaper because only a small section of the frame is analyzed. This can be expressed using a CAN REPLACE hint:\n",
      "\n",
      "CREATE HINT ScoreChangeOCR CAN REPLACE ScoreActionRecog\n",
      "\n",
      "Users can also express relationships where one of the models do not process frames. For example, consider the TV News analysis example from Section 1. An alternative way of detecting Bernie Sanders can be to search for him in video transcripts. This can be expressed as a CAN FILTER hint since it has disjoint labels from the face recognition model:\n",
      "\n",
      "CREATE HINT TranscriptSearch CAN FILTER FaceRecognition\n",
      "\n",
      "CONDITIONED ON [ ' Sanders ']\n",
      "\n",
      "5 APPLYING RELATIONAL HINTS\n",
      "\n",
      "We now describe the design of VIVA a video analytics system that interprets hints to optimize queries. VIVA enables users to query videos using SQL, provides an interface to specify hints and an accuracy threshold, and automatically applies hints to a query.\n",
      "\n",
      "Figure 2 shows VIVA’s system architecture. Blue components are specially designed for translating hints to executable plans. Users register hints with the registrar which are stored in the hints table. The parser processes the query and creates a query model tree. This tree is passed to the hint validator that determines which hints are relevant to the existing query. The planner and optimizer apply hint transformation rules to produce additional query plans, estimates accuracy, selectivity, and finally the overall cost of each plan. Depending on a user-defined optimization target (performance, cheapest price or best performance per dollar), VIVA selects the plan that meets the user’s accuracy requirements. This plan is sent to the execution engine to process the user’s input.\n",
      "\n",
      "We walk through each blue component of Figure 2 by breaking down the query optimization steps shown in Figure 3. We design our system with the following goals in mind:\n",
      "\n",
      "Figure 2: VIVA Architecture Diagram.\n",
      "\n",
      "accuracy is the following, where the changes made are bolded: Result\n",
      "\n",
      "+ EmotionDetect . label isin [ ' angry ']\n",
      "\n",
      "+ ObjectDetectFast . pcount = 2 AND \\\n",
      "\n",
      "ObjectDetectFast . label isin [ ' person '] + FaceRecognition . label isin [ ' Tapper ' , ' Sanders ']\n",
      "\n",
      "+ FaceRecogNews . label isin [ ' Sanders ' ] \\\n",
      "\n",
      "AND FaceRecogNews . conf > 0.8\n",
      "\n",
      "+ FaceDetect . label isin [ ' face ' ] AND \\\n",
      "\n",
      "FaceDetect . conf > 0.7\n",
      "\n",
      "Note the optimizer in applying the hints also finds the optimial execution order to execute the most selective models first.\n",
      "\n",
      "4.3 Sources of Relational Hints\n",
      "\n",
      "We now give examples of where model relationships can originate from and how users can capture these relationships using hints. Model Variants. Models having the same signatures and either equal or overlapping classes are ideal candidates for CAN REPLACE relationships. For example, when analyzing 5 popular open-source repositories for object detection models, we find there are at least 24 unique models with varying accuracy and performance characteristics [13, 45, 55, 57, 60]. Also, there are 16 pre-trained image classification models in PyTorch with varying performance and accuracy profiles [44]. These different profiles can come from the model architecture or the use of techniques like quantization that post-process the model to further trade accuracy for performance [23].\n",
      "\n",
      "Additionally, an emerging trend is building models using a common set of layers and fine-tuning the rest for a specific application. The common layers are known as the “prefix” and the fine-tuned layers as the “suffix”. These are also considered variants of the original model. A system can take advantage of saving the results of the prefix after the first call and reuse the results multiple times only needing to run the suffix model [24]. Model variants are best expressed as CAN REPLACE relationships. Proxy Models. Proxy models can be trained to be smaller (in total GFLOPs) approximate versions of a larger, more accurate ML model. They can also be used to limit the number of invocations to the larger model. For example, a model may be trained on a subset of data because a fixed-view camera only needs to identify the original model’s objects from a single viewpoint [25]. Other techniques like TASTI [28] train embedding indices at query optimization time. These indices run a model on a small fraction of the input dataset and store a representation (embeddings) of the frames and results\n",
      "\n",
      "VIVAQuery PlannerQuery OptimizerRelational hintregistrarQuery ParserRelational hints tableH1...H2...Execution engineModel libraryHint Validator\f",
      "Figure 3: VIVA Query Optimization Steps using Relational Hints.\n",
      "\n",
      "for node in DepthFirstSearch(QueryTree) do\n",
      "\n",
      "𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠.𝑎𝑝𝑝𝑒𝑛𝑑 (Permutations(𝑛𝑜𝑑𝑒 ) ) while 𝑁𝑢𝑚𝑃𝑙𝑎𝑛𝑠! = 𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠 do\n",
      "\n",
      "Algorithm 1 Plan Generation with Hints 1: 𝑉 𝑎𝑙𝑖𝑑𝐻𝑖𝑛𝑡𝑠 ← 𝑉 𝑎𝑙𝑖𝑑𝑎𝑡𝑒𝐻𝑖𝑛𝑡𝑠 (𝐴𝑙𝑙𝐻𝑖𝑛𝑡𝑠, 𝑄𝑢𝑒𝑟 𝑦) 2: procedure Planner(𝑄𝑢𝑒𝑟 𝑦𝑇 𝑟𝑒𝑒) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end procedure\n",
      "\n",
      "𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠.𝑎𝑝𝑝𝑒𝑛𝑑 (ApplyHints(𝑛𝑜𝑑𝑒 ) ) 𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠 ← 𝐿𝑒𝑛 (𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑇 𝑟𝑒𝑒𝑠 ) 𝑁𝑢𝑚𝑃𝑙𝑎𝑛𝑠 ← 𝑁 𝑒𝑤𝑃𝑙𝑎𝑛𝑠\n",
      "\n",
      "end for return 𝐴𝑙𝑙𝑃𝑙𝑎𝑛𝑠\n",
      "\n",
      "end while\n",
      "\n",
      "(bullet point) Validating hints (Section 5.1) – The validator requires rules to arbitrate which hints can be used to generate additional plans. (bullet point) Applying hints to generate plans (Section 5.2) – The planner needs to translate validated hints to query plans using a formal set of transformation rules while limiting the number of total plans considered.\n",
      "\n",
      "(bullet point) Providing accuracy guarantees (Section 5.4) – Given a user’s accuracy requirement, the optimizer must compare plans, and only select those that meet the accuracy requirement.\n",
      "\n",
      "5.1 Validating Hints\n",
      "\n",
      "After VIVA’s parser produces a tree with the models to run, the hint validator determines which registered hints are valid for the query. This is Step 1 in Figure 3. Throughout this section, we refer to the original model as M and the hint model as H. For both hint types, this is a fast static analysis where VIVA is comparing hint arguments to the model classes. CAN REPLACE hints. CAN REPLACE is only applied if the overlap in classes of H and M are equal or larger than the classes predicated in the user’s query. The requirement is the model H must produce the same predicated results as M. The hint is discarded if the classes are not equal or there is no overlap. To limit the search space, hint validation only retains hints that are likely to yield a plan with a lower cost than the original query ccomparing the profiled latency of the hint model to the original model. The latency is profiled offline as a one-time step. CAN FILTER hints. VIVA validates that the user-specified classes in CONDITIONED ON are in model H. Because the classes can either overlap, be equal, or disjoint, there is no validation for M.\n",
      "\n",
      "5.2 Generating Plans using Hints\n",
      "\n",
      "We now describe the steps the planner takes to recursively apply hints to a query (Step 2 in Figure 3). Transforming a Query Plan. VIVA’s parser transforms a user’s query into an intermediate representation called a model tree. Each tree node represents a model and its predicate. If a parent node has a child, the output of the parent is dependent on its child (or children’s) predicate(s) which preserves dependencies.\n",
      "\n",
      "Algorithm 1 shows the planner’s steps. The planner first reorders the predicates of that node’s children (line 4) then applies the hints exhaustively (lines 5 ∼ 8) until no new candidate trees can be generated at that node. The order the hints are applied does not matter. Remaining order-agnostic is necessary: a hint could generate a tree that could be further modified. This is Step 2 of Figure 3, where we show a subset of the plans generated for the query after applying the hints from the table. A CAN REPLACE hint can modify a tree in two ways: (1) H replaces M in the new tree, (2) the planner will insert H before M if the user has set FALLBACK ENABLED. M will only run on frames where H did not produce a label because the confidence was too low. The predicate is applied to the union of H and M’s results. For CAN FILTER hints, the planner will generate a tree where a model H predicated on CONDITIONED ON is inserted before model M. All inputs to M are predicated on H. Enumerating Plans. To ensure all plans have been generated using hints, VIVA’s planner analytically computes the number of expected plans based on the number of models and hints. The standard way of generating alternate query plans is to permute the predicates of independent models. Let 𝑁 be the number of independent models at depth 𝑖. The number of valid plans is the product of the permutations of those models: 𝑖 −1 𝑗=1 N𝑗 ! For example, if a plan has three independent models A, B and C, there are six plans representing the permutations of these models (ABC, BAC, etc.). We also to consider cases where hints generate additional plans. A CAN REPLACE hint will only generate an alternate plan if either one or both models of the hint appear in the plan. A CAN FILTER hint will only generate an alternate plan if M appears in the plan where H will be inserted into the plan.\n",
      "\n",
      "5.3 Canary Inputs\n",
      "\n",
      "VIVA takes as input the query plan, an accuracy requirement and a canary input video. A canary input is a short video the user provides that represents events they are querying in the dataset.\n",
      "\n",
      "HintML UDFExecutionProfilesPlanOptimizationAccuracyEstimation100%90%85%40%> 80%500400Cost Estimation300MINBest PlanHintValidation CREATE HINT AS ObjDetV1 CAN REPLACEObjDetV2 FALLBACK 0 Relational HintsidHINTPARAMETERSH1ObjDetV1 CAN REPLACE ObjDetV2FALLBACK 0H2FaceDet CAN FILTER FaceRecogCONDITIONED ON ['FACE'] .........PlanGenerationH1FROFRFDOH2FRFDOH1 + H2QueryPlanFRO...CandidatePlans2Valid133a3bOffline hint generationSELECT FaceRecog(frame) AS f, ObjDetV2(frame) AS o FROM video WHERE ...ACCURACY 0.90 QueryCanary Input Video\f",
      "Using small, representative canaries is common when tuning or calibrating computer vision, machine learning, and data mining systems before execution [35]. They have also been used by recent video analytics systems [59]. Canary data is similar to sampling during pre-processing to optimize the query [7] except in our case the user explicitly defines the data used. Empirically, we found it suffices to use a canary with at least one occurence of the event queried and some amount of noise to generate true positives and true negatives for calculating F1 score.\n",
      "\n",
      "VIVA runs the original query plan on this input to generate ground-truth labels. Alternate query plans are also run on the canary and their accuracies are computed with respect to the groundtruth labels. The canary needs to closely resemble but not necessarily match the type of events a user is looking for. VIVA uses this to find which alternate plans will closely resemble the original plan i.e., have high accuracy. The canary is specifically provided by the user and not sampled from the input dataset because it serves as a labelled set of frames, similar to a validation set.\n",
      "\n",
      "In the TV News analysis example from Section 4.2, a canary input could contain scenes of Bernie Sanders upset in an interview with Jake Tapper or, more generally, scenes of two people being interviewed with one person reacting angrily. A poor canary has events for which the models in the original query plan would have low accuracy (e.g., a basketball player dunking). In this work, canaries are manually selected to be representative of the query. In future work, we plan to explore techniques to automate the selection of canaries or notify users when their canary is not representative.\n",
      "\n",
      "5.4 Selecting Plans to Meet User Requirements The planner produces 𝑋 plans P = {𝑃1, 𝑃2, · · · 𝑃𝑋 }. A plan 𝑃𝑥 consists of 𝑁 ordered models M = {𝑀1, 𝑀2, · · · 𝑀𝑁 }. The goal of VIVA’s optimizer is to select the fastest or most price-efficient plan P∗ that satisfies a user’s accuracy requirement A. We now discuss how VIVA’s optimizer selects this plan. Estimating Plan Accuracy. Relational hints enable users to express knowledge to the query optimizer to evaluate alternate query plans. Using hints, the planner may generate query plans that tradeoff accuracy for performance. VIVA provides users the ability to express an accuracy target the optimizer must still meet in selecting the plan with the lowest cost. Step 3a of Figure 3 shows this step of the optimizer estimating accuracy for each plan.\n",
      "\n",
      "We next detail our approach to accuracy estimation where we use common techniques from recent work for using the original models to generate ground-truth labels [25]. We use F1 score to estimate accuracy [3, 5, 63], and compute an F1 score per plan (not per model). To estimate each plan’s accuracy, VIVA first runs the original models and candidate models over the canary input’s frames and stores these results in a table. During query optimization, VIVA queries the table only with each plan’s predicates to produce a final set of labels, 𝑅. This eliminates the need to run the model again for each plan. The results from the user’s initial query plan are used as the ground-truth labels 𝑅𝑡𝑟𝑢𝑡ℎ. Finally, the candidate plan’s F1 score is computed using 𝑅𝑡𝑟𝑢𝑡ℎ and 𝑅. Selectivity and Cost Estimation. The last step in plan selection, Step 3b of Figure 3, is to estimate cost. VIVA determines how many frames 𝑓𝑖 a model 𝑀𝑖 needs to process. This is based on\n",
      "\n",
      "the selectivity 𝑠𝑖 −1 of the upstream model 𝑀𝑖 −1 and is given by: 𝑓𝑖 = 𝑀𝑖 −1  (times) 𝑠𝑖 −1. We use a standard approach of estimating selectivity: VIVA samples a number of frames from the input dataset. We sample frames at a fixed rate from the input dataset similar to prior work; other techniques for sampling can also be used [3, 38]. VIVA estimates selectivity independently for each model.\n",
      "\n",
      "(𝐵). For each new model, VIVA profiles 𝐿𝑀𝑖 𝐻 𝑗\n",
      "\n",
      "VIVA’s cost model is designed to support arbitrary backend hardware platforms and models with arbitrary batch sizes. The latency of executing 𝑀𝑖 on hardware platform 𝐻 𝑗 for a batch of 𝐵 inputs is 𝐿𝑀𝑖 (𝐵) once 𝐻 𝑗 and stores its value for future cost estimations. If there is a data transfer time associated with a particular platform like the GPU, VIVA will profile transferring different batches of frames and builds a model to estimate transferring any number of frames. Profiling and building the model is a one-time, offline step. When estimating the cost, VIVA includes the data transfer time estimated by the model as part of the overall plan cost. For 𝑉 different hardware platforms, there are up to 𝑁 𝑉 different hardware configurations that models in 𝑃𝑥 can run on K𝑃𝑥 = {H 1 , H 2 }. A 𝑃𝑥 𝑃𝑥 hardware configuration H𝑐 = {𝐻1, 𝐻2, . . . , 𝐻𝑁 } corresponds to a 𝑃𝑥 model 𝑀𝑖 in plan 𝑃𝑥 running on hardware platform 𝐻𝑖 . Then, the estimated cost of running 𝑃𝑥 with H𝑐 𝑃𝑥 is: 𝑁 ∑︁\n",
      "\n",
      ", . . . , H 𝑁 𝑉 𝑃𝑥\n",
      "\n",
      "𝐶 (𝑃𝑥 , H𝑐 𝑃𝑥\n",
      "\n",
      ") = 𝐿𝑇 𝑟𝑎𝑖𝑛 +\n",
      "\n",
      "𝐿𝑀𝑖 𝐻𝑖\n",
      "\n",
      "(𝐵)  (times) (𝑓𝑖 /𝐵)\n",
      "\n",
      "𝑖=1\n",
      "\n",
      "𝐿𝑇 𝑟𝑎𝑖𝑛 is the time to train models specific to the query. If models are trained in parallel, 𝐿𝑇 𝑟𝑎𝑖𝑛 is the maximum time to train all models. If models are trained sequentially, 𝐿𝑇 𝑟𝑎𝑖𝑛 is the sum of times to train all query-specific proxy models [25, 28]. If all models are available, 𝐿𝑇 𝑟𝑎𝑖𝑛 = 0. The cost-optimal hardware configuration for models in 𝑃𝑥 is:\n",
      "\n",
      "H ∗\n",
      "\n",
      "𝑃𝑥 = arg min\n",
      "\n",
      "H𝑐\n",
      "\n",
      "𝐶 (𝑃𝑥 , H𝑐 𝑃𝑥\n",
      "\n",
      ")\n",
      "\n",
      "𝑃𝑥 ∈ K𝑃𝑥 Finally, let 𝐴𝑥 be the estimated accuracy for 𝑃𝑥 . The cost-optimal plan, P∗, among plans satisfying the user accuracy requirement is: 𝐶 (𝑃𝑥 , H ∗ 𝑃𝑥\n",
      "\n",
      "), 𝑠.𝑡 . 𝐴𝑥 ≥ A\n",
      "\n",
      "P∗ = arg min 𝑃𝑥 ∈ P\n",
      "\n",
      "A user can parameterize the query with 3 targets: performance, cheapest price, or best performance per dollar. For performance, VIVA will return the fastest plan given the hardware available. For cheapest price, VIVA will return the cheapest plan based on estimated latency multiplied by the cost per hour. For best performance per dollar, VIVA will return the plan and target platform that delivers the highest end-to-end performance at the lowest price.\n",
      "\n",
      "5.5 Plan Pruning\n",
      "\n",
      "As the complexity of queries increases with more models, predicates, and hints, the number of possible plans generated grows exponentially. This increases VIVA’s query optimization latency because of the larger search space to consider given plans generated using hints. We apply several heuristics and pruning techniques to limit the search space and provide fast query optimization.\n",
      "\n",
      "At hint validation, VIVA finds any model that is more expensive than the original model in the user’s query and removes it before the plan generation step. Using these models would generate plans that are strictly more expensive than the original query. During plan Table 2: Queries, Datasets, Predicates, and Validated Hints Per Query.\n",
      "\n",
      "Application\n",
      "\n",
      "Dataset\n",
      "\n",
      "Query Description\n",
      "\n",
      "Predicates\n",
      "\n",
      "# Hints\n",
      "\n",
      "Traffic News Sports Bias\n",
      "\n",
      "Jackson square traffic camera [25, 28] “Big three news” broadcasts [2, 20] NBA games [21, 52] Casual conversations dataset [17]\n",
      "\n",
      "Cars turning left with people in intersection at night Jake Tapper interviewing angry Bernie Sanders LeBron James dunks Non-white females over the age of 19\n",
      "\n",
      "time of day = night ∧ object = (people & car) ∧ object track emotion = angry ∧ count(object = people) = 2 ∧ face = (Sanders & Tapper) action = dunking basketball ∧ face = James age > 19 ∧ race != non-white ∧ gender = female\n",
      "\n",
      "7 7 2 3\n",
      "\n",
      "generation, VIVA eliminates redundant calls to models that could be a result of CAN REPLACE hints and push predicates down to the first call of the model. Thus, duplicate plans are eliminated which can occur as a result of hints replacing interchangeable models.\n",
      "\n",
      "Our pruning approach is akin to recent work in video analytics optimization, CORE [63], which uses branch-and-bound to evaluate the plan cost after each model. CORE only retains plans that are likely to have a lower cost than the best plan found thus far. VIVA prunes a plan if 1) the accuracy requirement was already met with a lower accuracy model and the current plan uses a higher accuracy model, 2) the accuracy requirement was not met with a higher accuracy model and the current plan uses a lower accuracy model, or 3) if a plan’s estimated cost exceeds the best complete plan’s cost after a given model, it is pruned.\n",
      "\n",
      "VIVA transparently applies these heuristics and pruning techniques which significantly limits the search space making query optimization fast. In future work, we plan to investigate other pruning techniques. For example, we can use statistics about CAN FILTER hints and their historical accuracy when generating plans. Historically low accuracy CAN FILTER hints can be pruned.\n",
      "\n",
      "6 IMPLEMENTATION\n",
      "\n",
      "VIVA is built on top of Spark [64] where users express queries using UDFs and predicates in SQL or Spark’s dataframe API in Python. We use Spark’s execution engine for UDF execution and take advantage of its optimizer for structured query optimizations. Video ingestion and indexing uses FFmpeg [11]. Frames are stored as raw byte arrays in a PySpark DataFrame to enable data pipelining.\n",
      "\n",
      "We use pretrained PyTorch models for applications such as action recognition, object detection, and facial recognition [10, 41, 43, 45], and TensorFlow for bias analysis and emotion recognition [1, 53]. Computer vision models to detect day/night scenes, motion detection, and similarity detection are implemented using OpenCV [4]. For the day/night scene detector, we use Scikit-learn’s Support Vector Machine (SVM) [50] implementation which we trained on 240 images of day/night frames from traffic camera feeds [25].\n",
      "\n",
      "By giving users the ability to express relationships across a wide range of models, hints naturally capture several existing video analytics optimizations. We use cheaper, less accurate object detection models to represent techniques like BlazeIt [25] that propose training low accuracy purpose-built models. We implement three layer sharing models for race, gender, and age detection, using on DeepFace [51], that share a common set of layers like those produced by Mainstream [24]. CAN FILTER hints can be used to relate cheap binary classifiers such as those produced using techniques like Probabilistic Predicates [36] or CORE [63]. We use models of similar computational footprints to represent these optimizations. We use TASTI [28] to generate candidate models for CAN REPLACE or CAN FILTER hints. We use a pre-trained ResNet18 embedding model. Unless otherwise noted, these indexes are trained and available at query time.\n",
      "\n",
      "7 EVALUATION\n",
      "\n",
      "We now evaluate VIVA using the queries from Table 2. In all cases, the query plans benefit from the execution engine’s query optimizer which applies standard structured query optimizations where possible. We deployed VIVA on Google Cloud Platform (GCP). We use a n1-highmem-16 instance (16 vCPUs, 104 GB of DRAM). This instance features Intel Xeon E5-2699 v4 CPUs operating at 2.20GHz, Ubuntu 20.04 with 5.15.0 kernel. For GPU experiments, we consider n1-highmem-16 instances with an NVIDIA T4 and a V100. Queries and Datasets. Table 2 shows the queries, datasets, predicates, and total number of validated hints used to evaluated VIVA. All queries are complex: each with multiple models and predicates and represent a range of different applications. TV News analysis is based on queries used to explore a decade of US cable news [12, 20]. Sports analysis is commonly used for game planning, as well as for creating highlight reels [21]. Traffic analysis is used for landscaping and autonomous vehicle training [25, 26, 32]. Bias analysis is used to assess and detect model bias from training data [17].\n",
      "\n",
      "We consider two inputs: the event searched is present in the video — Event Present, and no instances of the event are in the video — Event not Present. As shown in Table 2, these inputs come from the same dataset. All videos are one hour long, 360p, and are processed at 1 FPS. The framerate we use is chosen to be consistent with prior work [25]. The canary input is 15 seconds. We use F1 score for accuracy. Also consistent with prior work, selectivity estimation is performed over 3% of the input frames [6]. Relational Hints. Table 3 shows example tasks, models, and hints we use in our evaluation. In total, we use 19 different hints — 11 CAN REPLACE, 4 CAN REPLACE with FALLBACK ENABLED, and 4 CAN FILTER— across 30 different models. We capture several sources of domain knowledge in our choice of hints. Model Variants. Using a smaller object detection as a replacement for a larger model is a classic example of a model variant where the models have the same output and classes but have different model architectures e.g., SmallObjDet CAN REPLACE LargeObjDet. For layer sharing models, a user specifies the suffix layers to run in the model relationship e.g., RaceID CAN REPLACE SuffixRaceID. VIVA automatically determines whether it is worthwhile to execute the combination of prefix and suffix layers. Proxy Models. Cheaper, less accurate models generated using TASTI can be used as a replacement for more expensive ones like face detection and action detection. To indicate that these models should use the larger models when a label cannot be produced, we set FALLBACK ENABLED for CAN REPLACE hints. Area Expertise. Classical computer vision techniques can be used by practitioners and expressed as model relationships. For example, Task\n",
      "\n",
      "Models\n",
      "\n",
      "Relational Hints\n",
      "\n",
      "Table 3: Tasks, Models, and Sample Relational Hints.\n",
      "\n",
      "Emotion Detection\n",
      "\n",
      "MTCNN Emotion Detection, HAAR Emotion Detection, TASTI Emotion Detection\n",
      "\n",
      "Object Detection\n",
      "\n",
      "Image Classification Facial Recognition Race Identification Action Recognition Day/Night Detection\n",
      "\n",
      "Small Object Detection, Large Object Detection, Object Similarity Detection, Motion Detection ResNet50 Image Classification, ResNet18 Quantized Image Classification Face Recognition, TASTI Face Recognition RaceID, Suffix RaceID Action Recog, Action Similarity Recog Pixel Brightness Detect, SVM for Day/Night Detection\n",
      "\n",
      "MTCNNEmoDet CAN REPLACE HAAREmoDet, TASTIEmoDet CAN REPLACE MTCNNEmoDet FALLBACK ENABLED SmallObjDet CAN REPLACE LargeObjDet, ObjSimDet CAN FILTER LargeObjDet, MotDet CAN FILTER LargeObjDet CONDITIONED ON [‘motion’] QImgCls CAN FILTER LargeObjDet TASTIFaceRecog CAN REPLACE FaceRecog FALLBACK ENABLED RaceID CAN REPLACE SuffixRaceID ActionSimDet CAN FILTER ActionRecog PixelBriDet CAN REPLACE SVM\n",
      "\n",
      "7.1 Improving Query Performance\n",
      "\n",
      "We first explore how VIVA uses hints to improve performance over predicate reordering with an accuracy requirement of 90%. Figure 4 shows the performance for each query for Event Present (Figure 4a) and Event not Present (Figure 4b). The latencies presented are inclusive of query optimization time. Table 4 shows the plan UpperPR uses and the best plan VIVA uses and its accuracy. Traffic Analysis. For Event Present, UpperPR filters by time of day, objects, and object tracking. EVA and BestPR filter by time of day last. Since Event Present is all night scenes, no filtering occurs with the SVM day/night detection. VIVA uses a pixel brightness detection and a faster object detection model than EVA since its accuracy estimator determines it can use what EVA considers a “low” accuracy model. This enables VIVA to improve performance by 4.8 (times) over the baselines. For Event not Present, VIVA and EVA first filter by time of day since this input is all day scenes. VIVA is slightly faster (1.2 (times)) because it uses the pixel brightness detection. UpperPR runs the time of day detection last, which leads to a 16.6 (times) drop in performance compared to VIVA. News Analysis. For Event Present, UpperPR first filters by emotion, which is the least efficient since this expensive model must process all frames. EVA and BestPR first filter by faces — a faster model — before doing object and emotion detection, respectively. VIVA uses a faster object detection (which EVA would classify as low accuracy), along with a TASTI-trained model for emotion detection. The TASTI-trained emotion detection is backed by the HAAR emotion detection. This improves performance 4.8 (times) over UpperPR, and 1.3 (times) over EVA and BestPR. For Event not Present, VIVA uses object similarity detection as a result of a CAN FILTER hint. Sports Analysis. For Event Present, all baselines use the same two models, with EVA and BestPR benefiting from predicate reordering. VIVA uses a TASTI-trained action detection backed by the original action detection model. This enables VIVA to improve performance by 1.5 (times) over UpperPR, and 1.2 (times) over EVA and BestPR. For Event not Present, VIVA uses a similarity detection for detecting dunks from a reference image. This improves performance up to 2.5 (times). Bias Analysis. For Event Present, VIVA uses a plan with common prefix layer models for race and age detection, specified using a CAN REPLACE hint. The common layers are run once and reused for the two suffix models. This improves performance by 1.5 (times) over UpperPR, and matches the performance of EVA and BestPR. For Event not Present, VIVA does not use the common prefix layer models, since the gender detection model can filter the majority of\n",
      "\n",
      "(a) Event Present in frames.\n",
      "\n",
      "(b) Event not Present in frames. Figure 4: Query Speedup Relative to UpperPR.\n",
      "\n",
      "a pixel brightness detector can be used as a replacement for an SVM model trained to detect whether a frame is from day or night (PixelBriDet CAN REPLACE SVM). Performing similarity detection to find frames similar to a reference image can be used as a replacement for detecting actions. In the case of the Traffic analysis query, since the analyzer is querying static camera feeds, they can infer that detecting cars and people can be cheaper using motion detect. They can relate motion detection to object detection as: MotDet CAN FILTER LargeObjDet CONDITIONED ON [‘motion’]. We compare VIVA to the following baselines: (bullet point) Upper Bound Predicate Reorder (UpperPR): this is the worstcase latency of predicate reordering for a given accuracy requirement if a system does not support selectivity and cost estimation for ML UDFs which is common in today’s execution engines. (bullet point) Best Predicate Reorder (BestPR): this represents what a user can expect if a video analytics system is able to do selectivity and cost estimation for ML UDFs to find the lowest latency ordering given an accuracy requirement.\n",
      "\n",
      "(bullet point) EVA: a recently-proposed, state-of-the-art video analytics system whose optimizer makes model and predicate reordering selections given a fixed accuracy [62]. Users specify a model’s accuracy using coarse-grained indicators: low for accuracies 80% and below, medium for accuracies [80%, 90%), and high for accuracies 90% and above. During query optimization, EVA selects each model to use separately based on the plan accuracy requirement.\n",
      "\n",
      "TrafficNewsSportsBias0246Query Speedup1.01.01.01.01.03.81.21.51.03.81.21.54.84.81.51.5UpperPRBestPREVAVIVATrafficNewsSportsBias01020Query Speedup1.01.01.01.014.24.61.81.614.24.61.81.616.68.22.51.5UpperPRBestPREVAVIVA\f",
      "Table 4: Best Plan Identified by VIVA. PR: Predicate Reorder, RP: CAN REPLACE, RPF: CAN REPLACE with FALLBACK ENABLED, FT: CAN FILTER.\n",
      "\n",
      "Application\n",
      "\n",
      "Original Plan\n",
      "\n",
      "Best Hint Plan: ∃: Event Present, : Event not Present\n",
      "\n",
      "Accuracy\n",
      "\n",
      "Traffic\n",
      "\n",
      "News\n",
      "\n",
      "Sports\n",
      "\n",
      "Bias\n",
      "\n",
      "TimeOfDay ∧ Object ∧ ObjectTrack\n",
      "\n",
      "Emotion ∧ Object ∧ Face\n",
      "\n",
      "Action ∧ Face\n",
      "\n",
      "Age ∧ Gender ∧ Race\n",
      "\n",
      "∃: RP(Object) ∧ ObjectTrack ∧ RP(TimeOfDay) : RP(TimeOfDay) ∧ RP(Object) ∧ ObjectTrack ∃: RP(Object) ∧ Face ∧ RPF(Emotion) ∧ RP(Emotion) : FT(Object) ∧ Object ∧ Face ∧ Emotion ∃: Face ∧ RPF(Action) ∧ Action : FT(Action) ∧ Action ∧ Face ∃: Gender ∧ RP(Race) ∧ RP(Age) : Gender ∧ Age ∧ Race\n",
      "\n",
      "100% 100% 91% 91% 100% 90% 100% 100%\n",
      "\n",
      "VIVA spends on average 20% of execution time on query optimization. This is in line with recently released systems like FiGO [6], MIRIS [3], and Jellybean [58] where 20%-25% of query execution is spent on optimization. Pruning eliminates on average 70% of plans for 3 out of 4 queries. The Sports application does not benefit from pruning because of the small number of hints used and relatively small number of additional plans generated. Without pruning, the News analysis query’s optimization time is 2.1 (times) higher. Query execution time represents the majority of the time for all queries: 80% on average, up to 84%. For larger inputs assuming the same query, the time spent on query execution will grow while query optimization stays constant. Lastly, query optimization time varies only by up to 50% across all queries despite the number of plans differing by up to 72 (times). This shows VIVA can scale as queries become more complex and the number of hints grows.\n",
      "\n",
      "7.3 Performance Impact of Hint Types\n",
      "\n",
      "We next analyze what hints improved performance by ablating the registered hints VIVA applies. We use the News and Traffic queries. For each query, we consider each available hint type separately (RP: CAN REPLACE, RPF: CAN REPLACE with FALLBACK ENABLED, FT: CAN FILTER) and VIVA using all available hints). Traffic analysis has no CAN REPLACE with FALLBACK ENABLED. News Analysis. Figure 5a shows the ablated performance for Event Present and Event not Present. In both cases, the best performance comes from using a mix of hints. For Event Present, RP uses a faster object detect. RPF uses TASTI-trained models for emotion and object detect but uses the more expensive object detection as a fallback model. Interestingly, VIVA selects a different predicate ordering in each case: object detection runs first for RP while face detection runs first with RPF. RP is faster than EVA and BestPR since RP identifies a faster but lower accuracy object detection that can meet the accuracy requirement compared to the high accuracy one used by EVA. FT uses the same plan as EVA and BestPR since using CAN FILTER hints do not meet the accuracy requirement. For Event not Present, VIVA uses an object similarity detection model to improve performance. Traffic Analysis. Figure 5b shows the ablated performance for Event Present and Event not Present. For both inputs, the best plan that meets the accuracy requirement is to use only CAN REPLACE hints. For Event Present, FT again picks the same plan as EVA since the motion detect model specified using the CAN FILTER hint does not meet the accuracy requirement. Plans are closer in performance for Event not Present since filtering by\n",
      "\n",
      "(a) News analysis.\n",
      "\n",
      "(b) Traffic analysis. Figure 5: Query Latency when Ablating Hints. RP: CAN REPLACE, RPF: CAN REPLACE with FALLBACK ENABLED, FT: CAN FILTER.\n",
      "\n",
      "Table 5: Query Optimization Latencies for Figure 4a Queries.\n",
      "\n",
      "Application\n",
      "\n",
      "# Plans w/o Pruning\n",
      "\n",
      "# Pruned Plans\n",
      "\n",
      "Query Opt. (% Total)\n",
      "\n",
      "Query Exec. (% Total)\n",
      "\n",
      "Total\n",
      "\n",
      "Traffic News Sports Bias\n",
      "\n",
      "60 432 6 42\n",
      "\n",
      "17 25 6 24 Average\n",
      "\n",
      "92s (17%) 116s (28%) 130s (18%) 88s (16%) 107s (20%)\n",
      "\n",
      "453s (83%) 302s (72%) 592s (82%) 473s (84%) 455s (80%)\n",
      "\n",
      "545s 418s 722s 561s 562s\n",
      "\n",
      "frames. VIVA is slightly slower in this case (1.1 (times)) compared to EVA and BestPR since both run the same plan but VIVA additionally performs accuracy estimation. Performance is similar across the board for this query because the original models and the CAN REPLACE suffix models have similar performance.\n",
      "\n",
      "7.2 Query Optimization Latency\n",
      "\n",
      "We next evaluate VIVA’s query optimization latency. During query optimization, VIVA estimates hint-generated plan accuracies and selectivities. This time increases with the number of independent query predicates and the number of applicable hints per query. Table 5 shows the absolute and relative time breakdowns for query optimization and execution using Event Present (a one hour input) with the 15 second canary input.\n",
      "\n",
      "BestPREVAFTRPRPFVIVA0200400600800Latency (sec)Event PresentEvent not PresentBestPREVAFTRPVIVA050010001500Latency (sec)268226822729Event PresentEvent not Present\f",
      "(a) News analysis.\n",
      "\n",
      "(b) Traffic analysis. Figure 6: Query Latency as Accuracy Requirements Vary.\n",
      "\n",
      "Figure 7: Query Latency of News Analysis with Training Latency. Arrow: TASTI index creation.\n",
      "\n",
      "time of day first is best in all cases. RP and VIVA use pixel brightness detection to improve performance over EVA and FT.\n",
      "\n",
      "7.4 Trading Off Latency and Accuracy\n",
      "\n",
      "We now evaluate how VIVA automatically chooses the highest performance plan as the accuracy requirement varies. Using the News and Traffic analysis queries, we sweep accuracy requirements from 60% to 95% and use the Event Present input. EVA uses low accuracy models for requirements 80% and below, medium accuracy models for [80%, 90%) requirements, and high accuracy models for requirements 90% and above. News Analysis. Figure 6a shows the results of varying accuracy where, aexpected, more stringent accuracy requirements also result in a decrease in performance. For accuracy requirements of 80% and 90%, VIVA selects the plan shown in Table 4. For accuracy requirement of 95%, VIVA uses the faster object detection model, but no longer uses TASTI-trained models or the faster emotion detect. However, the performance is similar to the plan shown in Table 4. The performance difference between these plans is 1.8 (times). VIVA outperforms EVA for all accuracy requirements (up to 1.5 (times)) since it identifies the best performing combination of hints that meet the accuracy requirements. Indeed, for accuracy requirements\n",
      "\n",
      "of 90% and 95%, VIVA uses faster models that meet the accuracy requirements, while EVA uses the slower, high accuracy models. Traffic Analysis. Figure 6b shows the results where VIVA identifies the plan shown in Table 4 meets all accuracy requirements, and hence uses the same plan in all cases. EVA has similar performance for low accuracies, but uses increasingly larger object detection models as the accuracy requirement becomes more stringent. This enables VIVA to improve performance over EVA by up to 4.8 (times).\n",
      "\n",
      "It can be difficult to know when models should be updated or re-trained using optimizations like TASTI and BlazeIt. By selecting to use these models for lower accuracy requirements, but not for more stringent ones, VIVA can guide training decisions.\n",
      "\n",
      "7.5 Impact of Training and Indexing\n",
      "\n",
      "We next investigate how plan selection can be impacted by the need to construct an index or train a model for replacement or filtering at query time. This results in additional cost from training or indexing to plans that include hints with CAN FILTER or CAN REPLACE with FALLBACK ENABLED. We use the setup from Section 7.1 and focus on the News analysis query. We vary the training latency from 0sec (already exists) to 100sec in increments of 10sec.\n",
      "\n",
      "Figure 7 shows the end-to-end latency (y-axis) for the two baselines and VIVA as the training latency varies (x-axis). We note VIVA matches or outperforms EVA even when spending up to a minute for training, since it spends less time on query execution. The arrow shows the case for creating a TASTI [28] index, which is on the order of seconds if frame embeddings are available. Proxy models can be trained in tens to hundreds of seconds [25], which can still be worth this additional upfront cost. Furthermore, caching this model means VIVA only incurs a one-time training cost that can benefit future queries as well. As noted in Section 5.4, VIVA considers this training time when selecting the best plan to execute.\n",
      "\n",
      "7.6 Optimizing Across Hardware Platforms\n",
      "\n",
      "We now evaluate VIVA’s ability to generate and compare plans for different hardware platforms. We consider three instances: a standalone n1-highmem-16 (CPU), a n1-highmem-16 with a T4 GPU, and a n1-highmem-16 with a V100 GPU. We use the GCP pricing for each: 0.66 $/hr for CPU, 0.91 $/hr for T4, and 2.40 $/hr for V100 [14]. We use out-of-the-box GPU implementation and fallback to CPU implementations if not available on the GPU. We study three optimization goals: performance (fastest plan), cheapest price, or best end-to-end performance per dollar. We use the Traffic and TV News queries on the Event Present input.\n",
      "\n",
      "Figure 8 shows the results where VIVA optimizes for performance with the final dollar cost of the plan annotated. For Traffic analysis, the T4 GPU is 1.8 (times) faster than the CPU while being 30% cheaper. While the T4 GPU instance is more expensive, the faster execution means the instance can be provisioned for less time. Similarly for News analysis, execution with the T4 is ∼2 (times) faster and 42% cheaper. In both cases, the V100’s performance improvement of ∼2 (times) does not outweigh its high cost, 1.8 (times) more expensive compared to a CPU. As shown in Table 6, the optimizer chooses the same plan in all cases since object detection can be significantly accelerated on GPU compared to running on CPU and the latency is the only variable when estimating cost. In this study, we do not\n",
      "\n",
      "60%70%80%90%95%0200400600800Latency (sec)EVAVIVA60%70%80%90%95%0100020003000Latency (sec)EVAVIVA020406080100Model Training Latency (sec)02004006008001000End-to-End Latency (sec)EVAVIVA\f",
      "or used instead of VIVA’s execution engine to further accelerate queries. Several recent projects have also focused on optimizing aspects of video retrieval from storage and how video data are stored and decoded [8, 16, 30, 61]. These techniques are also important for end-to-end efficiency but are complimentary to this paper’s focus. Functional Dependencies. Functional dependencies [22, 34] help database designers automatically determine the relation of one attribute to another. However, existing work is limited to structured data that can be easily analyzed to determine relationships. Video analytics queries execute expensive DNNs over unstructured records, which makes it infeasible to infer the relationships without first materializing the results. Hints enable VIVA to consider additional query plans that can improve performance and cost without having to first materialize the results. Specifying Domain Knowledge. Providing extra knowledge to a system to improve query execution is an idea with roots in the early days of query processing. Hints most closely resemble early work in semantic integrity constraints [33], and more generally hints in existing database systems, such as MicrosoftSQL hints [37] and MySQL hints [39]. A key difference from domain knowledge for structured data is that ML models are probabilistic and require a system to consider and provide accuracy guarantees. VIVA reasons about the accuracy impact on plans using hints.\n",
      "\n",
      "9 CONCLUSION\n",
      "\n",
      "In this paper, we addressed the challenge of users having to manually explore performance-accuracy tradeoffs across combinations of optimizations in video analytics queries with multiple predicates. We proposed relational hints, a declarative interface to express ML model relationships, informed by domain specific knowledge. Relational hints eliminate the need for users to manually rewrite their queries when a new model becomes available and manually reason about how the use and order of the various models available impact their query’s performance and accuracy. To determine how and when relational hints can be used to optimize queries, we designed the VIVA video analytics system. VIVA uses hints that are validated for each query to generate additional query plans using a formal set of transformations, and selects the best performance plan that meets user accuracy requirements. Using relational hints, we show that VIVA over Spark improves performance up to 16.6 (times) without sacrificing accuracy for a range of complex queries.\n",
      "\n",
      "ACKNOWLEDGEMENTS\n",
      "\n",
      "We thank the Stanford Platform Lab and its affiliates (Cisco, Facebook, Google, Nasdaq, NEC, VMware, and Wells Fargo), the Open Philanthropy project, and Sutter Hill Ventures. We also thank affiliates of the Stanford DAWN project—Ant Financial, Facebook, Google, and VMware—as well as Toyota Research Institute (“TRI”), Cisco, SAP, and the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. TRI provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. Francisco Romero was supported by a Stanford DARE Fellowship.\n",
      "\n",
      "Figure 8: VIVA Execution using the CPU, T4 GPU, and V100 GPU. Latencies presented are the fastest plans given the available hardware.\n",
      "\n",
      "Table 6: Hardware Platform Selection. Perf./$ normalized to CPU. Chosen HW is bolded. ∗Model executes on CPU if GPU selected.\n",
      "\n",
      "App.\n",
      "\n",
      "Traffic\n",
      "\n",
      "News\n",
      "\n",
      "Opt. Target (HW avail.) Perf. (CPU, T4, V100) Cost (CPU, T4) Cost (CPU, V100) Perf. (CPU, T4, V100) Cost (CPU, T4) Cost (CPU, V100)\n",
      "\n",
      "Selected Plan\n",
      "\n",
      "Perf./$\n",
      "\n",
      "RP(Obj.) ∧ ObjTrack ∧ RP(TimeOfDay)∗\n",
      "\n",
      "RP(Obj.) ∧ Face ∧ RPF(Emo.)∗ ∧ RP(Emo.)\n",
      "\n",
      "1.04 2.33 1.00 1.31 2.23 1.00\n",
      "\n",
      "include mixed precision models that could take advantage of halfprecision units on the GPU. Such models can be defined as CAN REPLACE hints and be considered during query optimization.\n",
      "\n",
      "Table 6 shows the results when VIVA optimizes for cost. VIVA selects a different hardware platform depending on hardware availability. Consistent with our previous results, when optimizing for dollar cost, VIVA will favor a CPU plan if the accelerator available is the V100 and favor the T4 over the CPU. When optimizing for performance per dollar, VIVA will choose the T4 plan since it is up to 2.3 (times) better than the plans for CPU and V100.\n",
      "\n",
      "8 RELATED WORK\n",
      "\n",
      "Accelerating Queries via Specialization. A large body of work uses cheap approximations to accelerate specific classes of queries, ranging from selection [26, 27, 36, 63], aggregation [25], and aggregation with predicates [29]. There is also work on using embedding indexes as cheap approximations [19, 28]. VIVA is the first system to provide a general interface, hints, that captures these optimizations and their impact on complex query performance and accuracy. Video Frame Sampling. Several projects have focused on decreasing the amount of data models need to process via dynamic sampling rates. MIRIS [3] executes object detection and object tracking at reduced framerates and increases the framerate for low confidence detections. ExSample [38] splits a video dataset into temporal chunks and prioritizes processing chunks with higher probabilities of finding a new object. It iteratively updates its estimates as more frames are processed by leveraging an adaptive sampling algorithm based on Thompson sampling [49]. Depending on the query type, varying the sampling rate can affect the accuracy since lower sampling rates may lead to missed objects. The optimizations enabled by hints are orthogonal to existing sampling techniques. Optimizing ML Execution and Storage. Systems such as Scanner [42], VideoStorm [65], and Llama [48] have focused on optimizing DNN execution by efficiently utilizing hardware resources for execution plans for video analytics. The scale-out and serverless techniques underpinning these systems are complementary to optimization with hints. Hence these systems can be integrated into,\n",
      "\n",
      "TrafficNews0200400600Latency (sec)$0.10$0.07$0.07$0.05$0.18$0.12VIVA (CPU)VIVA (T4)VIVA (V100)\f",
      "REFERENCES [1] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A system for large-scale machine learning. In OSDI. [2] Internet Archive. 2022. TV News Archive. https://archive.org/details/tv. [3] Favyen Bastani, Songtao He, Arjun Balasingam, Karthik Gopalakrishnan, Mohammad Alizadeh, Hari Balakrishnan, Michael Cafarella, Tim Kraska, and Sam Madden. 2020. MIRIS: Fast Object Track Queries in Video. In SIGMOD.\n",
      "\n",
      "[4] G. Bradski. 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools\n",
      "\n",
      "[5]\n",
      "\n",
      "(2000). Jiashen Cao, Ramyad Hadidi, Joy Arulraj, and Hyesoon Kim. 2021. THIA: Accelerating Video Analytics using Early Inference and Fine-Grained Query Planning. arXiv:2102.08481 Jiashen Cao, Karan Sarkar, Ramyad Hadidi, Joy Arulraj, and Hyesoon Kim. 2022. FiGO: Fine-Grained Query Optimization in Video Analytics. In SIGMOD. [7] Surajit Chaudhuri, Bolin Ding, and Srikanth Kandula. 2017. Approximate query\n",
      "\n",
      "[6]\n",
      "\n",
      "processing: No silver bullet. In SIGMOD.\n",
      "\n",
      "[8] Maureen Daum, Haynes Brandon, Dong He, Amrita Mazumdar, and Magdalena Balazinska. 2021. TASM: A Tile-Based Storage Manager for Video Analytics. In ICDE.\n",
      "\n",
      "[9] Maureen Daum, Enhao Zhang, Dong He, Magdalena Balazinska, Brandon Haynes, Ranjay Krishna, Apryle Craig, and Aaron Wirsing. 2022. VOCAL: Video Organization and Interactive Compositional AnaLytics. In CIDR.\n",
      "\n",
      "[10] Tim Esler. 2022. InceptionResNet Face Recognition in PyTorch. https://github.\n",
      "\n",
      "com/timesler/facenet-pytorch\n",
      "\n",
      "[11] FFmpeg. 2022. FFmpeg. https://ffmpeg.org/. [12] Daniel Y Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang, Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher Ré, and Kayvon Fatahalian. 2019. Rekall: Specifying video events using compositions of spatiotemporal labels. arXiv:1910.02993\n",
      "\n",
      "[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX:\n",
      "\n",
      "Exceeding YOLO Series in 2021. arXiv:2107.08430\n",
      "\n",
      "[14] Google. 2022. Google Compute Engine: GPUs Pricing. https://cloud.google.com/\n",
      "\n",
      "compute/gpus-pricing.\n",
      "\n",
      "[15] Google. 2022. Tesseract OCR. https://github.com/tesseract-ocr/tesseract. [16] Brandon Haynes, Maureen Daum, Dong He, Amrita Mazumdar, Magdalena Balazinska, Alvin Cheung, and Luis Ceze. 2021. VSS: A Storage System for Video Analytics. In SIGMOD.\n",
      "\n",
      "[17] Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Cristian Canton Ferrer. 2021. Towards Measuring Fairness in AI: the Casual Conversations Dataset. arXiv:2104.02821\n",
      "\n",
      "[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn.\n",
      "\n",
      "In ICCV. IEEE.\n",
      "\n",
      "[19] Wenjia He, Michael R. Anderson, Maxwell Strome, and Michael Cafarella. 2020.\n",
      "\n",
      "[20]\n",
      "\n",
      "[21]\n",
      "\n",
      "[22]\n",
      "\n",
      "A Method for Optimizing Opaque Filter Queries. In SIGMOD. James Hong, Will Crichton, Haotian Zhang, Daniel Y. Fu, Jacob Ritchie, Jeremy Barenholtz, Ben Hannel, Xinwei Yao, Michaela Murray, Geraldine Moriba, Maneesh Agrawala, and Kayvon Fatahalian. 2021. Analysis of Faces in a Decade of US Cable TV News. In SIGKDD. James Hong, Matthew Fisher, Michaël Gharbi, and Kayvon Fatahalian. 2021. Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition. In ICCV. Ihab F. Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboulnaga. 2004. CORDS: Automatic Discovery of Correlations and Soft Functional Dependencies. In SIGMOD.\n",
      "\n",
      "[23] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In CVPR.\n",
      "\n",
      "[24] Angela H Jiang, Daniel L-K Wong, Christopher Canel, Lilia Tang, Ishan Misra, Michael Kaminsky, Michael A Kozuch, Padmanabhan Pillai, David G Andersen, and Gregory R Ganger. 2018. Mainstream: Dynamic {Stem-Sharing} for {MultiTenant} Video Processing. In ATC.\n",
      "\n",
      "[25] Daniel Kang, Peter Bailis, and Matei Zaharia. 2019. BlazeIt: Optimizing Declarative Aggregation and Limit Queries for Neural Network-Based Video Analytics. In PVLDB.\n",
      "\n",
      "[26] Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, and Matei Zaharia. 2017. NoScope: optimizing neural network queries over video at scale. In PVLDB. [27] Daniel Kang, Edward Gan, Peter Bailis, Tatsunori Hashimoto, and Matei Zaharia. 2020. Approximate Selection with Guarantees using Proxies. In PVLDB. [28] Daniel Kang, John Guibas, Peter Bailis, Tatsunori Hashimoto, and Matei Zaharia. 2022. Semantic Indexes for Machine Learning-based Queries over Unstructured Data. In SIGMOD.\n",
      "\n",
      "[29] Daniel Kang, John Guibas, Peter Bailis, Yi Sun, Tatsunori Hashimoto, and Matei Zaharia. 2021. Accelerating Approximate Aggregation Queries with Expensive Predicates. In PVLDB.\n",
      "\n",
      "[30] Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, and Matei Zaharia. 2021. Jointly optimizing preprocessing and inference for DNN-based visual analytics. In PVLDB.\n",
      "\n",
      "[31] Daniel Kang, Francisco Romero, Peter Bailis, Christos Kozyrakis, and Matei Zaharia. 2022. VIVA: An End-to-End System for Interactive Video Analytics. In CIDR.\n",
      "\n",
      "[32] Fiodar Kazhamiaka, Matei Zaharia, and Peter Bailis. 2021. Challenges and Opportunities for Autonomous Vehicle Query Systems. In CIDR. Jonathan J King. 1979. Exploring the Use of Domain Knowledge for Query Processing Efficiency. Technical Report. Stanford University, Dept of Computer Science.\n",
      "\n",
      "[33]\n",
      "\n",
      "[34] Sebastian Kruse and Felix Naumann. 2018. Efficient Discovery of Approximate\n",
      "\n",
      "Dependencies. In PVLDB.\n",
      "\n",
      "[35] Michael A Laurenzano, Parker Hill, Mehrzad Samadi, Scott Mahlke, Jason Mars, and Lingjia Tang. 2016. Input responsiveness: using canary inputs to dynamically steer approximation. In PLDI.\n",
      "\n",
      "[36] Yao Lu, Aakanksha Chowdhery, Srikanth Kandula, and Surajit Chaudhuri. 2018. Accelerating Machine Learning Inference with Probabilistic Predicates. In SIGMOD.\n",
      "\n",
      "[37] Microsoft. 2022. MicrosoftSQL Hints (Transact-SQL). https://docs.microsoft.\n",
      "\n",
      "com/en-us/sql/t-sql/queries/hints-transact-sql?view=sql-server-ver15. [38] Oscar Moll, Favyen Bastani, Sam Madden, Mike Stonebraker, Vijay Gadepally, and Tim Kraska. 2022. ExSample: Efficient Searches on Video Repositories through Adaptive Sampling. In ICDE.\n",
      "\n",
      "[39] MySQL. 2022. MySQL Optimizer Hints. https://dev.mysql.com/doc/refman/8.0/\n",
      "\n",
      "en/optimizer-hints.html.\n",
      "\n",
      "[40] Scoreboard OCR. 2022. Scoreboard OCR. https://scoreboard-ocr.com/start. [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS.\n",
      "\n",
      "[42] Alex Poms, William Crichton, Pat Hanrahan, and Kayvon Fatahalian. 2018. Scanner: Efficient Video Analysis at Scale. In SIGGRAPH.\n",
      "\n",
      "[43] PyTorch. 2022. 3D ResNet Video Classification in PyTorch. https://pytorch.org/\n",
      "\n",
      "hub/facebookresearch_pytorchvideo_resnet/\n",
      "\n",
      "[44] PyTorch. 2022. Image Classification: Models and Pre-Trained Weights. https:\n",
      "\n",
      "//pytorch.org/vision/stable/models.html#classification\n",
      "\n",
      "[45] PyTorch. 2022.\n",
      "\n",
      "Object Detection: Models and Pre-Trained Weights.\n",
      "\n",
      "https://pytorch.org/vision/stable/models.html#object-detection-instancesegmentation-and-person-keypoint-detection\n",
      "\n",
      "[46] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott Gardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius, Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. 2020. MLPerf Inference Benchmark. In ISCA. [47] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. 2021.\n",
      "\n",
      "INFaaS: Automated Model-less Inference Serving. In ATC.\n",
      "\n",
      "[48] Francisco Romero, Mark Zhao, Neeraja J. Yadwadkar, and Christos Kozyrakis. 2021. Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines. In SoCC.\n",
      "\n",
      "[49] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen.\n",
      "\n",
      "2017. A Tutorial on Thompson Sampling. arXiv:1707.02038\n",
      "\n",
      "[50] Scikit. 2022. Support Vector Classifier in Scikit. https://scikit-learn.org/stable/\n",
      "\n",
      "modules/generated/sklearn.svm.SVC.html\n",
      "\n",
      "[51] Sefik Ilkin Serengil and Alper Ozpinar. 2021. HyperExtended LightFace: A Facial\n",
      "\n",
      "Attribute Analysis Framework. In ICEET.\n",
      "\n",
      "[52] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. In SOSP. Justin Shenk. 2022. Facial Expression Recognition. justinshenk/fer\n",
      "\n",
      "https://github.com/\n",
      "\n",
      "[53]\n",
      "\n",
      "[54] TensorFlow. 2022. Signatures in TensorFlow Lite. https://www.tensorflow.org/\n",
      "\n",
      "lite/guide/signatures.\n",
      "\n",
      "[55] Ultralytics. 2022. Yolov5 Object Detection in PyTorch. https://github.com/\n",
      "\n",
      "ultralytics/yolov5.\n",
      "\n",
      "[56] Paul Viola and Michael Jones. 2001. Rapid object detection using a boosted\n",
      "\n",
      "cascade of simple features. In CVPR.\n",
      "\n",
      "[57] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. 2021. You Only Learn One Representation: Unified Network for Multiple Tasks. arXiv:2105.04206 [58] Yongji Wu, Matthew Lentz, Danyang Zhuo, and Yao Lu. 2023. Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures. In PVLDB.\n",
      "\n",
      "[59] Ran Xu, Jinkyu Koo, Rakesh Kumar, Peter Bai, Subrata Mitra, Sasa Misailovic, and Saurabh Bagchi. 2018. VideoChef: Efficient Approximation for Streaming Video Processing Pipelines. In ATC.\n",
      "\n",
      "[60] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua Lai. 2022. PP-YOLOE: An evolved version of YOLO. arXiv:2203.16250\n",
      "\n",
      "[61] Tiantu Xu, Luis Materon Botelho, and Felix Xiaozhu Lin. 2019. VStore: A Data\n",
      "\n",
      "Store for Analytics on Large Videos. In EuroSys.\n",
      "\n",
      "[62] Zhuangdi Xu, Gaurav Kakker, Joy Arulraj, and Umakishore Ramachandran. 2022. EVA: A Symbolic Approach to Accelerating Exploratory Video Analytics with Materialized Views. In SIGMOD.\n",
      "\n",
      "[63] Zhihui Yang, Zuozhi Wang, Yicong Huang, Yao Lu, Chen Li, and X. Sean Wang. 2022. Optimizing Machine Learning Inference Queries with Correlative Proxy Models. In PVLDB.\n",
      "\n",
      "[64] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica. 2010. Spark: Cluster computing with working sets.. In HotCloud. [65] Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, and Michael J Freedman. 2017. Live Video Analytics at Scale with Approximation and Delay-Tolerance. In NSDI. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in text.replace('\\n\\n\\x0c', ' ').replace('-\\n\\n', '').replace('-\\n', '').replace('•', '(bullet point)').replace('×', ' (times)').split('\\n\\n'):\n",
    "    print(p.replace('\\n', ' '))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400708d7-6f25-4afc-baa1-72cde4efd5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

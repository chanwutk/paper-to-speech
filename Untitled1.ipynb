{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bad6b3e9-8506-4b8f-ad48-65124934fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marko\n",
    "import os\n",
    "import re\n",
    "\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca560e9-91f6-421a-9f0a-de057efda3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>test</h2>\n",
      "<p>test <em>testtest</em></p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(marko.convert('test\\n----\\ntest _testtest_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a6878b34-001a-4cd3-8877-9db4765c7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h6>Abstract</h6>\n",
      "<h2>1 Introduction</h2>\n",
      "<h2>2 Background</h2>\n",
      "<h2>3 NOSCOPE Architecture</h2>\n",
      "<h2>4 Model Specialization</h2>\n",
      "<h2>5 Difference Detection</h2>\n",
      "<h2>6 Cost-Based Model Search</h2>\n",
      "<h3>6.1 Training Data Generation</h3>\n",
      "<h3>6.2 Cost Model</h3>\n",
      "<h3>6.3 Model Search</h3>\n",
      "<h2>7 Implementation</h2>\n",
      "<h2>8 Limitations</h2>\n",
      "<h2>9 Evaluation</h2>\n",
      "<h3>9.1 Experimental Setup</h3>\n",
      "<h3>9.2 End-to-End Performance</h3>\n",
      "<h3>9.3 Impact of the CBO</h3>\n",
      "<h4>9.3.1 Running Time of the CBO</h4>\n",
      "<h3>9.4 Impact of Individual Models</h3>\n",
      "<h3>9.5 Impact of Model Specialization</h3>\n",
      "<h3>9.6 Comparison Against Baselines</h3>\n",
      "<h2>10 Related Work</h2>\n",
      "<h2>11 Conclusions</h2>\n",
      "<h2>Acknowledgements</h2>\n",
      "<h2>References</h2>\n",
      "\n",
      "-----\n",
      "<h2>1 Introduction</h2>\n",
      "\n",
      "<p>Video represents a rich source of high-value, high-volume data: video comprised over 70% of all Internet traffic [<a href=\"#bib.bib2\">2</a>] in 2015 and over 300 hours of video are uploaded to YouTube every minute [<a href=\"#bib.bib3\">3</a>]. We can leverage this video data to answer queries about the physical world, our lives and relationships, and our evolving society.</p>\n",
      "<p>Figure 1: NoScope is a system for accelerating neural network analysis over videos via inference-optimized model search. Given an input video, target object, and reference neural network, NoScope automatically searches for and trains a cascade of models—including difference detectors and specialized networks—that can reproduce the binarized outputs of the reference network with high accuracy—but up to three orders of magnitude faster.</p>\n",
      "<p>It is increasingly infeasible—both too costly and too slow—to rely on manual, human-based inspection of large-scale video data. Thus, automated analysis is critical to answering these queries at scale. The literature offers a wealth of proposals for storing and querying [<a href=\"#bib.bib7\">7</a>, <a href=\"#bib.bib59\">59</a>, <a href=\"#bib.bib101\">101</a>, <a href=\"#bib.bib73\">73</a>, <a href=\"#bib.bib6\">6</a>, <a href=\"#bib.bib36\">36</a>, <a href=\"#bib.bib57\">57</a>, <a href=\"#bib.bib38\">38</a>, <a href=\"#bib.bib48\">48</a>, <a href=\"#bib.bib94\">94</a>] videos, largely based on classical computer vision techniques. In recent times, however, deep <em>neural networks (NNs)</em> [<a href=\"#bib.bib58\">58</a>, <a href=\"#bib.bib40\">40</a>, <a href=\"#bib.bib66\">66</a>, <a href=\"#bib.bib44\">44</a>, <a href=\"#bib.bib67\">67</a>] have largely displaced classical computer vision methods due to their incredible accuracy—often rivaling or exceeding human capabilities—in visual analyses ranging from object classification [<a href=\"#bib.bib82\">82</a>] to image-based cancer diagnosis [<a href=\"#bib.bib31\">31</a>, <a href=\"#bib.bib95\">95</a>].</p>\n",
      "<p>Unfortunately, applying NNs to video data is prohibitively expensive at scale. The fastest NNs for accurate object detection run at 30-80 frames per second (fps), or 1-2.5 × real time (e.g., 50 fps on an NVIDIA K80 GPU, ~$4000 retail, $0.70-0.90 per hour on cloud; 80 fps on an NVIDIA P100, ~$4600 retail) [<a href=\"#bib.bib80\">80</a>, <a href=\"#bib.bib79\">79</a>, <a href=\"#bib.bib81\">81</a>].11 1 In this work, we focus on multi-scale object detection, or identifying objects regardless of their scale in the image [<a href=\"#bib.bib80\">80</a>, <a href=\"#bib.bib79\">79</a>, <a href=\"#bib.bib81\">81</a>]. Object detection models are more costly than classification models, which process images pre-centered on an object of interest, but are required to find objects in most realistic video applications. Given continued decreases in image sensor costs (e.g., &lt; $0.65 for a 640x480 VGA CMOS sensor), the computational overheads of NNs lead to a three order-of-magnitude imbalance between the cost of data acquisition and the cost of data processing. Moreover, state-of-the-art NNs continue to get deeper and more costly to evaluate; for example, Google’s winning NN in the 2014 ImageNet competition had 22 layers; Microsoft’s winning NN in 2015 had 152 layers [<a href=\"#bib.bib44\">44</a>].</p>\n",
      "<p>In response, we present NoScope, a system for querying videos that can reduce the cost of NN-based analysis by up to three orders of magnitude via <em>inference-optimized model search</em>. Our NoScope prototype supports queries in the form of the presence or absence of a particular object class. Given a query consisting of a target video, object to detect, and reference <em>pre-trained</em> neural network (e.g., webcam in Taipei, buses, YOLOv2 [<a href=\"#bib.bib79\">79</a>]), NoScope automatically searches for and trains a sequence, or <em>cascade</em> [<a href=\"#bib.bib90\">90</a>], of models that preserves the accuracy of the reference network but are specialized to the target query and are therefore far less computationally expensive. That is, instead of simply running the reference NN over the target video, NoScope searches for, learns, and executes a query-specific pipeline of cheaper models that approximates the reference model to a specified target accuracy. NoScope’s query-specific pipelines forego the generality of the reference NN—that is, NoScope’s cascades are <em>only</em> accurate in detecting the target object in the target video—but in turn execute up to three orders of magnitude faster (i.e., 265-15,500 × real-time) with 1-5% loss in accuracy for binary detection tasks over real-world fixed-angle webcam and surveillance video. To do so, NoScope leverages both new types of models and a new optimizer for model search:</p>\n",
      "<p>First, NoScope’s <em>specialized models</em> forego the full generality of the reference NN but faithfully mimic its behavior for the target query. In the context of our example query of detecting buses, consider the following buses that appeared in a public webcam in Taipei:</p>\n",
      "<p><img src=\"figures/examples/taipei-diff/cropped/0.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-diff/cropped/2.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-diff/cropped/6.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-diff/cropped/7.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-diff/cropped/10.jpg\" alt=\"[Uncaptioned image]\" /></p>\n",
      "<p>In this video stream, buses only appear from a small set of perspectives. In contrast, NNs are often trained to recognize thousands of objects, from sheep to apples, and from different angles; this leads to unnecessary computational overhead. Thus, NoScope instead performs <em>model specialization</em>, using the full NN to generate labeled training data (i.e., examples) and subsequently training smaller NNs that are tailored to a given video stream and to a smaller class of objects. NoScope then executes these specialized models, which are up to 340 × faster than the full NN, and consults the full NN only when the specialized models are uncertain (i.e., produce results with confidence below an automatically learned threshold).</p>\n",
      "<p>Second, NoScope’s <em>difference detectors</em> highlight temporal differences across frames. Consider the following frames, which appeared sequentially in our Taipei webcam:</p>\n",
      "<p><img src=\"figures/examples/taipei-similar/cropped/0.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-similar/cropped/1.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-similar/cropped/2.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-similar/cropped/3.jpg\" alt=\"[Uncaptioned image]\" /><img src=\"figures/examples/taipei-similar/cropped/4.jpg\" alt=\"[Uncaptioned image]\" /></p>\n",
      "<p>These frames are nearly identical, and all contain the same bus. Therefore, instead of running the full NN (or a specialized NN) on each frame, NoScope learns a low-cost difference detector (based on differences of frame content) that determines whether the contents have changed across frames. NoScope’s difference detectors are fast and accurate—up to 100k frames per second on the CPU.</p>\n",
      "<p>A key challenge in combining the above insights and models is that the optimal choice of cascade is data-dependent. Individual model performance varies across videos, with distinct trade-offs between speed, selectivity, and accuracy. For example, a difference detector based on subtraction from the previous frame might work well on mostly static scenes but may add overhead in a video overseeing a busy highway. Likewise, the complexity (e.g., number of layers) of specialized NNs required to recognize different object classes varies widely based on both the target object and video. Even setting the thresholds in the cascade represents trade-off: should we make a difference detector’s threshold less aggressive to reduce its false negative rate, or should we make it more aggressive to eliminate more frames early in the pipeline and avoid calling a more expensive model?</p>\n",
      "<p>To solve this problem, NoScope performs inference-optimized model search using a cost-based optimizer that automatically finds a fast model cascade for a given query and accuracy target. The optimizer applies candidate models to training data, then computes the optimal thresholds for each combination of models using an efficient linear parameter sweep through the space of feasible thresholds. The entire search requires time comparable to labeling the sample data using the reference NN (an unavoidable step in obtaining such data).</p>\n",
      "<p>We evaluate a NoScope prototype on binary classification tasks on cameras that are in a fixed location and at a fixed angle; this includes pedestrian and automotive detection as found in monitoring and surveillance applications. NoScope demonstrates up to three order of magnitude speedups over general-purpose state-of-the-art NNs while retaining high—and configurable—accuracy (within 1-5%) across a range of videos, indicating a promising new strategy for efficient inference and analysis of video data. In summary, we make the following contributions in this work:</p>\n",
      "<ol>\n",
      "<li>\n",
      "<ol>\n",
      "<li>\n",
      "</li>\n",
      "</ol>\n",
      "<p>NoScope, a system for accelerating neural network queries over video via inference-optimized model search.</p>\n",
      "</li>\n",
      "<li>\n",
      "<ol start=\"2\">\n",
      "<li>\n",
      "</li>\n",
      "</ol>\n",
      "<p>New techniques for a ) neural network model specialization based on a given video and query; b ) fast difference detection across frames; and c ) cost-based optimization to automatically identify the fastest cascade for a given accuracy target.</p>\n",
      "</li>\n",
      "<li>\n",
      "<ol start=\"3\">\n",
      "<li>\n",
      "</li>\n",
      "</ol>\n",
      "<p>An evaluation of NoScope on fixed-angle binary classification demonstrating up to three orders of magnitude speedups on real-world data.</p>\n",
      "</li>\n",
      "</ol>\n",
      "<p>The remainder of this paper proceeds as follows. Section <a href=\"#S2\" title=\"2 Background ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">2</a> provides additional background on NNs and our target environment. Section <a href=\"#S3\" title=\"3 NOSCOPE Architecture ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">3</a> describes the NoScope architecture. Section <a href=\"#S4\" title=\"4 Model Specialization ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">4</a> describes NoScope’s use of model specialization, Section <a href=\"#S5\" title=\"5 Difference Detection ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">5</a> describes NoScope’s difference detectors, and Section <a href=\"#S6\" title=\"6 Cost-Based Model Search ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">6</a> describes NoScope’s inference-optimized model search via cost-based optimization. Section <a href=\"#S7\" title=\"7 Implementation ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">7</a> describes the NoScope prototype implementation and Section <a href=\"#S8\" title=\"8 Limitations ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">8</a> describes limitations of the current system. Section <a href=\"#S9\" title=\"9 Evaluation ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">9</a> experimentally evaluates NoScope, Section <a href=\"#S10\" title=\"10 Related Work ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">10</a> discusses related work, and Section <a href=\"#S11\" title=\"11 Conclusions ‣ NoScope: Optimizing Neural Network Queries over Video at Scale\">11</a> concludes.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('noscope.md', 'r') as f:\n",
    "    html = marko.convert(f.read())\n",
    "    # for l in re.findall('\\[(<a .*?<\\/a>)\\]', html):\n",
    "    #     print(\"\\n  \".join(l.split(\", \")))\n",
    "    # print()\n",
    "    # print()\n",
    "    # print()\n",
    "    # print()\n",
    "\n",
    "\n",
    "    \n",
    "    # html = re.sub(\n",
    "    #     '\\[(<a .*?<\\/a>)\\]',\n",
    "    #     # '\\n\\n\\n\\n\\\\1\\n\\n\\n\\n',\n",
    "    #     '',\n",
    "    #     html\n",
    "    # )\n",
    "    # html = re.sub(\n",
    "    #     '<a.*?>(.*?)<\\/a>',\n",
    "    #     '\\\\1',\n",
    "    #     html\n",
    "    # )\n",
    "    # html = re.sub('<br \\/>', '\\n', html)\n",
    "    # # print(\n",
    "    # #     html\n",
    "    # #         # .replace('<br />', ' ')\n",
    "    # # )\n",
    "    # html = re.sub(\n",
    "    #     '<img src=\".*?\" alt=\"\\[Uncaptioned image\\]\" />',\n",
    "    #     ' Uncaptioned image.',\n",
    "    #     html,\n",
    "    # )\n",
    "    # html = re.sub(\n",
    "    #     '<img src=\".*?\" alt=\"\\[(.*?)\\]\" />',\n",
    "    #     ' Image with caption: \\\\1. End caption.',\n",
    "    #     html,\n",
    "    # )\n",
    "    # html = html.replace('×', ' times ')\n",
    "\n",
    "    headers = re.findall('<h\\d>[^<]*?<\\/h\\d>', html)\n",
    "    bodies = re.split('<h\\d>[^<]*?<\\/h\\d>', html)\n",
    "    print('\\n'.join(headers))\n",
    "    # print(bodies[3])\n",
    "    for body in bodies:\n",
    "        for l in body.split('\\n'):\n",
    "            if l.startswith('<p>'):\n",
    "                assert l.endswith('</p>')\n",
    "\n",
    "    contents = [{'content': bodies[0]}]\n",
    "    bodies = bodies[1:]\n",
    "    contents += [\n",
    "        {\n",
    "            'heading': \" \".join(heading.split('\\n')),\n",
    "            'content': content,\n",
    "        }\n",
    "        for heading, content\n",
    "        in zip(headers, bodies)\n",
    "    ]\n",
    "    print()\n",
    "    print('-----')\n",
    "    print(contents[2]['heading'])\n",
    "    print(contents[2]['content'])\n",
    "    # parser = HTMLParser()\n",
    "    # print(contents[3]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831ba07-f073-4d9f-84c2-51a9111e05ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d99c8-e4a5-4006-a5fa-05ac3b62e776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a63f4f0-afe4-4d53-9af7-3c2d8b95aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1badf845-9161-4448-a2ff-cbe5c113b59e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tts_models/multilingual/multi-dataset/xtts_v2\n",
      "tts_models/multilingual/multi-dataset/xtts_v1.1\n",
      "tts_models/multilingual/multi-dataset/your_tts\n",
      "tts_models/multilingual/multi-dataset/bark\n",
      "tts_models/bg/cv/vits\n",
      "tts_models/cs/cv/vits\n",
      "tts_models/da/cv/vits\n",
      "tts_models/et/cv/vits\n",
      "tts_models/ga/cv/vits\n",
      "tts_models/en/ek1/tacotron2\n",
      "tts_models/en/ljspeech/tacotron2-DDC\n",
      "tts_models/en/ljspeech/tacotron2-DDC_ph\n",
      "tts_models/en/ljspeech/glow-tts\n",
      "tts_models/en/ljspeech/speedy-speech\n",
      "tts_models/en/ljspeech/tacotron2-DCA\n",
      "tts_models/en/ljspeech/vits\n",
      "tts_models/en/ljspeech/vits--neon\n",
      "tts_models/en/ljspeech/fast_pitch\n",
      "tts_models/en/ljspeech/overflow\n",
      "tts_models/en/ljspeech/neural_hmm\n",
      "tts_models/en/vctk/vits\n",
      "tts_models/en/vctk/fast_pitch\n",
      "tts_models/en/sam/tacotron-DDC\n",
      "tts_models/en/blizzard2013/capacitron-t2-c50\n",
      "tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
      "tts_models/en/multi-dataset/tortoise-v2\n",
      "tts_models/en/jenny/jenny\n",
      "tts_models/es/mai/tacotron2-DDC\n",
      "tts_models/es/css10/vits\n",
      "tts_models/fr/mai/tacotron2-DDC\n",
      "tts_models/fr/css10/vits\n",
      "tts_models/uk/mai/glow-tts\n",
      "tts_models/uk/mai/vits\n",
      "tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      "tts_models/nl/mai/tacotron2-DDC\n",
      "tts_models/nl/css10/vits\n",
      "tts_models/de/thorsten/tacotron2-DCA\n",
      "tts_models/de/thorsten/vits\n",
      "tts_models/de/thorsten/tacotron2-DDC\n",
      "tts_models/de/css10/vits-neon\n",
      "tts_models/ja/kokoro/tacotron2-DDC\n",
      "tts_models/tr/common-voice/glow-tts\n",
      "tts_models/it/mai_female/glow-tts\n",
      "tts_models/it/mai_female/vits\n",
      "tts_models/it/mai_male/glow-tts\n",
      "tts_models/it/mai_male/vits\n",
      "tts_models/ewe/openbible/vits\n",
      "tts_models/hau/openbible/vits\n",
      "tts_models/lin/openbible/vits\n",
      "tts_models/tw_akuapem/openbible/vits\n",
      "tts_models/tw_asante/openbible/vits\n",
      "tts_models/yor/openbible/vits\n",
      "tts_models/hu/css10/vits\n",
      "tts_models/el/cv/vits\n",
      "tts_models/fi/css10/vits\n",
      "tts_models/hr/cv/vits\n",
      "tts_models/lt/cv/vits\n",
      "tts_models/lv/cv/vits\n",
      "tts_models/mt/cv/vits\n",
      "tts_models/pl/mai_female/vits\n",
      "tts_models/pt/cv/vits\n",
      "tts_models/ro/cv/vits\n",
      "tts_models/sk/cv/vits\n",
      "tts_models/sl/cv/vits\n",
      "tts_models/sv/cv/vits\n",
      "tts_models/ca/custom/vits\n",
      "tts_models/fa/custom/glow-tts\n",
      "tts_models/bn/custom/vits-male\n",
      "tts_models/bn/custom/vits-female\n",
      "tts_models/be/common-voice/glow-tts\n",
      "vocoder_models/universal/libri-tts/wavegrad\n",
      "vocoder_models/universal/libri-tts/fullband-melgan\n",
      "vocoder_models/en/ek1/wavegrad\n",
      "vocoder_models/en/ljspeech/multiband-melgan\n",
      "vocoder_models/en/ljspeech/hifigan_v2\n",
      "vocoder_models/en/ljspeech/univnet\n",
      "vocoder_models/en/blizzard2013/hifigan_v2\n",
      "vocoder_models/en/vctk/hifigan_v2\n",
      "vocoder_models/en/sam/hifigan_v2\n",
      "vocoder_models/nl/mai/parallel-wavegan\n",
      "vocoder_models/de/thorsten/wavegrad\n",
      "vocoder_models/de/thorsten/fullband-melgan\n",
      "vocoder_models/de/thorsten/hifigan_v1\n",
      "vocoder_models/ja/kokoro/hifigan_v1\n",
      "vocoder_models/uk/mai/multiband-melgan\n",
      "vocoder_models/tr/common-voice/hifigan\n",
      "vocoder_models/be/common-voice/hifigan\n",
      "voice_conversion_models/multilingual/vctk/freevc24\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(TTS().list_models().list_models()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec985cef-2c8d-444a-82d7-a90130743a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd455f0c-4496-4ed7-ad61-6dedbff9821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices = [device + f':{i}' for i in range(1, 6)]\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c84ee71-9b51-4ef6-8214-06792736af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7396e6a-5826-4881-831f-22dca3c43eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chanwutk/projects/paper-to-speech/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    }
   ],
   "source": [
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e37885-6a23-49ba-8971-8b381f1082e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Claribel Dervla',\n",
       " 'Daisy Studious',\n",
       " 'Gracie Wise',\n",
       " 'Tammie Ema',\n",
       " 'Alison Dietlinde',\n",
       " 'Ana Florence',\n",
       " 'Annmarie Nele',\n",
       " 'Asya Anara',\n",
       " 'Brenda Stern',\n",
       " 'Gitta Nikolina',\n",
       " 'Henriette Usha',\n",
       " 'Sofia Hellen',\n",
       " 'Tammy Grit',\n",
       " 'Tanja Adelina',\n",
       " 'Vjollca Johnnie',\n",
       " 'Andrew Chipper',\n",
       " 'Badr Odhiambo',\n",
       " 'Dionisio Schuyler',\n",
       " 'Royston Min',\n",
       " 'Viktor Eka',\n",
       " 'Abrahan Mack',\n",
       " 'Adde Michal',\n",
       " 'Baldur Sanjin',\n",
       " 'Craig Gutsy',\n",
       " 'Damien Black',\n",
       " 'Gilberto Mathias',\n",
       " 'Ilkin Urbano',\n",
       " 'Kazuhiko Atallah',\n",
       " 'Ludvig Milivoj',\n",
       " 'Suad Qasim',\n",
       " 'Torcull Diarmuid',\n",
       " 'Viktor Menelaos',\n",
       " 'Zacharie Aimilios',\n",
       " 'Nova Hogarth',\n",
       " 'Maja Ruoho',\n",
       " 'Uta Obando',\n",
       " 'Lidiya Szekeres',\n",
       " 'Chandra MacFarland',\n",
       " 'Szofi Granger',\n",
       " 'Camilla Holmström',\n",
       " 'Lilya Stainthorpe',\n",
       " 'Zofija Kendrick',\n",
       " 'Narelle Moon',\n",
       " 'Barbora MacLean',\n",
       " 'Alexandra Hisakawa',\n",
       " 'Alma María',\n",
       " 'Rosemary Okafor',\n",
       " 'Ige Behringer',\n",
       " 'Filip Traverse',\n",
       " 'Damjan Chapman',\n",
       " 'Wulf Carlevaro',\n",
       " 'Aaron Dreschner',\n",
       " 'Kumar Dahl',\n",
       " 'Eugenio Mataracı',\n",
       " 'Ferran Simen',\n",
       " 'Xavier Hayasaka',\n",
       " 'Luis Moray',\n",
       " 'Marcos Rudaski']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers = [*tts.synthesizer.tts_model.speaker_manager.name_to_id]\n",
    "speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eff90f51-59f3-43c6-8f8a-6043bfd88c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations. Videos that are shot using commodity hardware such as phones and surveillance cameras record various metadata such as time and location. We encounter such geospatial videos on a daily basis and such videos have been growing in volume significantly. Yet, we do not have data management systems that allow users to interact with such data effectively. In this paper, we describe Spatialyze, a new framework for end to end querying of geospatial videos. Spatialyze comes with a domain specific language where users can construct geospatial video analytic workflows using a 3 step, declarative, build-filter-observe paradigm. Internally, Spatialyze leverages the declarative nature of such workflows, the temporal-spatial metadata stored with videos, and physical behavior of real-world objects to optimize the execution of workflows. Our results using real world videos and workflows show that Spatialyze can reduce execution time by up to 5.3 time, while maintaining up to 97.1% accuracy compared to unoptimized execution.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab232fce-1d27-4cbb-a6d8-c26bc02cdd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claribel Dervla\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.158528566360474\n",
      " > Real-time factor: 1.0028349810291335\n",
      "Daisy Studious\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.951785564422607\n",
      " > Real-time factor: 1.0070355409416705\n",
      "Gracie Wise\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.279573917388916\n",
      " > Real-time factor: 1.2768439359681303\n",
      "Tammie Ema\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.491836309432983\n",
      " > Real-time factor: 1.169638111378916\n",
      "Alison Dietlinde\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.300155162811279\n",
      " > Real-time factor: 0.9901244536149268\n",
      "Ana Florence\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.2706451416015625\n",
      " > Real-time factor: 1.1510462763664162\n",
      "Annmarie Nele\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.765522480010986\n",
      " > Real-time factor: 1.0632609952976555\n",
      "Asya Anara\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.412165641784668\n",
      " > Real-time factor: 0.9843784978371946\n",
      "Brenda Stern\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.919591188430786\n",
      " > Real-time factor: 1.1526727434493143\n",
      "Gitta Nikolina\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 8.021759510040283\n",
      " > Real-time factor: 1.1344266110594423\n",
      "Henriette Usha\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.8155739307403564\n",
      " > Real-time factor: 1.154554381316492\n",
      "Sofia Hellen\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.7352190017700195\n",
      " > Real-time factor: 1.2237275790130926\n",
      "Tammy Grit\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.787364482879639\n",
      " > Real-time factor: 1.0926102882803999\n",
      "Tanja Adelina\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 8.035516262054443\n",
      " > Real-time factor: 1.1289576764852463\n",
      "Vjollca Johnnie\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.378495693206787\n",
      " > Real-time factor: 1.0708203651115367\n",
      "Andrew Chipper\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.05367636680603\n",
      " > Real-time factor: 0.9566821910249543\n",
      "Badr Odhiambo\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.680695295333862\n",
      " > Real-time factor: 1.1215535636352758\n",
      "Dionisio Schuyler\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.867428302764893\n",
      " > Real-time factor: 1.1054987302590664\n",
      "Royston Min\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.446954727172852\n",
      " > Real-time factor: 1.0287220067678617\n",
      "Viktor Eka\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.397521257400513\n",
      " > Real-time factor: 1.1526453108713663\n",
      "Abrahan Mack\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.335159778594971\n",
      " > Real-time factor: 1.0372629285821782\n",
      "Adde Michal\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 8.28251600265503\n",
      " > Real-time factor: 1.1394970915602438\n",
      "Baldur Sanjin\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.087601900100708\n",
      " > Real-time factor: 0.9780096761858401\n",
      "Craig Gutsy\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.964413404464722\n",
      " > Real-time factor: 0.9505627634968747\n",
      "Damien Black\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.100007772445679\n",
      " > Real-time factor: 0.9266246817932368\n",
      "Gilberto Mathias\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.533007621765137\n",
      " > Real-time factor: 1.0977504630162926\n",
      "Ilkin Urbano\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.730466842651367\n",
      " > Real-time factor: 1.159283166795265\n",
      "Kazuhiko Atallah\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.790332317352295\n",
      " > Real-time factor: 0.9166917547215544\n",
      "Ludvig Milivoj\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.046335220336914\n",
      " > Real-time factor: 1.1198233739452774\n",
      "Suad Qasim\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.500806570053101\n",
      " > Real-time factor: 0.9163301165664728\n",
      "Torcull Diarmuid\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.470732688903809\n",
      " > Real-time factor: 0.9493749054503952\n",
      "Viktor Menelaos\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.394556045532227\n",
      " > Real-time factor: 0.9127529220686433\n",
      "Zacharie Aimilios\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.672855615615845\n",
      " > Real-time factor: 0.918042242714444\n",
      "Nova Hogarth\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.2192702293396\n",
      " > Real-time factor: 0.961621427668421\n",
      "Maja Ruoho\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.301922082901001\n",
      " > Real-time factor: 1.0880053379282022\n",
      "Uta Obando\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.519597768783569\n",
      " > Real-time factor: 0.9037300314963593\n",
      "Lidiya Szekeres\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.610514402389526\n",
      " > Real-time factor: 0.9418918456319973\n",
      "Chandra MacFarland\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.974584579467773\n",
      " > Real-time factor: 1.0748141196654304\n",
      "Szofi Granger\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.84037709236145\n",
      " > Real-time factor: 0.9673570734130964\n",
      "Camilla Holmström\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.443500280380249\n",
      " > Real-time factor: 0.9626361893877878\n",
      "Lilya Stainthorpe\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.129584074020386\n",
      " > Real-time factor: 1.0371188523031731\n",
      "Zofija Kendrick\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.233471870422363\n",
      " > Real-time factor: 1.1557830053827036\n",
      "Narelle Moon\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.537088871002197\n",
      " > Real-time factor: 0.9244664546280045\n",
      "Barbora MacLean\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.0581955909729\n",
      " > Real-time factor: 0.945199909295769\n",
      "Alexandra Hisakawa\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.362130403518677\n",
      " > Real-time factor: 0.9479739390581875\n",
      "Alma María\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.174639463424683\n",
      " > Real-time factor: 0.9703985643211472\n",
      "Rosemary Okafor\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.969747543334961\n",
      " > Real-time factor: 0.9609926799624452\n",
      "Ige Behringer\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.300101041793823\n",
      " > Real-time factor: 0.945272373241384\n",
      "Filip Traverse\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.368494987487793\n",
      " > Real-time factor: 1.050668096702702\n",
      "Damjan Chapman\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.07822322845459\n",
      " > Real-time factor: 0.9552459102194072\n",
      "Wulf Carlevaro\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 7.859217882156372\n",
      " > Real-time factor: 1.0812603218375512\n",
      "Aaron Dreschner\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.2929112911224365\n",
      " > Real-time factor: 1.0926569702756845\n",
      "Kumar Dahl\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.399990081787109\n",
      " > Real-time factor: 0.9326410417112044\n",
      "Eugenio Mataracı\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.67413067817688\n",
      " > Real-time factor: 0.9301027748875026\n",
      "Ferran Simen\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.970361709594727\n",
      " > Real-time factor: 0.9651391270004253\n",
      "Xavier Hayasaka\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 6.342362642288208\n",
      " > Real-time factor: 0.9516133387483328\n",
      "Luis Moray\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 5.983283042907715\n",
      " > Real-time factor: 1.04947332868871\n",
      "Marcos Rudaski\n",
      " > Text splitted to sentences.\n",
      "['The road traffic system is a highly interactive social system in which individuals']\n",
      " > Processing time: 8.347606182098389\n",
      " > Real-time factor: 1.2247465953054766\n"
     ]
    }
   ],
   "source": [
    "for speaker in speakers:\n",
    "    print(speaker)\n",
    "    tts.tts_to_file(text=text, speaker=speaker, language=\"en\", file_path='-'.join(speaker.split(' ')) + '.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdcf8eaa-6647-4d86-a82c-f11b8aee1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvoices = [' '.join(v[:-len('.wav')].split('-')) for v in os.listdir('fvoices2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb11d807-c725-427d-b31f-963eb796e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tanja Adelina',\n",
       " 'Alexandra Hisakawa',\n",
       " 'Vjollca Johnnie',\n",
       " 'Nova Hogarth',\n",
       " 'Suad Qasim',\n",
       " 'Annmarie Nele']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f48dd310-c795-49d7-abe1-b883aa00f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in fvoices:\n",
    "    assert v in speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98c8f36f-d5df-40ec-8172-66ac468730f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 'Tanja Adelina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e80ac71-fe90-4e8c-8242-efdf577eeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['The details to derive the number of plans using these optimizations are described Section .', '.', 'As new models and optimization opportunities arise the number of plans for complex queries will only grow.', 'Even for expert users it is challenging to manually reason about this large number of query plans.', 'It requires users to not only have an intuition about the potential of an optimization but also have deep knowledge of the performanceaccuracy tradeoffs and selectivity of models within a large space of query plans.', 'DOMAIN SPECIFIC MODEL RELATIONSHIPS There is potential for multiple models and predicates to improve the execution of a query.', 'However there is no framework to reason about the relationship between models.', 'In this section we propose a framework to define model relationships.', 'Suppose we have two models M and H that we use to process a set of frames F. The models emit a labeled frame with high confidence that satisfies a predicate or produce no output and the frame is dropped.', 'The label is the output of the model assigned to the frame given its trained classes while the predicate is part of a users query that filters by a specific labels.', 'For example for an object detection model the trained classes can be bus car person etc. while the predicate can be to only return frames where the label is a person.', 'This is a common scenario where a user runs a model that generates class labels and only wants to keep frames from a certain class.', 'There are multiple execution plans that with a given probability can produce the same set of frames and labels on F  Plan A Run M on F  Plan B Run H on F  Plan C Run H on F then M on Hs output  Plan D Run M on F then H on Ms output We then ask the question under what conditions do the above plans produce approximately the same results For plans A and B to produce the same results M and H must be interchangeable H can replace M in the execution plan or viceversa.', 'For Plans C and D to produce the same results as A and B H can only drop frames M would also have dropped H is a filter for M. We can characterize a model by its signature and output classes.', 'The signature is the models input and output specification.', 'T']\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      " > Processing time: 195.96603560447693\n",
      " > Real-time factor: 1.2687241281636152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.wav'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text=text[:len(text) // 2], speaker_wav='./Lisa.wav', language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "619f0d48-80b1-4f9e-9441-76cdd3bed4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0df157c-9897-41f5-ac71-b97190c8d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b72c8976-ab84-40c7-b565-f04558acb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def text_to_speech(text, output_file='output.mp3', language='en'):\n",
    "    try:\n",
    "        # tts = gTTS(text=text, lang=language, slow=False)\n",
    "        # tts.save(output_file)\n",
    "        texts = text.split('.')\n",
    "        texts_chunks = chunks(texts, 40)\n",
    "        texts_chunks = ['.'.join(t) for t in texts_chunks]\n",
    "        os.makedirs(output_file, exist_ok=True)\n",
    "        for idx, _text in tqdm(enumerate(texts_chunks)):\n",
    "            print(f'Chunk #{idx:02d} ({len(_text.split(\" \"))} words)')\n",
    "            tts.tts_to_file(text=_text, speaker=random.choice(fvoices), language=\"en\", file_path=os.path.join(output_file, output_file + f'.part{idx:02d}.mp3'))\n",
    "        # return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595a544-0e89-49a0-aac9-574690b40cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vldb_viva.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #00 (393 words)\n",
      " > Text splitted to sentences.\n",
      "['Optimizing Video Analytics with Declarative Model Relationships', 'Francisco Romero∗ Stanford University faromero@stanford.edu', 'Daniel Kang Stanford University ddkang@cs.stanford.edu', 'Johann Hauswald∗ Stanford University & Sutter Hill Ventures johannh@stanford.edu', 'Matei Zaharia Stanford University matei@cs.stanford.edu', 'Aditi Partap Stanford University aditi712@stanford.edu', 'Christos Kozyrakis Stanford University christos@cs.stanford.edu', 'ABSTRACT', '1 INTRODUCTION', 'The availability of vast video collections and the accuracy of ML models has generated significant interest in video analytics systems.', 'Since naively processing all frames using expensive models is impractical, researchers have proposed optimizations such as selectively using faster but less accurate models to replace or filter frames for expensive models.', 'However, these optimizations are difficult to apply on queries with multiple predicates and models, as users must manually explore a large optimization space.', 'Without significant systems expertise or time investment, an analyst may manually create an execution plan that is unnecessarily expensive and/or terribly inaccurate.', 'We propose Relational Hints, a declarative interface that allows users to suggest ML model relationships based on domain knowledge.', 'Users can express two key relationships: when a model can replace another (CAN REPLACE) and when a model can be used to filter frames for another (CAN FILTER).', 'We aim to design an interface to express model relationships informed by domain specific knowledge and define the constraints by which these relationships hold.', 'We then present the VIVA video analytics system that uses relational hints to optimize SQL queries on video datasets.', 'VIVA automatically selects and validates the hints applicable to the query, generates possible query plans using a formal set of transformations, and finds the best performance plan that meets a user’s accuracy requirements.', 'VIVA relieves users from rewriting and manually optimizing video queries as new models become available and execution environments evolve.', 'We evaluate VIVA implemented on top of Spark and show that hints improve performance up to 16.6× without sacrificing accuracy.', 'PVLDB Reference Format: Francisco Romero, Johann Hauswald, Aditi Partap, Daniel Kang, Matei Zaharia, and Christos Kozyrakis.', 'Optimizing Video Analytics with Declarative Model Relationships.', 'PVLDB, 16(3): 447 - 460, 2022.', 'doi:10.14778/3570690.3570695', 'PVLDB Artifact Availability:', 'The source code, data, and/or other artifacts have been made available at https://github.com/stanford-mast/viva-vldb23-artifact.', '∗Denotes equal contribution.', 'This work is licensed under the Creative Commons BY-NC-ND 4.0 International License.', 'Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license.', 'For any use beyond those covered by this license, obtain permission by emailing info@vldb.org.', 'Copyright is held by the owner/author(s).', 'Publication rights licensed to the VLDB Endowment.', 'Proceedings of the VLDB Endowment, Vol.', '16, No']\n",
      " > Processing time: 314.6885290145874\n",
      " > Real-time factor: 1.0635201728593213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [05:15, 315.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #01 (816 words)\n",
      " > Text splitted to sentences.\n",
      "['3 ISSN 2150-8097.', 'doi:10.14778/3570690.3570695', 'Video analytics, the ability to extract insights from video, is enabled by increasingly accurate machine learning (ML) models and access to large archives of professionally produced content or videos captured by devices like cellphones, security cameras, and videoconference systems.', 'While we can already answer queries over videos like “have any cars passed this intersection that match an AMBER alert?”, several challenges remain before video analytics are as practical and as performant over analytics on structured data.', 'For complex video analytics queries with multiple predicates or ML models, users must manually optimize their queries to avoid the high cost of naively executing large models on every frame using expensive hardware.', 'For example, it takes over 14 GPU-months to process 100 camera-months of video using a very accurate YOLOv5 model for object detection [55].', 'Consider an analyst studying political coverage of major cable news channels that writes a query to find instances of Bernie Sanders, a politician, reacting angrily to Jake Tapper, a TV news host [20].', 'Their query may use object detection to find scenes with two people, face recognition to find instances of Jake Tapper and Bernie Sanders, and emotion detection to detect angry reactions.', 'This query can take minutes to execute using unnecessarily accurate models, even on small video inputs, making it challenging for the analyst to interactively explore their dataset.', 'To improve performance, the analyst may use domain knowledge to explore the following model optimizations: • Replacement: use a different model for a task, such as a cheaper', 'but less accurate object detector [25, 27, 28, 47].', '• Input Filtering: use a fast model to filter inputs to an expensive model [26, 36, 63].', 'For example, insert a binary classifier to detect faces before recognizing Tapper or Sanders in frames.', '• Predicate Reordering: run emotion detection before face detection', 'because it is more selective.', 'The domain knowledge needed to consider such optimizations may come from (1) historical or similar queries using alternate models, (2) insights about the training data or query dataset like knowing angry emotions are less prevalent than neutral or happy ones, or (3) knowledge about a general area of expertise (e.g., news, traffic, or sports analysis) suggesting a particular fine-tuned model would be better suited for the domain.', 'Unfortunately, systems today do not provide an interface for users to specify optimizations based on domain knowledge.', 'Users must manually explore the performance-accuracy tradeoff across numerous combinations of optimizations in queries with multiple predicates.', 'We found that, for the news analysis query in which there are nearly 100 plan options, performance can vary by up to 11.7× across different query plans with an accuracy requirement of 80%.', 'Systems today provide no easy way to validate potential optimizations either.', 'Hence, in order to optimize video analysis queries, users must build significant expertise in ML models and systems, taking away valuable time and money from their primary task of gleaning insights from video data.', 'The first goal of this work is to design a user interface to express model relationships informed by domain specific knowledge and define the constraints by which these relationships hold.', 'Our second goal is to develop a video query engine that automatically validates relationships and optimizes complex queries.', 'The engine explores alternate query plans and handles performance-accuracy tradeoffs, relieving users from manual exploration and optimization.', 'We propose a declarative SQL interface for model relationships called relational hints.', 'We capture the semantics of two relationships between a model M and model H considered for optimization: • H CAN REPLACE M denotes H and M are interchangeable in the query plan.', 'For example, H may be a faster object detection model the user wants to consider instead of the current one.', '• H CAN FILTER M denotes H can be used to filter inputs to M. For example, H could be a fast binary classifier for detecting the presence of a face prior to recognizing a specific person.', 'Hints capture high-level relationships based on the models’ output signatures and their class labels.', 'Hence, they can often be reused across queries similar to an index.', 'They remove the need for users to manually rewrite queries when a new model becomes available and reason about how the probabilistic nature of models impact their query’s end-to-end accuracy goal.', 'We develop VIVA a video analytics system that optimizes complex SQL queries using relational hints.', 'VIVA’s hint validator first determines what hints are applicable for the query.', 'VIVA’s planner uses the validated hints to generate alternate plans using model replacements, data filtering, and predicate reordering while pruning and limiting the search space for fast query optimization.', 'VIVA’s optimizer enumerates plans enabled by hints and automatically navigates the performance-accuracy tradeoff to select the best performance plan meeting the user’s accuracy requirements.', 'In summary, we make the following contributions:', '• We highlight the difficulty of manually optimizing complex video queries, showing that performance on the same query applying different optimizations can vary by up to 11.7× (Section 2)']\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      " > Processing time: 493.43296575546265\n",
      " > Real-time factor: 1.107993713767627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [13:30, 420.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #02 (830 words)\n",
      " > Text splitted to sentences.\n",
      "['• We formalize a declarative SQL interface for users to specify intuitive relationships between ML models used in video analytics based on domain knowledge (Sections 3 and 4).', '• We detail the design of VIVA that incorporates relational hints in query planning and optimization given the user’s accuracy requirements (Section 5).', '• We implement VIVA of Spark [64] (Section 6) and show that across four real-world queries with different video inputs, hints improve performance up to 16.6× while meeting user accuracy requirements (Section 7).', '2 THE COMPLEXITY OF VIDEO QUERIES', 'Recent work on video analytics optimization focuses primarily on optimizing a single predicate that uses an ML model, implemented as a user-defined function (UDF) in a query execution engine.', 'Current proposed techniques explore the performance-accuracy tradeoff using fast proxy models to replace more expensive ones [25, 63], cheap filters to reduce the amount of processing needed [26, 36] and indexing for video data [28].', 'In contrast, we focus on complex queries composed of multiple compute-intensive ML models and predicates.', 'These queries are typically applied on large datasets using scale-out execution engines [42, 48].', 'Unfortunately, executing these complex queries with multiple ML models is prohibitively expensive and slow.', 'Prior work estimates that a similar query to the TV News analysis query previously mentioned over one year of CNN videos would, at time of writing, take over 4 hours and cost more than $300 using cloud GPUs [31].', 'While it is possible to optimize in isolation each model and predicate in a complex query, this approach is unlikely to lead to best overall performance (lowest latency) and makes it difficult to achieve an overall accuracy goal.', 'We use the TV News analysis query to illustrate the difficulty of manual optimization.', 'We consider the impact of predicate reordering and two optimizations: • Model Replacement – replace the original model with one that has the same input/output specification but a different performance accuracy profile.', 'BlazeIt [25] and TASTI [28] are techniques that can generate very fast, purpose-built models but still require a user to manually specify the use of these fast models.', '• Data Filter Model – run a cheap classifier ahead of a more expensive model to filter frames that are unlikely to satisfy the predicate.', 'Systems like PP [36] and CORE [63] automate the insertion of filter models but do not take into account the effects of other optimizations from an end-to-end query perspective.', 'In the TV News analysis query, the analyst is looking for instances of the politician Bernie Sanders reacting angrily to a news anchor Jake Tapper.', 'The analyst uses object detection to find frames with two people, face recognition to find frames with both Sanders and Tapper, and emotion detection to find the angry emotion.', 'Figure 1 shows the accuracies and latencies of different plans for this query after applying the optimizations discussed above.', 'Accuracy is calculated using F1 score [3, 5, 63] with respect to the original plan.', 'The analyst sets an accuracy requirement of 80%.', 'Figure 1 shows that selecting the best of the 6 possible reorderings of the 3 predicates in the query leads to 5× latency improvement.', 'Execution engines today treat UDFs as black-boxes that are not optimized by the execution engine [64]; the analyst must explore the impact of all possible orders.', 'Next, we show the impact of manually considering model replacements.', 'If the analyst uses a faster emotion detection (ED) model, latency improves by 1.01× but accuracy drops to 79%.', 'In contrast, replacing object detection (OD) with a faster person detection model to label people in frames reduces latency by 1.2× over the best reordering without affecting accuracy (Best).', 'Finally, we investigate the impact of using fast filter models, such as a cheap face detection (Haar) model to filter frames without faces [56] and a similarity detector (Sim) to remove frames that are not similar to a reference frame [4].', 'The Haar filter does not degrade accuracy but increases the latency by 1.5× over OD as its selectivity is low and acts as a poor filter.', 'The Sim filter Table 1: Model Relationship Matrix: dimensions to evaluate how to relate ML models.', 'The result is the relationship in a query plan.', 'Signature', 'Equal Not Equal', 'Classes', 'Equal or Overlap CAN REPLACE CAN FILTER', 'Disjoint CAN FILTER CAN FILTER', 'Figure 1: Manually Optimizing a TV News Analysis Query.', 'PR: Predicate Reordering, RP: Replacing models, FT: Filters, dashed line denotes the user’s accuracy requirement of 80%.', 'reduces latency by 2× over OD: it skips the expensive face recognition model for 94% of the frames without impacting accuracy.', 'The key difference between the Haar and Sim filters is the former supports general face detection while the latter finds similarities to a reference frame.', 'The process of exploring the performance-accuracy tradeoffs and the interactions between these optimizations is long and cumbersome.', 'Users must exhaustively study all options or use some ad-hoc trial and error process to find a good plan.', 'For this query where the are 6 permutations, 2 replacements, and 2 filters, there are almost 100 plans to consider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 557.5162954330444\n",
      " > Real-time factor: 1.107415580588791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [22:49, 483.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #03 (763 words)\n",
      " > Text splitted to sentences.\n",
      "['The details to derive the number of plans using these optimizations are described Section 5.2.', 'As new models and optimization opportunities arise, the number of plans for complex queries will only grow.', 'Even for expert users, it is challenging to manually reason about this large number of query plans.', 'It requires users to not only have an intuition about the potential of an optimization but also have deep knowledge of the performance-accuracy tradeoffs and selectivity of models within a large space of query plans.', '3 DOMAIN SPECIFIC MODEL RELATIONSHIPS', 'There is potential for multiple models and predicates to improve the execution of a query.', 'However, there is no framework to reason about the relationship between models.', 'In this section, we propose a framework to define model relationships.', 'Suppose we have two models, M and H, that we use to process a set of frames F. The models emit a labeled frame with high confidence that satisfies a predicate or produce no output and the frame is dropped.', 'The label is the output of the model assigned to the frame given its trained classes while the predicate is part of a user’s query that filters by a specific label(s).', 'For example, for an object detection model, the trained classes can be bus, car, person, etc. while the predicate can be to only return frames where the label is a person.', 'This is a common scenario where a user runs a model that generates class labels and only wants to keep frames from a', 'certain class.', 'There are multiple execution plans that, with a given probability, can produce the same set of frames and labels on F: • Plan A: Run M on F • Plan B: Run H on F • Plan C: Run H on F, then M on H’s output • Plan D: Run M on F, then H on M’s output We then ask the question: under what conditions do the above plans produce approximately the same results?', 'For plans A and B to produce the same results, M and H must be interchangeable: H can replace M in the execution plan (or vice-versa).', 'For Plans C and D to produce the same results as A and B, H can only drop frames M would also have dropped; H is a filter for M. We can characterize a model by its signature and output classes.', 'The signature is the model’s input and output specification.', 'This is similar to terminology used by TensorFlow [54].', 'To compare two models, we ask the following questions: • Are the model signatures equal or not?', '• Do the models have equal, overlapping, or disjoint output classes?', 'Table 1 captures the different options along these dimensions.', 'If H and M have equal signatures and equal or overlapping classes, then H can replace M. While two models can produce equivalent outputs, they may still differ in performance (execution latency) and/or accuracy.', 'This type of model is referred to as a variant [46– 48] or a proxy model [25].', 'The model architecture, dataset, and training parameters affect a model’s performance and accuracy.', 'If H and M have equal signatures and disjoint classes, or their signatures are different, H can potentially filter frames for M. For example, consider an image classifier that outputs animal labels per image.', 'Now consider an object detector that can produce the same class labels but the class is attributed to a bounding box.', 'These two model signatures are not equal but there is overlap in the classes.', 'The image classifier can be predicated on whether an animal was found.', 'This only passes frames to the object detector that are highly likely to have an animal.', 'The image classifier acts as a filter.', 'The setup is similar for disjoint labels except a user specifies under what conditions the predicate for the image classifier is true.', 'This can be specified based on a user’s domain knowledge.', '4 RELATIONAL HINTS We next propose a declarative interface called Relational Hints (hints for short) that formalizes the model relationships defined in Table 1.', 'Hints allow users to declaratively express domain specific knowledge about model relationships.', 'The goal is to provide a query planner with information that enables alternate query plans.', 'A planner can select among these plans to improve query performance or reduce the price while meeting a user’s accuracy requirements.', 'We first describe the different types of hints and their syntax.', 'Next, we walk through a workflow of how hints are used in SQL queries.', 'Finally, we describe sources informing the design of relational hints and relate them to well-known optimizations and domain intuition']\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      " > Processing time: 364.59052181243896\n",
      " > Real-time factor: 1.074222183229323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [28:54, 437.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #04 (806 words)\n",
      " > Text splitted to sentences.\n",
      "['4.1 Relational Hint Types', 'A hint takes as input two models and a type, CAN REPLACE or CAN FILTER, that establishes a relationship between the models.', 'We map Table 1 to our declarative hint interface.', 'OrigBestEDODHaarSim050100150200250300Latency (sec)0255075100F1 scorePRRPFTLatencyF1 score\\x0cDefinition 1 A relational hint is a user defined model relationship informed by domain knowledge for the purpose of suggesting alternate query plans to an optimizer.', 'Similar to MicrosoftSQL hints [37] or MySQL hints [39], hints are options specified to the query optimizer to consider alternate query plans.', 'Unlike the aforementioned hints, relational hints are not enforced.', 'Users set a minimum query accuracy requirement and the optimizer chooses which hint(s) (if any) meet that requirement.', 'The accuracy is in reference to the unmodified query plan where the labels produced by the original models represent the ground-truth.', 'The accuracy is calculated on a video supplied by the user called a canary input.', 'A canary input is a shorter clip that represents the type of events the user is looking for.', 'A hint associates a hint model H to an original model M using model specific domain knowledge.', 'Definition 2 Domain knowledge is external information about a model’s signature and class labels in relation to another model.', 'CAN REPLACE Hint.', 'If a model H’s signature and classes are', 'equal or overlap with model M’s signature and classes, a user can define a CAN REPLACE hint to suggest H can replace M in a plan:', 'CREATE HINT H CAN REPLACE M', '[ FALLBACK DISABLED | ENABLED ]', 'A CAN REPLACE hint is optionally parameterized by a FALLBACK argument.', 'When disabled (the default), this expresses to the system that model H should completely replace M when processing frames.', 'If enabled, the processing will fallback to the original model M if H does not produce a label because its confidence is too low.', 'We assume confidence thresholds are pre-tuned and set for each model as is commonly done with existing optimizations [25].', 'In effect, this threshold arbitrates whether the model will generate a label or not.', 'This can also be exposed to the user as a parameter to tune.', 'Setting FALLBACK ENABLED may result in M having to process the same inputs that did not satisfy H’s confidence threshold.', 'This may negate some of the performance benefits of using H. However, it gives finer control to the user of the relationship and how much they want to trade-off performance and accuracy.', 'CAN FILTER Hint.', 'If model H’s signature is equal and its classes are disjoint from model M, or if model H’s signature is not equal to model M’s signature, a user can define a CAN FILTER hint to suggest that model H can filter frames for model M. Specifically, frames are only processed by M if they satisfy H’s predicate with high confidence using the model’s pre-set threshold:', 'CREATE HINT H CAN FILTER M', '[ CONDITIONED ON ANY | <list -of - classes > ]', 'A CAN FILTER hint is optionally parameterized by a CONDITIONED ON parameter which specifies the relationship between the model classes.', 'By default, this parameter is set to ANY: any class in H can satisfy the condition.', 'A user can optionally specify a list of classes.', 'The list of classes means a user can condition M’s input on the results of H’s predicate as defined by the condition.', 'Prior work investigates automatically inferring relationships using historical data [36, 63].', 'Our interface could be extended', 'to support automatic inference of these relationships by setting CONDITIONED ON to AUTO.', 'In this work, we focus on the overall interface of expressing model relationships and leave it to future work to investigate inferring these relationships.', '4.2 Example Workflow with Relational Hints', 'We now walk through a workflow using three relational hints for the TV News analysis query searching for Bernie Sanders reacting angrily to Jake Tapper Hints are registered once and automatically used on future queries when applicable.', 'The first hint expresses knowledge that two object detection models have the same signature (labeled bounding boxes of objects) and generate the same number of classes but vary in performance and accuracy.', 'These models can be related using a CAN REPLACE hint:', 'CREATE HINT ObjectDetectFast CAN REPLACE ObjectDetect', 'The second hint uses a tuned face recognition model trained on journalists and personalities.', 'This is similar to a BlazeIt trained model to represent a more expensive model [18].', 'This model has the same signature (labeled bounding boxes of faces) as a general face recognition model, with some overlap in classes, including labels for Bernie Sanders and Jake Tapper.', 'These models are again related using a CAN REPLACE:', 'CREATE HINT FaceRecogNews CAN REPLACE FaceRecognition', 'FALLBACK ENABLED', 'The CAN REPLACE hint is parameterized with FALLBACK ENABLED to indicate the original FaceRecognition model should be used if FaceRecogNews does not emit a label due to low confidence.', 'The third hint considers a binary detector with labels face/no face.', 'This binary detector can be trained using optimizations like Probabilistic Predicates [36]']\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.pdf'):\n",
    "        print(f)\n",
    "\n",
    "        # path = open(f, 'rb') \n",
    "        # pdfReader = pypdf.PdfReader(path) \n",
    "        # text = ''\n",
    "        # for i in range(len(pdfReader.pages)):\n",
    "        #     page = pdfReader.pages[i]\n",
    "        #     text += page.extract_text()\n",
    "        text = text_from_pdf(f)\n",
    "        text2 = ''\n",
    "        for p in text.replace('\\n\\n\\x0c', ' ').replace('-\\n\\n', '').replace('-\\n', '').split('\\n\\n'):\n",
    "            text2 += p.replace('\\n', ' ') + '\\n\\n'\n",
    "        text_to_speech(text2, f.replace('.pdf', ''))\n",
    "        # speak = pyttsx3.init() \n",
    "        # speak.save_to_file(text, f.replace('.pdf', '.mp3'))\n",
    "        # speak.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e815ba-ae0d-4890-9a5d-fbe33a071bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
